<!DOCTYPE html>
<html lang="en" dir="auto">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Hands-on: Foundational Aspects of Machine Learning using Python / Foundational Aspects of Machine Learning using Python / Statistics and machine learning</title>
        
            <meta name="google-site-verification" content="9mOXn2JL833-i7-aioCCEuIdG4_tb6qjwUozB5GJnPQ" />

<!-- JavaScript Error Monitoring, and performance tracking. -->
<script
  src="https://browser.sentry-cdn.com/7.52.1/bundle.tracing.min.js"
  integrity="sha384-muuFXKS3752PNA4rPm9Uq6BLvOfV4CXyr9MHDBPvozOJJUWLKkogEFWOIRoVps43"
  crossorigin="anonymous"
></script>
<script type="text/javascript">
if(localStorage.getItem('sentry-opt-out') !== 'opt-out' && navigator.doNotTrack !== "1") {
	console.log("Sentry: opt-in");
	Sentry.init({
		dsn: "https://45e0ec6e4373462b92969505df37cf40@sentry.galaxyproject.org/10",
		release: "galaxy-training-network@c8f75f0bb8b08e7d5da83ea63d24e71238e0309c",
		integrations: [new Sentry.BrowserTracing(), new Sentry.Replay()],
		sampleRate: 0.1,
		tracesSampleRate: 0.1,
		// Capture Replay for no sessions by default
		replaysSessionSampleRate: 0.01,
		// plus for 1% of sessions with an error
		replaysOnErrorSampleRate: 0.01,
		// PII OFF
		sendDefaultPii: false, // Off by default but just in case.
		environment: "production",
	});
}
</script>

<!-- Page view tracking -->
<script defer data-domain="training.galaxyproject.org" src="https://plausible.galaxyproject.eu/js/plausible.js"></script>
<script>
if(localStorage.getItem('plausible-opt-out') !== 'opt-out' && navigator.doNotTrack !== "1") {
	localStorage.removeItem("plausible_ignore")
	console.log("Plausible: opt-in");
	window.plausible = window.plausible || function() { (window.plausible.q = window.plausible.q || []).push(arguments) }
} else {
	// if they're opting-out, or DNT
	// we might get one page by accident but we won't get future ones.
	localStorage.setItem("plausible_ignore", "true")
}
</script>

        
        <link rel="shortcut icon" href="/training-material/favicon.ico" type="image/x-icon">
        <link rel="alternate" type="application/atom+xml" href="/training-material/feed.xml">
        <link rel="canonical" href="https://training.galaxyproject.org/training-material/topics/statistics/tutorials/intro-to-ml-with-python/tutorial.html">
        <link rel="license" href="https://spdx.org/licenses/CC-BY-4.0">
        <link rel="preload" href="/training-material/assets/fonts/AtkinsonHyperlegible/Atkinson-Hyperlegible-Regular-102a.woff2" as="font" type="font/woff2" crossorigin>
        <link rel="preload" href="/training-material/assets/fonts/AtkinsonHyperlegible/Atkinson-Hyperlegible-Bold-102a.woff2" as="font" type="font/woff2" crossorigin>
        <link rel="preload" href="/training-material/assets/fonts/AtkinsonHyperlegible/Atkinson-Hyperlegible-Italic-102a.woff2" as="font" type="font/woff2" crossorigin>
        
        <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" as="script" crossorigin>
        
        <link rel="preload" href="/training-material/assets/css/main.css?v=3" as="style">
        <link rel='preload' href='/training-material/assets/js/bundle.theme.f1f2de89.js' as='script'>
<link rel='preload' href='/training-material/assets/js/bundle.main.40d4e218.js' as='script'>
        <link rel="stylesheet" href="/training-material/assets/css/main.css?v=3">
        <link rel="manifest" href="/training-material/manifest.json">
        <meta name="theme-color" content="#2c3143"/>
	

        <meta name="DC.identifier" content="https://github.com/galaxyproject/training-material">
<meta name="DC.type" content="text">
<meta name="DC.title" content="Foundational Aspects of Machine Learning using Python">
<meta name="DC.publisher" content="Galaxy Training Network">
<meta name="DC.date" content="2025-03-11 17:21:16 +0000">
<meta name="DC.creator" content="Wandrille Duchemin"><meta name="description" content="Statistical Analyses for omics data and machine learning using Galaxy tools">
        <meta property="og:site_name" content="Galaxy Training Network">
	<meta property="og:title" content="Statistics and machine learning / Foundational Aspects of Machine Learning using Python / Hands-on: Foundational Aspects of Machine Learning using Python">
        <meta property="og:description" content="Statistical Analyses for omics data and machine learning using Galaxy tools">
        <meta property="og:image" content="https://galaxy-training.s3.amazonaws.com/social/topics/statistics/tutorials/intro-to-ml-with-python/tutorial.png">
	<script type="application/ld+json">


{
  "@context": "http://schema.org",
  "@type": "LearningResource",
  "http://purl.org/dc/terms/conformsTo": {
    "@id": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
    "@type": "CreativeWork"
  },
  "audience": {
    "@type": "EducationalAudience",
    "educationalRole": "Students"
  },
  "citation": [
    {
      "@type": "CreativeWork",
      "name": "Galaxy Training: A Powerful Framework for Teaching!",
      "url": "https://doi.org/10.1371/journal.pcbi.1010752"
    },
    {
      "@type": "CreativeWork",
      "name": "Community-Driven Data Analysis Training for Biology",
      "url": "https://doi.org/10.1016/j.cels.2018.05.012"
    }
  ],
  "discussionUrl": "https://gitter.im/Galaxy-Training-Network/Lobby",
  "headline": "Foundational Aspects of Machine Learning using Python",
  "interactivityType": "mixed",
  "isAccessibleForFree": true,
  "isFamilyFriendly": true,
  "license": "https://spdx.org/licenses/CC-BY-4.0.html",
  "producer": {
    "@type": "Organization",
    "http://purl.org/dc/terms/conformsTo": {
      "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
      "@type": "Organization"
    },
    "id": "https://training.galaxyproject.org",
    "email": "galaxytrainingnetwork@gmail.com",
    "name": "Galaxy Training Network",
    "legalName": "Galaxy Training Network",
    "alternateName": "GTN",
    "url": "https://training.galaxyproject.org",
    "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
    "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
    "keywords": [
      "galaxy",
      "bioinformatics",
      "training",
      "fair",
      "accessible"
    ],
    "status": "active",
    "foundingDate": "2015-06-29",
    "socialMedia": "https://mstdn.science/@gtn",
    "type": "project"
  },
  "provider": {
    "@type": "Organization",
    "http://purl.org/dc/terms/conformsTo": {
      "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
      "@type": "Organization"
    },
    "id": "https://training.galaxyproject.org",
    "email": "galaxytrainingnetwork@gmail.com",
    "name": "Galaxy Training Network",
    "legalName": "Galaxy Training Network",
    "alternateName": "GTN",
    "url": "https://training.galaxyproject.org",
    "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
    "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
    "keywords": [
      "galaxy",
      "bioinformatics",
      "training",
      "fair",
      "accessible"
    ],
    "status": "active",
    "foundingDate": "2015-06-29",
    "socialMedia": "https://mstdn.science/@gtn",
    "type": "project"
  },
  "sourceOrganization": {
    "@type": "Organization",
    "http://purl.org/dc/terms/conformsTo": {
      "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
      "@type": "Organization"
    },
    "id": "https://training.galaxyproject.org",
    "email": "galaxytrainingnetwork@gmail.com",
    "name": "Galaxy Training Network",
    "legalName": "Galaxy Training Network",
    "alternateName": "GTN",
    "url": "https://training.galaxyproject.org",
    "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
    "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
    "keywords": [
      "galaxy",
      "bioinformatics",
      "training",
      "fair",
      "accessible"
    ],
    "status": "active",
    "foundingDate": "2015-06-29",
    "socialMedia": "https://mstdn.science/@gtn",
    "type": "project"
  },
  "workTranslation": [],
  "creativeWorkStatus": "Draft",
  "dateModified": "2025-03-11 17:21:16 +0000",
  "datePublished": "2025-03-11 17:21:16 +0000",
  "copyrightHolder": {
    "@type": "Organization",
    "http://purl.org/dc/terms/conformsTo": {
      "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
      "@type": "Organization"
    },
    "id": "https://training.galaxyproject.org",
    "email": "galaxytrainingnetwork@gmail.com",
    "name": "Galaxy Training Network",
    "legalName": "Galaxy Training Network",
    "alternateName": "GTN",
    "url": "https://training.galaxyproject.org",
    "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
    "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
    "keywords": [
      "galaxy",
      "bioinformatics",
      "training",
      "fair",
      "accessible"
    ],
    "status": "active",
    "foundingDate": "2015-06-29",
    "socialMedia": "https://mstdn.science/@gtn",
    "type": "project"
  },
  "funder": [],
  "funding": [],
  "identifier": "https://gxy.io/GTN:",
  "accessMode": [
    "textual",
    "visual"
  ],
  "accessModeSufficient": [
    "textual",
    "visual"
  ],
  "accessibilityControl": [
    "fullKeyboardControl",
    "fullMouseControl"
  ],
  "accessibilityFeature": [
    "alternativeText",
    "tableOfContents"
  ],
  "accessibilitySummary": "The text aims to be as accessible as possible. Image descriptions will vary per tutorial, from images being completely inaccessible, to images with good descriptions for non-visual users.",
  "isPartOf": {
    "@type": "CreativeWork",
    "name": "Statistics and machine learning",
    "description": "Statistical Analyses for omics data and machine learning using Galaxy tools",
    "url": "https://training.galaxyproject.org/training-material/topics/statistics/"
  },
  "abstract": "```python",
  "learningResourceType": "e-learning",
  "name": "Foundational Aspects of Machine Learning using Python",
  "url": "https://training.galaxyproject.org/training-material/topics/statistics/tutorials/intro-to-ml-with-python/tutorial.html",
  "version": 1,
  "timeRequired": "PT3H",
  "teaches": "- general sklearn syntax intro\n- overfit/underfit\n- the need for regularization\n- cross validation and a test set\n- metrics and imbalance",
  "keywords": [
    "Statistics and machine learning",
    "elixir",
    "ai-ml",
    "work-in-progress",
    "jupyter-notebook"
  ],
  "description": "## Abstract\n\n```python\n\n\n## About This Material\n\nThis is a Hands-on Tutorial from the GTN which is usable either for individual self-study, or as a teaching material in a classroom.\n\n\n## Questions this  will address\n\n - to do\n\n\n## Learning Objectives\n\n- general sklearn syntax intro\n- overfit/underfit\n- the need for regularization\n- cross validation and a test set\n- metrics and imbalance\n\n",
  "inLanguage": {
    "@type": "Language",
    "name": "English",
    "alternateName": "en"
  },
  "competencyRequired": [
    {
      "@context": "http://schema.org",
      "@type": "LearningResource",
      "url": "https://training.galaxyproject.org/training-material/topics/introduction/",
      "name": "Introduction to Galaxy Analyses",
      "description": "Introduction to Galaxy Analyses",
      "provider": {
        "@type": "Organization",
        "http://purl.org/dc/terms/conformsTo": {
          "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
          "@type": "Organization"
        },
        "id": "https://training.galaxyproject.org",
        "email": "galaxytrainingnetwork@gmail.com",
        "name": "Galaxy Training Network",
        "legalName": "Galaxy Training Network",
        "alternateName": "GTN",
        "url": "https://training.galaxyproject.org",
        "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
        "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
        "keywords": [
          "galaxy",
          "bioinformatics",
          "training",
          "fair",
          "accessible"
        ],
        "status": "active",
        "foundingDate": "2015-06-29",
        "socialMedia": "https://mstdn.science/@gtn",
        "type": "project"
      }
    },
    {
      "@context": "http://schema.org",
      "@type": "LearningResource",
      "url": "https://training.galaxyproject.org/training-material/topics/data-science/tutorials/python-basics/tutorial.html",
      "name": "Introduction to Python",
      "description": "Hands-on for 'Introduction to Python' tutorial",
      "learningResourceType": "e-learning",
      "interactivityType": "expositive",
      "provider": {
        "@type": "Organization",
        "http://purl.org/dc/terms/conformsTo": {
          "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
          "@type": "Organization"
        },
        "id": "https://training.galaxyproject.org",
        "email": "galaxytrainingnetwork@gmail.com",
        "name": "Galaxy Training Network",
        "legalName": "Galaxy Training Network",
        "alternateName": "GTN",
        "url": "https://training.galaxyproject.org",
        "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
        "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
        "keywords": [
          "galaxy",
          "bioinformatics",
          "training",
          "fair",
          "accessible"
        ],
        "status": "active",
        "foundingDate": "2015-06-29",
        "socialMedia": "https://mstdn.science/@gtn",
        "type": "project"
      }
    },
    {
      "@context": "http://schema.org",
      "@type": "LearningResource",
      "url": "https://training.galaxyproject.org/training-material/topics/data-science/tutorials/python-warmup-stat-ml/tutorial.html",
      "name": "Python - Warm-up for statistics and machine learning",
      "description": "Hands-on for 'Python - Warm-up for statistics and machine learning' tutorial",
      "learningResourceType": "e-learning",
      "interactivityType": "expositive",
      "provider": {
        "@type": "Organization",
        "http://purl.org/dc/terms/conformsTo": {
          "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
          "@type": "Organization"
        },
        "id": "https://training.galaxyproject.org",
        "email": "galaxytrainingnetwork@gmail.com",
        "name": "Galaxy Training Network",
        "legalName": "Galaxy Training Network",
        "alternateName": "GTN",
        "url": "https://training.galaxyproject.org",
        "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
        "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
        "keywords": [
          "galaxy",
          "bioinformatics",
          "training",
          "fair",
          "accessible"
        ],
        "status": "active",
        "foundingDate": "2015-06-29",
        "socialMedia": "https://mstdn.science/@gtn",
        "type": "project"
      }
    }
  ],
  "author": [
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "http://purl.org/dc/terms/conformsTo": {
        "@id": "https://bioschemas.org/profiles/Person/0.3-DRAFT",
        "@type": "CreativeWork"
      },
      "url": "https://training.galaxyproject.org/training-material/hall-of-fame/wandrilled/",
      "mainEntityOfPage": "https://training.galaxyproject.org/training-material/hall-of-fame/wandrilled/",
      "name": "Wandrille Duchemin",
      "image": "https://avatars.githubusercontent.com/wandrilled",
      "description": "A contributor to the GTN project.",
      "memberOf": [
        {
          "@type": "Organization",
          "http://purl.org/dc/terms/conformsTo": {
            "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
            "@type": "Organization"
          },
          "id": "https://training.galaxyproject.org",
          "email": "galaxytrainingnetwork@gmail.com",
          "name": "Galaxy Training Network",
          "legalName": "Galaxy Training Network",
          "alternateName": "GTN",
          "url": "https://training.galaxyproject.org",
          "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
          "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
          "keywords": [
            "galaxy",
            "bioinformatics",
            "training",
            "fair",
            "accessible"
          ],
          "status": "active",
          "foundingDate": "2015-06-29",
          "socialMedia": "https://mstdn.science/@gtn",
          "type": "project"
        }
      ]
    }
  ],
  "contributor": [],
  "about": [
    {
      "@type": "CreativeWork",
      "name": "Statistics and machine learning",
      "description": "Statistical Analyses for omics data and machine learning using Galaxy tools",
      "url": "https://training.galaxyproject.org/training-material/topics/statistics/"
    },
    {
      "@type": "DefinedTerm",
      "@id": "http://edamontology.org/topic_2269",
      "inDefinedTermSet": "http://edamontology.org",
      "termCode": "topic_2269",
      "url": "https://bioportal.bioontology.org/ontologies/EDAM/?p=classes&conceptid=http%3A%2F%2Fedamontology.org%2Ftopic_2269"
    }
  ],
  "educationalLevel": "Intermediate",
  "mentions": [
    {
      "@type": "Thing",
      "url": "https://training.galaxyproject.org/training-material/topics/statistics/tutorials/intro-to-ml-with-python/statistics-intro-to-ml-with-python.ipynb",
      "name": "Jupyter Notebook (with Solutions)"
    },
    {
      "@type": "Thing",
      "url": "https://training.galaxyproject.org/training-material/topics/statistics/tutorials/intro-to-ml-with-python/statistics-intro-to-ml-with-python-course.ipynb",
      "name": "Jupyter Notebook (without Solutions)"
    }
  ]
}</script></head>
    <body data-spy="scroll" data-target="#toc" data-brightness="auto" data-contrast="auto">
        <script  src='/training-material/assets/js/bundle.theme.f1f2de89.js'></script>
        <header>
    <nav class="navbar navbar-expand-md navbar-dark" aria-label="Site Navigation">
        <div class="container">
            <a class="navbar-brand" href="/training-material/">
                <img src="/training-material/assets/images/GTN-60px.png" height="30" alt="Galaxy Training Network logo">
                
                    Galaxy Training!
                
            </a>

            <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#top-navbar" aria-controls="top-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="top-navbar">
                <ul class="navbar-nav">
                    <li class="nav-item">
                        
                        <a class="nav-link" href="/training-material/topics/statistics" title="Go back to list of tutorials">
                            <i class="far fa-folder" aria-hidden="true"></i> Statistics and machine learning
                        </a>
                        
                    </li>

                    <li class="nav-item">
                        
                        <a class="nav-link" href="/training-material/learning-pathways" title="Learning Pathways">
                           <i class="fas fa-graduation-cap" aria-hidden="true"></i><span class="visually-hidden">curriculum</span> Learning Pathways
                        </a>
                        
                    </li>

                    <li class="nav-item dropdown">
    <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-expanded="false" title="Help">
        <i class="far fa-question-circle" aria-hidden="true"></i><span class="visually-hidden">help</span> Help
    </a>
    <div class="dropdown-menu dropdown-menu-right">
	<a class="dropdown-item" href="/training-material/faqs/index.html" title="Check our FAQs">
           <i class="far fa-question-circle" aria-hidden="true"></i><span class="visually-hidden">question</span> FAQs
        </a>
        
        
        
        <a class="dropdown-item" href="/training-material/topics/statistics/faqs/" title="Check our FAQs for the Statistics and machine learning topic">
           <i class="far fa-question-circle" aria-hidden="true"></i><span class="visually-hidden">question</span> Topic FAQs
        </a>
        
        
        
        <a class="dropdown-item" href="https://help.galaxyproject.org/" title="Discuss on Galaxy Help">
            <i class="far fa-comments" aria-hidden="true"></i><span class="visually-hidden">feedback</span> Galaxy Help Forum
        </a>
        <a class="dropdown-item" href="https://gitter.im/Galaxy-Training-Network/Lobby" title="Discuss on gitter">
           <i class="fab fa-gitter" aria-hidden="true"></i><span class="visually-hidden">gitter</span> Discuss on Matrix
        </a>
    </div>
</li>


                    <li class="nav-item dropdown">
    <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-expanded="false" title="Settings">
	<i class="fas fa-cog" aria-hidden="true"></i><span class="visually-hidden">galaxy-gear</span> Settings
    </a>
    <div class="dropdown-menu dropdown-menu-right">

	<h6 class="dropdown-header">Preferences</h6>

	<a href="/training-material/user/theme.html" class="dropdown-item">
		<i class="fas fa-palette" aria-hidden="true"></i><span class="visually-hidden">gtn-theme</span> Theme
	</a>

	<a href="/training-material/user/privacy.html" class="dropdown-item">
		<i class="fas fa-lock" aria-hidden="true"></i><span class="visually-hidden">pref-dataprivate</span> Data Privacy
	</a>

	<div class="dropdown-divider"></div>

	<h6 class="dropdown-header">For Everyone</h6>

        <a class="dropdown-item" href="https://github.com/galaxyproject/training-material/edit/main/topics/statistics/tutorials/intro-to-ml-with-python/tutorial.md">
          <i class="fab fa-github" aria-hidden="true"></i><span class="visually-hidden">github</span> Propose a change or correction
        </a>

	<h6 class="dropdown-header">Instructor Utilities</h6>

        <a class="dropdown-item" href="/training-material/stats.html">
            <i class="fas fa-chart-column" aria-hidden="true"></i><span class="visually-hidden">galaxy-barchart</span> GTN statistics
        </a>

        <a class="dropdown-item" href="https://plausible.galaxyproject.eu/training.galaxyproject.org?period=12mo&page=/training-material/topics/statistics/tutorials/intro-to-ml-with-python/tutorial.html">
            <i class="fas fa-chart-column" aria-hidden="true"></i><span class="visually-hidden">galaxy-barchart</span> Page View Metrics
        </a>

        <!-- link to feedback -->
        
            
            
                <a class="dropdown-item" href="/training-material/feedback.html">
                    <i class="fas fa-chart-column" aria-hidden="true"></i><span class="visually-hidden">galaxy-barchart</span> GTN feedback
                </a>
            
        

        <div class="dropdown-item">
            <div>
                <i class="fas fa-history" aria-hidden="true"></i><span class="visually-hidden">galaxy-rulebuilder-history</span> Previous Versions
            </div>

            <div id="archive-selector">
            
                <a class="btn btn-warning" href="https://training.galaxyproject.org/archive/">Older Versions</a>
            </div>

        </div>

    </div>
</li>


                    <!-- Search bar-->
                    <li class="nav-item">
                      <div id="navbarSupportedContent" role="search">
                        <!-- Search form -->
                        <form class="form-inline mr-auto" method="GET" action="/training-material/search2">
                          <i class="fas fa-search nav-link" aria-hidden="true"></i>
                          <div class="md-form mb-2">
                            <input name="query" class="form-control nicer" type="text" placeholder="Search Tutorials" aria-label="Search">
                          </div>
                        </form>
                      </div>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
</header>

        
        <div class="container main-content" role="main">
        














<!-- Gitter -->






<article class="tutorial topic-statistics">
    <h1 data-toc-skip>Foundational Aspects of Machine Learning using Python</h1>
    
    <div class="alert alert-warning" role="alert">
        <h4 class="alert-heading">Under Development!</h4>
        <p>This tutorial is not in its final state. The content may change a lot in the next months.
        Because of this status, it is also not listed in the topic pages.</p>
    </div>
    

    <section aria-labelledby="overview-box" id="tutorial-metadata">
    <div markdown="0">

	<div class="contributors-line">
		
<table class="contributions">
	
	<tr>
		<td><abbr title="These people wrote the bulk of the tutorial, they may have done the analysis, built the workflow, and wrote the text themselves.">Author(s)</abbr></td>
		<td>
			<a href="/training-material/hall-of-fame/wandrilled/" class="contributor-badge contributor-wandrilled"><img src="https://avatars.githubusercontent.com/wandrilled?s=36" alt="Wandrille Duchemin avatar" width="36" class="avatar" />
    Wandrille Duchemin</a>
		</td>
	</tr>
	

	

	

	

	

	

	

	

	

	

</table>


	</div>

</div>


    <blockquote class="overview">
        <div id="overview-box" class="box-title">Overview</div>
        
        <img alt="Creative Commons License: CC-BY" class="float-right" style="border-width:0; display: inline-block; margin:0" src="/training-material/assets/images/cc-by.png" width="88" height="31"/>
        
        <strong><i class="far fa-question-circle" aria-hidden="true"></i> Questions:</strong>
        <ul>
        
        <li><p>to do</p>
</li>
        
        </ul>

        <strong><i class="fas fa-bullseye" aria-hidden="true"></i> Objectives: </strong>
        <ul>
        
        <li><p>general sklearn syntax intro</p>
</li>
        
        <li><p>overfit/underfit</p>
</li>
        
        <li><p>the need for regularization</p>
</li>
        
        <li><p>cross validation and a test set</p>
</li>
        
        <li><p>metrics and imbalance</p>
</li>
        
        </ul>

        
        <strong><i class="fas fa-check-circle" aria-hidden="true"></i> Requirements:</strong>
        <ul>
        
    
        
        
        
        <li>
          <a href="/training-material/topics/introduction">Introduction to Galaxy Analyses</a>
        </li>
        
    


        
    
        
        
        
            
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                        
                        
                            
                                <li>
                                  <a href="/training-material/topics/data-science/tutorials/python-basics/tutorial.html"><i class="fas fa-laptop" aria-hidden="true"></i><span class="visually-hidden">tutorial</span> Hands-on: Introduction to Python</a>
                                </li>
                            
                        
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                        
                        
                            
                                <li>
                                  <a href="/training-material/topics/data-science/tutorials/python-warmup-stat-ml/tutorial.html"><i class="fas fa-laptop" aria-hidden="true"></i><span class="visually-hidden">tutorial</span> Hands-on: Python - Warm-up for statistics and machine learning</a>
                                </li>
                            
                        
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
        
    


        </ul>
        

        
        <div><strong><i class="fas fa-hourglass-half" aria-hidden="true"></i> Time estimation:</strong> 3 hours</div>
        

        
        <div><strong><i class="fas fa-graduation-cap" aria-hidden="true"></i> Level: </strong>
        


 Intermediate <span class="visually-hidden">Intermediate</span>
<span class="level intermediate" title="Intermediate Tutorial">
  <i class="fas fa-graduation-cap" aria-hidden="true"></i> <i class="fas fa-graduation-cap" aria-hidden="true"></i> <i class="fas fa-graduation-cap" aria-hidden="true"></i>
</span>

</div>
        

        

        

        <div id="supporting-materials"><strong><i class="fa fa-external-link" aria-hidden="true"></i> Supporting Materials:</strong></div>
        <ul class="supporting_material">
            
                <li class="btn btn-default"><a href="/training-material/topics/statistics/tutorials/intro-to-ml-with-python/slides.html" title="Slides for this tutorial">
                    <i class="fab fa-slideshare" aria-hidden="true"></i> Slides
                </a></li>
            

            

            

            

            

            

            
                <li class="btn btn-default supporting_material">


    <a class="topic-icon" href="/training-material/topics/statistics/tutorials/intro-to-ml-with-python/statistics-intro-to-ml-with-python.ipynb" title="Jupyter Notebook" alt="Foundational Aspects of Machine Learning using Python Notebook">
        <i class="far fa-file-code" aria-hidden="true"></i> Jupyter Notebook
    </a>
    

</li>
            

            
            
            

            <!-- Check the GTN Video Library for recordings of this tutorial or associated slides -->
            












            
                
                <li class="btn btn-default supporting_material">






    <a href="#" class="dropdown-toggle" data-toggle="dropdown" aria-expanded="false" title="Where to run the tutorial">
        <i class="fas fa-globe" aria-hidden="true"></i><span class="visually-hidden">instances</span>&nbsp;Available on these Galaxies 
    </a>
    <ul class="dropdown-menu">
        
    
	<li class="dropdown-header">
		<b>Possibly Working</b>
	</li>
    
	<li>
		<a class="dropdown-item" href="https://usegalaxy.eu" title="">
			UseGalaxy.eu
		</a>
	</li>
    
	<li>
		<a class="dropdown-item" href="https://usegalaxy.org" title="">
			UseGalaxy.org
		</a>
	</li>
    
	<li>
		<a class="dropdown-item" href="https://usegalaxy.org.au" title="">
			UseGalaxy.org.au
		</a>
	</li>
    
	<li>
		<a class="dropdown-item" href="https://usegalaxy.fr" title="">
			UseGalaxy.fr
		</a>
	</li>
    
    

    </ul>

</li>
                
            
        </ul>

        <div><strong><i class="far fa-calendar" aria-hidden="true"></i> Published:</strong> Mar 11, 2025 </div>
        <div><strong><i class="far fa-calendar" aria-hidden="true"></i> Last modification:</strong> Mar 11, 2025 </div>
        <div><strong><i class="fas fa-balance-scale" aria-hidden="true"></i> License:</strong>
		
            Tutorial Content is licensed under
            
              <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
            
            The GTN Framework is licensed under <a rel="license" href="https://github.com/galaxyproject/training-material/blob/main/LICENSE.md">MIT</a>
        </div>
        

	
	

	
	
	<div><strong><i class="fas fa-code-commit" aria-hidden="true"></i><span class="visually-hidden">version</span> Revision:</strong> 1 </div>

    </blockquote>
    </section>

    <div class="container">
        <div class="row">
            <!-- sidebar, which will move to the top on a small screen -->
            <div class="col-sm-2 hide-when-printing">
                <nav id="toc" data-toggle="toc" class="sticky-top" aria-label="Table of Contents"></nav>
            </div>
            <div class="col-sm-10">
                 
                <section aria-label="Tutorial Notebook access information" id="tutorial-notebook">
                     
                     
                     
                     
                    
                        <blockquote class="comment">
                            <div class="box-title">Best viewed in a Jupyter Notebook</div>
                            <p>
                                This tutorial is best viewed in a Jupyter notebook! You can load this notebook one of the following ways<!--, or you can click to run it in MyBinder-->
                            </p>
                            <!--
                            <p><b>Launching on MyBinder</b>
                                Click here and the notebook will launch on MyBinder.org
                                <a href="https://mybinder.org/v2/gh/galaxyproject/training-material/main?filepath=training-material%2Ftopics%2Fstatistics%2Ftutorials%2Fintro-to-ml-with-python%2Ftutorial.md.ipynb" title="Launch notebook on MyBinder"><img src="https://mybinder.org/badge_logo.svg" alt="My Binder Logo"/></a>
                            </p>
                            -->
                            
                            <p><b>Run on the GTN with JupyterLite (in-browser computations)</b>
                                <ol>
                                    <li><a href="/training-material/jupyter/lab/index.html?path=statistics-intro-to-ml-with-python.ipynb">Click to Launch JupyterLite</a></li>
                                </ol>
                            </p>
                            
                            <p><b>Launching the notebook in Jupyter in Galaxy</b>
                                <ol>
                                    <li><a href="/training-material/faqs/galaxy/interactive_tools_jupyter_launch.html">Instructions to Launch JupyterLab</a></li>
                                    <li>Open a Terminal in JupyterLab with <b>File -> New -> Terminal</b></li>
                                    <li>Run <code>wget https://training.galaxyproject.org/training-material/topics/statistics/tutorials/intro-to-ml-with-python/statistics-intro-to-ml-with-python.ipynb</code></li>
                                    <li>Select the notebook that appears in the list of files on the left.</li>
                                </ol>
                            </p>
                            <p><b>Downloading the notebook</b>
                                <ol>
                                    <li>Right click one of these links: <a href="https://training.galaxyproject.org/training-material/topics/statistics/tutorials/intro-to-ml-with-python/statistics-intro-to-ml-with-python.ipynb">Jupyter Notebook (With Solutions)</a>, <a href="https://training.galaxyproject.org/training-material/topics/statistics/tutorials/intro-to-ml-with-python/statistics-intro-to-ml-with-python-course.ipynb">Jupyter Notebook (Without Solutions)</a></li>
                                    <li>Save Link As..</li>
                                </ol>
                            </p>
                        </blockquote>
                    
                </section>
                

                <section aria-label="Tutorial Content" id="tutorial-content">
                <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">reload_ext</span> <span class="n">autoreload</span>
<span class="o">%</span><span class="n">autoreload</span> <span class="mi">2</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="kn">import</span> <span class="n">matplotlib.pylab</span> <span class="k">as</span> <span class="n">pylab</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>

<span class="kn">from</span> <span class="n">warnings</span> <span class="kn">import</span> <span class="n">filterwarnings</span>

<span class="n">pylab</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="sh">'</span><span class="s">figure.figsize</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">rc</span><span class="p">(</span><span class="sh">"</span><span class="s">font</span><span class="sh">"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>


<span class="n">plt</span><span class="p">.</span><span class="nf">rc</span><span class="p">(</span><span class="sh">'</span><span class="s">xtick</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="sh">'</span><span class="s">medium</span><span class="sh">'</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="sh">'</span><span class="s">in</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">rc</span><span class="p">(</span><span class="sh">'</span><span class="s">xtick.major</span><span class="sh">'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">rc</span><span class="p">(</span><span class="sh">'</span><span class="s">xtick.minor</span><span class="sh">'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">rc</span><span class="p">(</span><span class="sh">'</span><span class="s">ytick</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="sh">'</span><span class="s">medium</span><span class="sh">'</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="sh">'</span><span class="s">in</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">rc</span><span class="p">(</span><span class="sh">'</span><span class="s">ytick.major</span><span class="sh">'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">rc</span><span class="p">(</span><span class="sh">'</span><span class="s">ytick.minor</span><span class="sh">'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
</code></pre></div></div>

<p>So far we have fitted our curves and doing so we have found the  best model explaining the point that we had for the fitting.</p>

<p>We also saw that we could choose different models according to how much the improvement obtained was worth the complexification of the model. But again we did it on the whole data that we had. We never really check how well our data was generalizing to points never seen before, or by how much the model we found was subject to outliers.</p>

<p>Machine learning procedures allow us to take those considerations into account. After highlighting the few caveats of the procedures we have used in the former notebook, we will introduce the foundation of the machine learning way to model.</p>

<p>More particularly we will see that the machine learning paradigm modifies the function to optimize that we have seen before by adding a penalty to covariables that generalize badly. We will also see that in a machine learning procedure, the generalization is approached by fitting and evaluating mutliple times your model on subset of your data.</p>

<p>The machine learning paradigm emphasizes the importance of building a general model that will be good at dealing with future, unknown, data points rather than being the best model on the data that we have now.</p>

<blockquote class="agenda">
  <div class="box-title agenda-title" id="agenda-1">Agenda</div>

  <p>In this tutorial, we will cover:</p>

<ol id="markdown-toc">
  <li><a href="#exploring-model-generalization-in-previous-methods" id="markdown-toc-exploring-model-generalization-in-previous-methods">Exploring model generalization in previous methods.</a>    <ol>
      <li><a href="#model-sensibility-to-a-few-particular-points" id="markdown-toc-model-sensibility-to-a-few-particular-points">Model sensibility to a few particular points.</a></li>
      <li><a href="#how-does-the-model-change-according-to-random-data-subsamples" id="markdown-toc-how-does-the-model-change-according-to-random-data-subsamples">How does the model change according to random data subsamples.</a></li>
      <li><a href="#splitting-data-and-regularization" id="markdown-toc-splitting-data-and-regularization">Splitting data and regularization.</a></li>
    </ol>
  </li>
  <li><a href="#regularization-in-the-case-of-ols-and-glm" id="markdown-toc-regularization-in-the-case-of-ols-and-glm">Regularization in the case of OLS and GLM</a></li>
  <li><a href="#the-machine-learning-framework" id="markdown-toc-the-machine-learning-framework">The machine learning framework</a></li>
  <li><a href="#ols-and-glm-regression-with-the-classical-ml-pipeline" id="markdown-toc-ols-and-glm-regression-with-the-classical-ml-pipeline">OLS and GLM regression with the classical ML pipeline</a>    <ol>
      <li><a href="#classical-ml-pipeline-ols-regression" id="markdown-toc-classical-ml-pipeline-ols-regression">classical ML pipeline OLS regression</a></li>
      <li><a href="#a-toy-model-to-visualize-logistic-regression" id="markdown-toc-a-toy-model-to-visualize-logistic-regression">A toy model to visualize logistic regression.</a></li>
      <li><a href="#classical-ml-pipeline-logistic-regression" id="markdown-toc-classical-ml-pipeline-logistic-regression">classical ML pipeline logistic regression</a></li>
      <li><a href="#imbalanced-dataset" id="markdown-toc-imbalanced-dataset">Imbalanced dataset</a></li>
    </ol>
  </li>
  <li><a href="#a-few-very-important-words-on-leakage" id="markdown-toc-a-few-very-important-words-on-leakage">A few VERY IMPORTANT words on leakage.</a></li>
  <li><a href="#exercise--logistic-regression" id="markdown-toc-exercise--logistic-regression">Exercise : logistic regression</a></li>
  <li><a href="#support-vector-machine" id="markdown-toc-support-vector-machine">Support Vector Machine</a>    <ol>
      <li><a href="#svm-for-classification" id="markdown-toc-svm-for-classification">SVM for Classification</a></li>
      <li><a href="#svm-for-regression" id="markdown-toc-svm-for-regression">SVM for Regression</a></li>
    </ol>
  </li>
  <li><a href="#decision-tree-modeling--a-new-loss-function-and-new-ways-to-do-regularization" id="markdown-toc-decision-tree-modeling--a-new-loss-function-and-new-ways-to-do-regularization">Decision tree modeling : a (new?) loss function and new ways to do regularization.</a>    <ol>
      <li><a href="#simple-decision-tree-for-classification" id="markdown-toc-simple-decision-tree-for-classification">Simple decision tree for classification.</a></li>
      <li><a href="#random-forest-in-classification" id="markdown-toc-random-forest-in-classification">Random Forest in classification.</a></li>
      <li><a href="#random-forest-in-regression" id="markdown-toc-random-forest-in-regression">Random Forest in regression.</a></li>
    </ol>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#classification-exercise--predicting-heart-disease-on-the-framingham-data-set" id="markdown-toc-classification-exercise--predicting-heart-disease-on-the-framingham-data-set">Classification exercise : predicting heart disease on the framingham data-set</a></li>
  <li><a href="#additionnal-regression-exercise--predicting-daily-maximal-temperature" id="markdown-toc-additionnal-regression-exercise--predicting-daily-maximal-temperature">Additionnal Regression exercise : predicting daily maximal temperature</a></li>
  <li><a href="#annexes" id="markdown-toc-annexes">Annexes</a>    <ol>
      <li><a href="#features-selection" id="markdown-toc-features-selection">Features selection</a></li>
    </ol>
  </li>
</ol>

</blockquote>

<h1 id="exploring-model-generalization-in-previous-methods">Exploring model generalization in previous methods.</h1>

<h2 id="model-sensibility-to-a-few-particular-points">Model sensibility to a few particular points.</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_h_n</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">data/Human_nuisance.csv</span><span class="sh">"</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">df_h_n</span><span class="p">.</span><span class="nf">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span> <span class="sh">"</span><span class="s">Breeding density(individuals per ha)</span><span class="sh">"</span><span class="p">:</span><span class="sh">"</span><span class="s">Breeding</span><span class="sh">"</span><span class="p">,</span>
                       <span class="sh">"</span><span class="s">Number of pedestrians per ha per min</span><span class="sh">"</span><span class="p">:</span><span class="sh">"</span><span class="s">N</span><span class="sh">"</span><span class="p">},</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">df_h_n</span><span class="p">.</span><span class="n">N</span><span class="p">,</span><span class="n">df_h_n</span><span class="p">.</span><span class="n">Breeding</span><span class="p">,</span><span class="sh">'</span><span class="s">ro</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Breeding density_h_n(individuals per ha)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Number of pedestrians per ha per min</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<!--<a href="./images/output_6_0.png" rel="noopener noreferrer"><img src="./images/output_6_0.png"  alt="png. "  width="702" height=532 loading="lazy"></a>-->

<p>Let’s get rid of the two last points. We could argue that they look fishy since they are the only two points that go up. Maybe they are driving the cubic fit?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">df_h_n</span><span class="p">.</span><span class="n">N</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span><span class="n">df_h_n</span><span class="p">.</span><span class="n">Breeding</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span><span class="sh">'</span><span class="s">ro</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Breeding density(individuals per ha)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Number of pedestrians per ha per min</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">statsmodels</span>
<span class="kn">import</span> <span class="n">statsmodels.api</span> <span class="k">as</span> <span class="n">sm</span>
<span class="kn">import</span> <span class="n">statsmodels.formula.api</span> <span class="k">as</span> <span class="n">smf</span>

<span class="n">list_co</span><span class="o">=</span><span class="p">[]</span><span class="c1">#list of covariable
</span>

<span class="n">df_nuisance</span><span class="o">=</span><span class="n">df_h_n</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">logLikelihoods</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>

    <span class="n">df_nuisance</span><span class="p">[</span><span class="sh">"</span><span class="s">N</span><span class="sh">"</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span><span class="o">=</span><span class="n">df_nuisance</span><span class="p">.</span><span class="n">N</span><span class="o">**</span><span class="n">i</span>
    
    <span class="n">list_co</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="sh">"</span><span class="s">N</span><span class="sh">"</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="p">)</span>

    <span class="c1">## create the model, without the last 2 points
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="p">.</span><span class="nf">ols</span><span class="p">(</span><span class="sh">"</span><span class="s">Breeding ~ </span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">list_co</span><span class="p">)</span> <span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">df_nuisance</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="p">:]</span> <span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">()</span><span class="c1">#we do the actual fit
</span>    
    <span class="n">models</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">list_co</span><span class="p">))</span>
    <span class="n">logLikelihoods</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="n">results</span><span class="p">.</span><span class="n">llf</span> <span class="p">)</span>
    
<span class="n">logLikelihoods</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[np.float64(-125.86306783486532),
 np.float64(-112.64380299806213),
 np.float64(-110.71030133123855),
 np.float64(-103.39762229283633)]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">scipy.stats</span> <span class="k">as</span> <span class="n">stats</span> 

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nf">len</span><span class="p">(</span><span class="n">logLikelihoods</span><span class="p">)):</span>
    <span class="c1">#calculating the pvalue for the LRT between the models
</span>    <span class="n">pval</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">stats</span><span class="p">.</span><span class="n">chi2</span><span class="p">.</span><span class="nf">cdf</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">logLikelihoods</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">logLikelihoods</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span><span class="mi">1</span><span class="p">)</span>
        
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">model {:&lt;10} - model {:&lt;12} : LRT p-value = {:.2e}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">models</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                                                                     <span class="n">models</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                                     <span class="n">pval</span><span class="p">))</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model N1         - model N1+N2        : LRT p-value = 2.72e-07
model N1+N2      - model N1+N2+N3     : LRT p-value = 4.92e-02
model N1+N2+N3   - model N1+N2+N3+N4  : LRT p-value = 1.31e-04
</code></pre></div></div>

<p>You see that the choice between quadratic and cubic is associated to a fairly high p-value here (~0.0492).</p>

<p>Let’s check how the model behaves:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="c1">## Quadratic model
</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">variables</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span> <span class="p">[</span><span class="sh">"</span><span class="s">N1+N2</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">N1+N2+N3</span><span class="sh">"</span><span class="p">]</span> <span class="p">):</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="p">.</span><span class="nf">ols</span><span class="p">(</span><span class="sh">"</span><span class="s">Breeding ~ </span><span class="sh">"</span><span class="o">+</span><span class="n">variables</span> <span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">df_nuisance</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">()</span><span class="c1">#we do the actual fit
</span>    
    <span class="n">predicted</span> <span class="o">=</span> <span class="n">results</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span> <span class="n">df_nuisance</span> <span class="p">)</span>
    
    <span class="c1">## computing goodness of fit metrics 
</span>    <span class="n">R2_2_h_n</span> <span class="o">=</span> <span class="nf">r2_score</span><span class="p">(</span><span class="n">df_nuisance</span><span class="p">.</span><span class="n">Breeding</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">predicted</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">MSE_2_h_n</span> <span class="o">=</span> <span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">df_nuisance</span><span class="p">.</span><span class="n">Breeding</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span><span class="n">predicted</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>

    <span class="n">R2_22_h_n</span> <span class="o">=</span> <span class="nf">r2_score</span><span class="p">(</span><span class="n">df_nuisance</span><span class="p">.</span><span class="n">Breeding</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:],</span> <span class="n">predicted</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:])</span>
    <span class="n">MSE_22_h_n</span> <span class="o">=</span> <span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">df_nuisance</span><span class="p">.</span><span class="n">Breeding</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:],</span> <span class="n">predicted</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:])</span>


    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span> <span class="n">x</span> <span class="o">=</span> <span class="n">df_nuisance</span><span class="p">.</span><span class="n">N</span> <span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">df_nuisance</span><span class="p">.</span><span class="n">Breeding</span> <span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">teal</span><span class="sh">'</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">df_h_n</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="sh">'</span><span class="s">orange</span><span class="sh">'</span><span class="p">]</span><span class="o">*</span><span class="mi">2</span> <span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">df_nuisance</span><span class="p">.</span><span class="n">N</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span><span class="n">predicted</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="p">,</span><span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">df_nuisance</span><span class="p">.</span><span class="n">N</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:],</span><span class="n">predicted</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span> <span class="p">,</span><span class="sh">'</span><span class="s">r--</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="n">variables</span><span class="o">+</span><span class="sh">'</span><span class="se">\n</span><span class="s">all but 2 : R2={0:.2f}, MSE={1:.2f}</span><span class="se">\n</span><span class="s"> last 2 : R2={2:.2f}, MSE={3:.2f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">R2_2_h_n</span><span class="p">,</span><span class="n">MSE_2_h_n</span><span class="p">,</span><span class="n">R2_22_h_n</span><span class="p">,</span><span class="n">MSE_22_h_n</span><span class="p">),</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
</code></pre></div></div>

<!--<a href="output_12_0.png" rel="noopener noreferrer"><img src="output_12_0.png"  alt="png. "   loading="lazy"></a>-->

<p>The cubic function is still overall better even on the points not used for the fitting (we actually kind of expected that).</p>

<h2 id="how-does-the-model-change-according-to-random-data-subsamples">How does the model change according to random data subsamples.</h2>

<p>We should check if this kind of behaviour where it becomes difficult to assert a good model is general or is it just because we decided to get rid of those two particular points. Let’s check with more random subsamples and something a bit more balanced between number of points for fitting and for checking : here two is bit low.</p>

<p>Just for memory’s sake, let’s fit all the data as we did before</p>

<p><em>Note : to run the code below you need to install pydotplus (!pip install pydotplus) if you don’t have it already</em></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">utils</span> <span class="kn">import</span> <span class="n">poly_fit_train_test</span>

<span class="n">cubic</span><span class="o">=</span><span class="p">[]</span>
<span class="n">quadratic</span><span class="o">=</span><span class="p">[]</span>

<span class="c1">## we reshape N to ake it compatible with the sklearn functions we use there
</span><span class="n">Nreshaped</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span> <span class="n">df_nuisance</span><span class="p">.</span><span class="n">N</span> <span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span><span class="c1"># here we are fitting our model and checking it on different random subsample of the data 
</span>    <span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span> <span class="mi">10</span><span class="o">**</span><span class="mi">4</span> <span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span> <span class="mi">1</span> <span class="p">,</span> <span class="mi">2</span> <span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span> <span class="p">)</span>
    
    <span class="n">cubic_metrics</span> <span class="o">=</span> <span class="nf">poly_fit_train_test</span><span class="p">(</span> <span class="n">Nreshaped</span><span class="p">,</span>
                                        <span class="n">df_nuisance</span><span class="p">.</span><span class="n">Breeding</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span><span class="p">,</span> <span class="n">deg</span> <span class="o">=</span> <span class="mi">3</span> <span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="c1">#this contain the fit and some scoring metric
</span>    <span class="n">quad_metrics</span>  <span class="o">=</span> <span class="nf">poly_fit_train_test</span><span class="p">(</span> <span class="n">Nreshaped</span><span class="p">,</span>
                                        <span class="n">df_nuisance</span><span class="p">.</span><span class="n">Breeding</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span><span class="p">,</span> <span class="n">deg</span> <span class="o">=</span> <span class="mi">2</span> <span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="n">cubic</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">cubic_metrics</span><span class="p">)</span>
    <span class="n">quadratic</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">quad_metrics</span><span class="p">)</span>
   
</code></pre></div></div>

<!--<a href="output_15_0.png" rel="noopener noreferrer"><img src="output_15_0.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_15_1.png" rel="noopener noreferrer"><img src="output_15_1.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_15_2.png" rel="noopener noreferrer"><img src="output_15_2.png"  alt="png. "   loading="lazy"></a>-->

<p>Let’s see what the R2 are between cubic and quadratic for many splitting of the dataset and for known and unknown data points.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cubic</span><span class="o">=</span><span class="p">[]</span>
<span class="n">quadratic</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span><span class="c1"># same as before but on way more different split 
</span>    <span class="n">temp3</span><span class="o">=</span><span class="nf">poly_fit_train_test</span><span class="p">(</span><span class="n">Nreshaped</span><span class="p">,</span><span class="n">df_nuisance</span><span class="p">.</span><span class="n">Breeding</span><span class="p">,</span><span class="n">deg</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="n">temp2</span><span class="o">=</span><span class="nf">poly_fit_train_test</span><span class="p">(</span><span class="n">Nreshaped</span><span class="p">,</span><span class="n">df_nuisance</span><span class="p">.</span><span class="n">Breeding</span><span class="p">,</span><span class="n">deg</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="nf">min</span><span class="p">(</span><span class="n">temp3</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span> <span class="ow">and</span> <span class="nf">min</span><span class="p">(</span><span class="n">temp2</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">cubic</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">temp3</span><span class="p">)</span>
        <span class="n">quadratic</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">temp2</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cubic_known</span> <span class="p">,</span> <span class="n">cubic_new</span> <span class="o">=</span>  <span class="nf">list</span><span class="p">(</span> <span class="nf">zip</span><span class="p">(</span> <span class="o">*</span><span class="n">cubic</span> <span class="p">)</span> <span class="p">)</span>
<span class="n">quadratic_known</span> <span class="p">,</span> <span class="n">quadratic_new</span> <span class="o">=</span>  <span class="nf">list</span><span class="p">(</span> <span class="nf">zip</span><span class="p">(</span> <span class="o">*</span><span class="n">quadratic</span> <span class="p">)</span> <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">hist</span><span class="p">([</span><span class="n">cubic_known</span><span class="p">,</span><span class="n">quadratic_known</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">cubic</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">quadratic</span><span class="sh">'</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Known</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">'</span><span class="s">best</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">R2</span><span class="sh">'</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">hist</span><span class="p">([</span><span class="n">cubic_new</span><span class="p">,</span><span class="n">quadratic_new</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">cubic</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">quadratic</span><span class="sh">'</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">New</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">'</span><span class="s">best</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">R2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<!--<a href="output_20_0.png" rel="noopener noreferrer"><img src="output_20_0.png"  alt="png. "   loading="lazy"></a>-->

<p>We see here that in most of those random cases the cubic model gives better prediction on the new data points. Yet all the outcome of those fitting are a bit different. How do we reconcile them?</p>

<h2 id="splitting-data-and-regularization">Splitting data and regularization.</h2>

<p><strong>Maybe then, what you would like to do is to find the model that is best at predicting new data point whatever the specific data you fit on is.</strong> You don’t want to underfit neither overfit and start modeling the noise of your data. You need to find a compromize. You will sometime hear people use the terms bias variance problem or the <strong>curse of dimensionality</strong> when refering to that problem.</p>

<p>The approach used for that is a mix of what’s called regularization, and splitting of your dataset. <strong>Regularization</strong>, as its name indicates has the ambition to smoothen your fit, to make sure that you don’t start to fit the noise in your data so you can be as general in your prediction as possible.  It does that by putting another layer of constraints on your covariables (features). That constraint on your covariable translates in either the objective function you want to maximize/minimize (by adding a term in your least square or your maximum likelihood), or by constraining the space of available models.</p>

<p>Whatever that regularization is, its strength is always optimized by looking at subsamples of the dataset.</p>

<p>It is a nice automated method for model exploration, generalization and testing, which for me really defines machine learning. All of this is related to something called the curse of dimensionality. <strong>And in any case, it relies on a splitting of your data set between at least a train and a test set</strong>.</p>

<!--<a href="image/Presentation1.png" rel="noopener noreferrer"><img src="image/Presentation1.png"  alt="presentation1. "   loading="lazy"></a>-->

<p>You need the test set to assess the actual generalization of your model. <strong>This test set should not be touched until the evaluation of your model.</strong> Ideally by then you are looking at a model which is both good on the train and the test set.</p>

<p>You can imagine that it is the noise that makes the coefficient in front of the 149th polynomial look very important (so big), because here by construction we know that a fit with a polynomial greater than 3 is going to fit the noise. So you should penalize big coefficients unless they are absolutely necessary. Here necessary is to be understood as necessary for understanding all the subsamples of your data.</p>

<h1 id="regularization-in-the-case-of-ols-and-glm">Regularization in the case of OLS and GLM</h1>

<p>In case of a Least Square fitting, you just have to add to your sum of squared errors a function that takes into account the parameters in front of your covariables. Looking at those equations you penalize weights that will take too much importance in the fitting, unless they are important in every substet of data that you fit on. We will see how those subsets are designed later on. By evaluating this new loss function on many subsets of the data we can perfom model comparison and choose model generalization, all at once.</p>

<blockquote class="comment">
  <div class="box-title comment-title" id="comment-6"><i class="far fa-comment-dots" aria-hidden="true" ></i> Comment</div>

  <p>we report here the formulas used in <code class="language-plaintext highlighter-rouge">scikit-learn</code> functions. Other libraries may have a different parameterization, but the concepts stay the same</p>

</blockquote>

<table>
  <tbody>
    <tr>
      <td>$\frac{1}{2n}\sum_i (y_i-f(\pmb X_i,\pmb{\beta}))^2 + \alpha\sum_{j}</td>
      <td>\beta_{j}</td>
      <td>$ , <strong>l1 regularization</strong> (Lasso) $\alpha$ being the weight that you put on that regularization</td>
    </tr>
  </tbody>
</table>

<p>$\sum_i (y_i-f(\pmb X_i,\pmb{\beta}))^2 + \alpha\sum_{j}\beta_{j}^{2}$ , <strong>l2 regularization</strong> (Ridge)</p>

<table>
  <tbody>
    <tr>
      <td>$\frac{1}{2n}\sum_i (y_i-f(\pmb X_i,\pmb{\beta}))^2 + \alpha\sum_{j}(\rho</td>
      <td>\beta_{j}</td>
      <td>+\frac{(1-\rho)}{2}\beta_{j}^{2})$ , <strong>elasticnet</strong></td>
    </tr>
  </tbody>
</table>

<p>For a deeper understanding of those notions, you may look at :</p>

<ul>
  <li>
    <p>https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net</p>
  </li>
  <li>
    <p>https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a</p>
  </li>
</ul>

<p>In case of a logistic regression you want to maximize your log likelihood which is now penalized by one of those functions:</p>

<table>
  <tbody>
    <tr>
      <td>$\sum_{i}log(p_{i}) - \alpha\sum_{j}</td>
      <td>\beta_{j}</td>
      <td>$ , <strong>l1 regularization</strong> (Lasso)</td>
    </tr>
  </tbody>
</table>

<p>$\sum_{i}log(p_{i}) - \frac{\alpha}{2}\sum_{j}\beta_{j}^{2}$ , <strong>l2 regularization</strong> (Ridge)</p>

<table>
  <tbody>
    <tr>
      <td>$\sum_{i}log(p_{i}) - \alpha\sum_{j}(\rho</td>
      <td>\beta_{j}</td>
      <td>+\frac{(1-\rho)}{2}\beta_{j}^{2})$ , <strong>elasticnet</strong></td>
    </tr>
  </tbody>
</table>

<p>Rule is : <strong>when you hypothesize that you have sparse features and so you believe that among all those features only a small subset is going to be interesting (but of course you don’t know which ones…) then you try to use the regularization that will tend to put more of your features at the zero weight (the l1 regularization) and so reduce the complexity of your model.</strong> This l1 norm that collapses non-important features to zero is another way to do feature selection.</p>

<p>Now, we need a way to find this coefficient $\alpha$ which will set the strength of our regularization. This parameter is called an <strong>hyperparameter</strong>, and cannot be found directly like the others, since even if it is part of a new model it serve a generalization purpose and so should not be found by optimization on our full dataset. To do that on top of our first splitting between train dataset and test dataset, we will need to perfom some more splitting of our train data set.</p>

<blockquote class="comment">
  <div class="box-title comment-title" id="comment-7"><i class="far fa-comment-dots" aria-hidden="true" ></i> Comment</div>

  <p>the polynomial number we were using before is also an hyperparameter and can be find by the same technic consisting of splitting our data. Later on we will see other hyperparameters that are either related to model choice or regularization or intrically both.</p>

</blockquote>

<p>Let’s apply this on a couple of new datasets</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="n">diabetes</span> <span class="o">=</span> <span class="nf">load_diabetes</span><span class="p">()</span>

<span class="n">df_diabetes</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">diabetes</span><span class="p">[</span><span class="sh">'</span><span class="s">data</span><span class="sh">'</span><span class="p">],</span>
                         <span class="n">columns</span><span class="o">=</span>  <span class="n">diabetes</span><span class="p">[</span><span class="sh">'</span><span class="s">feature_names</span><span class="sh">'</span><span class="p">])</span>

<span class="n">df_diabetes</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>sex</th>
      <th>bmi</th>
      <th>bp</th>
      <th>s1</th>
      <th>s2</th>
      <th>s3</th>
      <th>s4</th>
      <th>s5</th>
      <th>s6</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.038076</td>
      <td>0.050680</td>
      <td>0.061696</td>
      <td>0.021872</td>
      <td>-0.044223</td>
      <td>-0.034821</td>
      <td>-0.043401</td>
      <td>-0.002592</td>
      <td>0.019907</td>
      <td>-0.017646</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.001882</td>
      <td>-0.044642</td>
      <td>-0.051474</td>
      <td>-0.026328</td>
      <td>-0.008449</td>
      <td>-0.019163</td>
      <td>0.074412</td>
      <td>-0.039493</td>
      <td>-0.068332</td>
      <td>-0.092204</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.085299</td>
      <td>0.050680</td>
      <td>0.044451</td>
      <td>-0.005670</td>
      <td>-0.045599</td>
      <td>-0.034194</td>
      <td>-0.032356</td>
      <td>-0.002592</td>
      <td>0.002861</td>
      <td>-0.025930</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.089063</td>
      <td>-0.044642</td>
      <td>-0.011595</td>
      <td>-0.036656</td>
      <td>0.012191</td>
      <td>0.024991</td>
      <td>-0.036038</td>
      <td>0.034309</td>
      <td>0.022688</td>
      <td>-0.009362</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.005383</td>
      <td>-0.044642</td>
      <td>-0.036385</td>
      <td>0.021872</td>
      <td>0.003935</td>
      <td>0.015596</td>
      <td>0.008142</td>
      <td>-0.002592</td>
      <td>-0.031988</td>
      <td>-0.046641</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDRegressor</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>

<span class="c1"># the SGDRegressor from sklearn
# uses gradient descent to find the best fit with the 
# objective function modified with the regularization term
</span>
<span class="n">logalphas</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">coef_dict</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span> <span class="p">:</span> <span class="p">[],</span>
             <span class="sh">'</span><span class="s">val</span><span class="sh">'</span> <span class="p">:</span> <span class="p">[],</span>
             <span class="sh">'</span><span class="s">log-alpha</span><span class="sh">'</span> <span class="p">:</span> <span class="p">[]}</span>
<span class="n">r2</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">50</span><span class="p">):</span>

    <span class="n">reg_diabetes</span> <span class="o">=</span> <span class="nc">SGDRegressor</span><span class="p">(</span> <span class="n">penalty</span><span class="o">=</span><span class="sh">'</span><span class="s">l1</span><span class="sh">'</span> <span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span> <span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span> <span class="p">)</span>
    <span class="n">reg_diabetes</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span> <span class="n">df_diabetes</span> <span class="p">,</span> <span class="n">diabetes</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">]</span> <span class="p">)</span>
    
    <span class="n">logalphas</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log10</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">r2</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="nf">r2_score</span><span class="p">(</span> <span class="n">diabetes</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">]</span> <span class="p">,</span> <span class="n">reg_diabetes</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">df_diabetes</span><span class="p">)</span> <span class="p">)</span> <span class="p">)</span>
    
    <span class="n">coef_dict</span><span class="p">[</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">]</span> <span class="o">+=</span> <span class="nf">list</span><span class="p">(</span> <span class="n">df_diabetes</span><span class="p">.</span><span class="n">columns</span> <span class="p">)</span>
    <span class="n">coef_dict</span><span class="p">[</span><span class="sh">'</span><span class="s">val</span><span class="sh">'</span><span class="p">]</span> <span class="o">+=</span> <span class="nf">list</span><span class="p">(</span> <span class="n">reg_diabetes</span><span class="p">.</span><span class="n">coef_</span> <span class="p">)</span>
    <span class="n">coef_dict</span><span class="p">[</span><span class="sh">'</span><span class="s">log-alpha</span><span class="sh">'</span><span class="p">]</span> <span class="o">+=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">log10</span><span class="p">(</span><span class="n">alpha</span><span class="p">)]</span><span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">df_diabetes</span><span class="p">.</span><span class="n">columns</span> <span class="p">)</span>

<span class="n">coef_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">coef_dict</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">logalphas</span> <span class="p">,</span> <span class="n">r2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">log10( alpha )</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">R2</span><span class="sh">"</span><span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">lineplot</span><span class="p">(</span> <span class="n">x</span> <span class="o">=</span> <span class="sh">'</span><span class="s">log-alpha</span><span class="sh">'</span> <span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">'</span><span class="s">val</span><span class="sh">'</span> <span class="p">,</span> <span class="n">hue</span> <span class="o">=</span> <span class="sh">'</span><span class="s">name</span><span class="sh">'</span> <span class="p">,</span> <span class="n">data</span><span class="o">=</span> <span class="n">coef_df</span> <span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">"</span><span class="s">regression of diabetes data with an L1 regularization.</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div>

<!--<a href="output_28_1.png" rel="noopener noreferrer"><img src="output_28_1.png"  alt="png. "   loading="lazy"></a>-->

<blockquote class="question">
  <div class="box-title question-title" id="question-1"><i class="far fa-question-circle" aria-hidden="true" ></i> Question</div>
  <p>adapt the code above to generate this plot with an <strong>l2</strong> penalty. How do you interpret the difference?
This is great, but how do we choose which level of regularization we want ?</p>
  <blockquote class="solution">
    <div class="box-title solution-title" id="solution-1"><button class="gtn-boxify-button solution" type="button" aria-controls="solution-1" aria-expanded="true"><i class="far fa-eye" aria-hidden="true" ></i> <span>Solution</span><span class="fold-unfold fa fa-minus-square"></span></button></div>
    <p>Answer</p>
  </blockquote>

</blockquote>

<p>It is a general rule that <strong>as you decrease $\alpha$, the $R^2$ on the data used for the fit increase</strong>, i.e. you risk overfitting.</p>

<p>Consequently, we cannot choose the value of $\alpha$ parameter from the data used to fit alone; we call such a parameter an <strong>hyper-parameter</strong>.</p>

<p><strong>Question:</strong> what are other hyper-parameters we could optimize at this point?</p>

<p>In order to find the optimal value of an hyper-parameter, we can separate our data into:</p>
<ul>
  <li>a <strong>train set</strong> : used to fit the model</li>
  <li>a <strong>validation set</strong> : used to evaluate how our model perform on new data</li>
</ul>

<p>Here the $R^2$ stays fairly low even with little to no regularization, so overfitting is not that likely (I have checked, it is not).</p>

<p>Let’s look at another data-set where overfitting is an issue.</p>

<p><a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1043-4">Acharjee et al.2016</a> propose several -omic dataset which they used to predict and gain knowledge on various phenotypic traits in potatos.</p>

<p>Here, we will concentrate on the their transcriptomics dataset and the phenotypic trait of the potato coloration.</p>

<p>We have pre-selected and normalized the 200 most promising genes (out of ~15 000).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">data/potato_data.phenotypic.csv</span><span class="sh">"</span> <span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">Flesh Colour</span><span class="sh">"</span><span class="p">]</span>
<span class="n">y</span><span class="p">.</span><span class="nf">describe</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>count    86.000000
mean     24.054824
std      13.023169
min       6.887500
25%      12.664600
50%      24.278800
75%      31.305050
max      57.035100
Name: Flesh Colour, dtype: float64
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dfTT</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">data/potato_data.transcriptomic.top200norm.csv</span><span class="sh">"</span> <span class="p">,</span> <span class="n">index_col</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">dfTT</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span> <span class="n">dfTT</span> <span class="p">)</span>

<span class="n">I</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span> <span class="nf">range</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span> <span class="n">I</span> <span class="p">)</span> 

<span class="c1"># we will use the first 30 points to evaluate the model
</span><span class="n">X_valid</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span> <span class="n">I</span><span class="p">[:</span><span class="mi">30</span><span class="p">]</span> <span class="p">,</span> <span class="p">:]</span> 
<span class="n">y_valid</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">I</span><span class="p">[:</span><span class="mi">30</span><span class="p">]]</span>

<span class="c1"># we will use the rest to train the model
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span> <span class="n">I</span><span class="p">[</span><span class="mi">30</span><span class="p">:]</span> <span class="p">,</span> <span class="p">:]</span> 
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">I</span><span class="p">[</span><span class="mi">30</span><span class="p">:]]</span>


<span class="n">logalphas</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">r2_train</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">r2_valid</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">200</span><span class="p">):</span>

    <span class="n">reg_diabetes</span> <span class="o">=</span> <span class="nc">SGDRegressor</span><span class="p">(</span> <span class="n">penalty</span><span class="o">=</span><span class="sh">'</span><span class="s">l1</span><span class="sh">'</span> <span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span> <span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">2000</span> <span class="p">)</span>
    <span class="n">reg_diabetes</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span> <span class="n">X_train</span> <span class="p">,</span> <span class="n">y_train</span> <span class="p">)</span>
    
    <span class="n">logalphas</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log10</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">r2_train</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="nf">r2_score</span><span class="p">(</span> <span class="n">y_train</span> <span class="p">,</span> <span class="n">reg_diabetes</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="p">)</span> <span class="p">)</span>
    <span class="n">r2_valid</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="nf">r2_score</span><span class="p">(</span> <span class="n">y_valid</span> <span class="p">,</span> <span class="n">reg_diabetes</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_valid</span><span class="p">)</span> <span class="p">)</span> <span class="p">)</span>
    
<span class="c1">## plotting and reporting 
</span><span class="n">bestI</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">r2_valid</span><span class="p">)</span>
<span class="n">bestLogAlpha</span> <span class="o">=</span> <span class="n">logalphas</span><span class="p">[</span><span class="n">bestI</span><span class="p">]</span>
<span class="n">bestR2_valid</span> <span class="o">=</span> <span class="n">r2_valid</span><span class="p">[</span><span class="n">bestI</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">"</span><span class="s">best alpha : {:.2f} - validation R2 : {:.2f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="n">bestLogAlpha</span> <span class="p">,</span> <span class="n">bestR2_valid</span><span class="p">))</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span> <span class="n">logalphas</span><span class="p">,</span> <span class="n">r2_train</span> <span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">train set</span><span class="sh">'</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span> <span class="n">logalphas</span><span class="p">,</span> <span class="n">r2_valid</span> <span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">validation set</span><span class="sh">'</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span> <span class="p">[</span><span class="n">bestLogAlpha</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="n">bestR2_valid</span><span class="p">]</span>  <span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">log10( alpha )</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">R2</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

</code></pre></div></div>

<!--<a href="output_35_2.png" rel="noopener noreferrer"><img src="output_35_2.png"  alt="png. "   loading="lazy"></a>-->

<p>So now, with the help of a validation set, we can clearly see the phases :</p>
<ul>
  <li><strong>underfitting</strong> : for high $\alpha$, the performance is low for both the train and the validation set</li>
  <li><strong>overfitting</strong> : for low $\alpha$, the performance is high for the train set, and low for the validation set</li>
</ul>

<p>We want the equilibrium point between the two where performance is ideal for the validation set.</p>

<p><strong>Problem :</strong> if you run the code above several time, you will see that the optimal point varies due to the random assignation to train or validation set.</p>

<p>There exists a myriad of possible strategies to deal with that problem, such as repeating the above many times and taking the average of the results for instance.
Note also that this problem gets less important as the validation set size increases.</p>

<p>So now, on top of our earlier regression model, we have added :</p>

<ul>
  <li>an <strong>hyper-parameter</strong> : $\alpha$, the strength of the regularization term</li>
  <li>a <strong>validation strategy</strong> for our model in order to avoid overfitting</li>
</ul>

<p>That’s it, we are now in the world of Machine Learning.</p>

<h1 id="the-machine-learning-framework">The machine learning framework</h1>

<p>Machine Learning, in a sense, is procuppied with the problem of <strong>overfitting</strong>, or how much model perform on new data.</p>

<p>To that end, we begin by dividing our data into :</p>

<ul>
  <li><strong>train</strong> set : find the best model</li>
  <li><strong>test</strong> set  : give an honest evaluation of how the model perform on completely new data.</li>
</ul>

<p>The train set will be used to find the best model, with the <strong>best parameter and hyper-parameter values</strong>.</p>

<p>The test set will only be used at the very end, to report model performance.</p>

<p>However, as we have seen the hyper-parameter cannot be set directly from the data that was used to train the model, thus, we deploy a <strong>cross-validation strategy</strong> by further splitting the train set.</p>

<p>For example, consider one of the most common strategy : <strong>k-fold cross validation</strong></p>

<!--<a href="image/kfold.png" rel="noopener noreferrer"><img src="image/kfold.png"  alt="k-fold validation. "   loading="lazy"></a>-->

<p>In k-fold cross-validation, you split you data in $k$ subpart, called fold.</p>

<p>Then, for a given hyper-parameter values combination, you actually train $k$ model: each time you use a different fold for validation (and the remaining $k-1$ folds for training).</p>

<p>You then compute the average performance across all fold : this is the <strong>cross-validated performance</strong>.</p>

<p>If we code ourselves a naive version of this, it could look something like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## I make a helper function with tests a bunch for a given validation set of alpha values 
</span><span class="k">def</span> <span class="nf">testAlphas</span><span class="p">(</span> <span class="n">X_train</span> <span class="p">,</span> <span class="n">y_train</span> <span class="p">,</span> <span class="n">X_valid</span> <span class="p">,</span> <span class="n">y_valid</span> <span class="p">,</span> <span class="n">alphas</span><span class="p">):</span>
    <span class="n">r2_valid</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
        <span class="n">reg_diabetes</span> <span class="o">=</span> <span class="nc">SGDRegressor</span><span class="p">(</span> <span class="n">penalty</span><span class="o">=</span><span class="sh">'</span><span class="s">l1</span><span class="sh">'</span> <span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span> <span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">2000</span> <span class="p">)</span>
        <span class="n">reg_diabetes</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span> <span class="n">X_train</span> <span class="p">,</span> <span class="n">y_train</span> <span class="p">)</span>
        <span class="n">r2_valid</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="nf">r2_score</span><span class="p">(</span> <span class="n">y_valid</span> <span class="p">,</span> <span class="n">reg_diabetes</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_valid</span><span class="p">)</span> <span class="p">)</span> <span class="p">)</span>
        
    <span class="k">return</span> <span class="n">r2_valid</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">k</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1">## 4-fold
</span>
<span class="c1">## ugly initialization , we have to handle the fact that the number of sample is not perfectly divisible by 4
</span><span class="n">I</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span> <span class="p">(</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="p">((</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="n">k</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="p">)[:</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="p">)</span>

<span class="c1">## shuffle randomly
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span> <span class="n">I</span> <span class="p">)</span> 
<span class="nf">print</span><span class="p">(</span> <span class="n">I</span> <span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[2 3 0 3 0 0 0 2 2 0 3 1 0 1 2 1 0 3 1 2 2 1 2 3 3 2 1 3 1 0 1 2 1 1 1 3 3
 3 1 1 3 0 1 2 2 1 3 0 3 2 3 2 3 3 0 2 3 0 2 1 3 1 2 3 2 0 0 3 0 1 2 3 0 0
 0 0 2 0 1 0 2 1 1 1 2 0]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">200</span><span class="p">)</span>

<span class="n">cumulated_r2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span> <span class="n">alphas</span><span class="p">.</span><span class="n">shape</span> <span class="p">)</span>


<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>


<span class="k">for</span> <span class="n">fold</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>

    <span class="n">X_valid</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span> <span class="n">I</span><span class="o">==</span><span class="n">fold</span> <span class="p">,</span> <span class="p">:]</span> 
    <span class="n">y_valid</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">I</span><span class="o">==</span><span class="n">fold</span><span class="p">]</span>

    <span class="c1"># we will use the rest to train the model
</span>    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span> <span class="n">I</span><span class="o">!=</span><span class="n">fold</span> <span class="p">,</span> <span class="p">:]</span> 
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">I</span><span class="o">!=</span><span class="n">fold</span><span class="p">]</span>
    
    <span class="n">r2</span> <span class="o">=</span> <span class="nf">testAlphas</span><span class="p">(</span> <span class="n">X_train</span> <span class="p">,</span> <span class="n">y_train</span> <span class="p">,</span> <span class="n">X_valid</span> <span class="p">,</span> <span class="n">y_valid</span> <span class="p">,</span> <span class="n">alphas</span><span class="p">)</span>
    <span class="n">cumulated_r2</span> <span class="o">+=</span> <span class="n">r2</span>
    
    
    <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span> <span class="n">np</span><span class="p">.</span><span class="nf">log10</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span> <span class="p">,</span> <span class="n">r2</span> <span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">'</span><span class="s">fold </span><span class="sh">'</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">fold</span><span class="p">)</span> <span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">dotted</span><span class="sh">'</span> <span class="p">)</span>

<span class="c1">## cross-validated r2 is the average across each fold:
</span><span class="n">cross_valid_r2</span> <span class="o">=</span> <span class="n">cumulated_r2</span> <span class="o">/</span> <span class="n">k</span>

<span class="n">bestI</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">cross_valid_r2</span><span class="p">)</span>
<span class="n">bestLogAlpha</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log10</span><span class="p">(</span> <span class="n">alphas</span><span class="p">[</span><span class="n">bestI</span><span class="p">]</span> <span class="p">)</span>
<span class="n">bestR2_valid</span> <span class="o">=</span> <span class="n">cross_valid_r2</span><span class="p">[</span><span class="n">bestI</span><span class="p">]</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span> <span class="n">np</span><span class="p">.</span><span class="nf">log10</span><span class="p">(</span><span class="n">alphas</span><span class="p">),</span> <span class="n">cross_valid_r2</span> <span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">cross-validated r2</span><span class="sh">'</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span> <span class="p">[</span><span class="n">bestLogAlpha</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="n">bestR2_valid</span><span class="p">]</span>  <span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">log10( alpha )</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">R2</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">"</span><span class="s">best alpha : {:.2f} - cross-validated R2 : {:.2f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="n">bestLogAlpha</span> <span class="p">,</span> <span class="n">bestR2_valid</span><span class="p">))</span>
</code></pre></div></div>

<!--<a href="output_41_1.png" rel="noopener noreferrer"><img src="output_41_1.png"  alt="png. "   loading="lazy"></a>-->

<p>There, you can realize that now, for each possible value of our hyper-parameter we fit and evaluate not 1, but $k$ models, here 4.</p>

<p>So, for 200 values of $\alpha$, that means 200x4 = 800 models to fit and evaluate.</p>

<p>Now, consider that we have other hyper-parameters, such as the type of regularization (L1 or L2),
or the degree of the polynomial we consider, and now you understand why Machine Learning can quickly become  computationnaly intensive.</p>

<p>Finally, here is the proposed strategy as aschematic, just to reiterate</p>

<!--<a href="image/Presentation2.png" rel="noopener noreferrer"><img src="image/Presentation2.png"  alt="presentation2. "   loading="lazy"></a>-->

<p>For those kind of methods to work, since we are solicitting our dataset many times, you need to have quite a lot of points. Typically the number of points for our sparrow disturbance is too small to perform nicely this procedure.</p>

<h1 id="ols-and-glm-regression-with-the-classical-ml-pipeline">OLS and GLM regression with the classical ML pipeline</h1>

<h2 id="classical-ml-pipeline-ols-regression">classical ML pipeline OLS regression</h2>

<p>Let’s come back to the diabetes dataset:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_diabetes</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">diabetes</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">][:</span><span class="mi">5</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([151.,  75., 141., 206., 135.])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1">#always split your dataset and do the fitting on the training
# the train_test_split will split the data into train and test set
# let's keep 25% of the data aside for the final test
</span><span class="n">X_diabetes_train</span><span class="p">,</span> <span class="n">X_diabetes_test</span><span class="p">,</span> <span class="n">y_diabetes_train</span><span class="p">,</span> <span class="n">y_diabetes_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">df_diabetes</span><span class="p">,</span> 
                                                                                        <span class="n">diabetes</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">],</span>
                                                                                        <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
                                                                                        <span class="n">random_state</span><span class="o">=</span><span class="mi">1425</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">train set size</span><span class="sh">'</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">y_diabetes_train</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">test  set size</span><span class="sh">'</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">y_diabetes_test</span> <span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>train set size 331
test  set size 111
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDRegressor</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="n">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># while not strictly necessary for this dataset, 
# regression model normally need the X values to be properly scaled.
# this is something that was done under the hood by statsmodels, but with sklearn
# we have to handle it explicitely.
#
# NB : in that particular dataset the data is already scaled, 
#      but we wanted to show this important step anyway
</span>
<span class="c1"># in our model we will also consider polynomials for our features,
# the PolynomialFeatures object generates all polynomials of a given degree for us
</span>

<span class="n">pipeline_reg_diabetes</span><span class="o">=</span><span class="nc">Pipeline</span><span class="p">([(</span><span class="sh">'</span><span class="s">scalar</span><span class="sh">'</span><span class="p">,</span><span class="nc">StandardScaler</span><span class="p">()),</span>
                                <span class="p">(</span><span class="sh">'</span><span class="s">poly</span><span class="sh">'</span><span class="p">,</span><span class="nc">PolynomialFeatures</span><span class="p">(</span><span class="n">include_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)),</span>
                                <span class="p">(</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">,</span><span class="nc">SGDRegressor</span><span class="p">())])</span>



<span class="c1"># define the hyperparameters you want to test
# with the range over which you want it to be tested.
# Here the hyperparamters 
#   * the degree of the polynomials
#   * the form of regularization : l1 or l2
#   * the strength of regularization : alpha
</span><span class="n">grid_values</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">poly__degree</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
               <span class="sh">'</span><span class="s">model__penalty</span><span class="sh">'</span><span class="p">:[</span><span class="sh">'</span><span class="s">l1</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">l2</span><span class="sh">'</span><span class="p">],</span>
               <span class="sh">'</span><span class="s">model__alpha</span><span class="sh">'</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)}</span>


<span class="c1">#Feed the pipeline and set of values to the GridSearchCV with the 
# score over which the decision should be taken (ir R^2).
# and the cross-validation scheme, here the number of fold in a stratified k-fold strategy
</span><span class="n">grid_reg_diabetes_r2</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">pipeline_reg_diabetes</span><span class="p">,</span> 
                                    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">grid_values</span><span class="p">,</span> 
                                    <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">r2</span><span class="sh">'</span><span class="p">,</span> 
                                    <span class="n">cv</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                                    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Where the actual fit happens
#  the gridSearchCV object will go through each hyperparameter value combination
#  and fit + evaluate each fold, and averages the score across each fold
#
#  it then finds the combination that gave the best score and
#  use it to re-train a model with the whole train data
</span><span class="n">grid_reg_diabetes_r2</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_diabetes_train</span><span class="p">,</span> <span class="n">y_diabetes_train</span><span class="p">)</span>

<span class="c1">#get the best cross-validated score 
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best score (</span><span class="sh">'</span><span class="o">+</span><span class="n">grid_reg_diabetes_r2</span><span class="p">.</span><span class="n">scoring</span><span class="o">+</span><span class="sh">'</span><span class="s">): </span><span class="sh">'</span><span class="p">,</span>
      <span class="n">grid_reg_diabetes_r2</span><span class="p">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="c1"># print the best parameters
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best parameter :</span><span class="sh">'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">grid_reg_diabetes_r2</span><span class="p">.</span><span class="n">best_params_</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s"> {:&gt;20} : {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">))</span>


</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Grid best score (r2):  0.4850203038301172
Grid best parameter :
         model__alpha : 4.5348785081285845
       model__penalty : l1
         poly__degree : 2
CPU times: user 1.8 s, sys: 112 ms, total: 1.92 s
Wall time: 5.5 s
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get the equivalent score on the test set
</span><span class="n">y_diabetes_decision_fn_scores_r2</span><span class="o">=</span><span class="n">grid_reg_diabetes_r2</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_diabetes_test</span><span class="p">,</span>
                                                            <span class="n">y_diabetes_test</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best parameter (max. </span><span class="sh">'</span><span class="o">+</span><span class="n">grid_reg_diabetes_r2</span><span class="p">.</span><span class="n">scoring</span><span class="o">+</span><span class="sh">'</span><span class="s">) model on test: </span><span class="sh">'</span><span class="p">,</span>
      <span class="n">y_diabetes_decision_fn_scores_r2</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Grid best parameter (max. r2) model on test:  0.46179242410993626
</code></pre></div></div>

<p>One can also access the best model parameter:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># here is a recipe to get the name of the features, with their power 
</span>
<span class="n">best_reg</span> <span class="o">=</span> <span class="n">grid_reg_diabetes_r2</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">[</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">]</span>
<span class="n">poly</span>     <span class="o">=</span> <span class="n">grid_reg_diabetes_r2</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">[</span><span class="sh">'</span><span class="s">poly</span><span class="sh">'</span><span class="p">]</span>

<span class="n">coef_names</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">_</span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">df_diabetes</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">+</span><span class="sh">'</span><span class="s">^</span><span class="sh">'</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">poly</span><span class="p">.</span><span class="n">powers_</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span><span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">df_diabetes</span><span class="p">.</span><span class="n">columns</span><span class="p">))</span> <span class="k">if</span> <span class="n">poly</span><span class="p">.</span><span class="n">powers_</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">poly</span><span class="p">.</span><span class="n">powers_</span><span class="p">))</span> <span class="p">]</span> 

<span class="n">sorted_features</span><span class="o">=</span><span class="nf">sorted</span><span class="p">(</span> <span class="p">[(</span><span class="n">coef_names</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="nf">abs</span><span class="p">(</span><span class="n">best_reg</span><span class="p">.</span><span class="n">coef_</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">poly</span><span class="p">.</span><span class="n">powers_</span><span class="p">))]</span> <span class="p">,</span>
                       <span class="n">key</span><span class="o">=</span><span class="nf">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Important features</span><span class="sh">'</span><span class="p">)</span>

<span class="k">for</span> <span class="n">feature</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">sorted_features</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\t</span><span class="s">{:&gt;10}</span><span class="se">\t</span><span class="s">{:.3f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span><span class="n">weight</span><span class="p">)</span> <span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Important features
         sex^2	38.439
         bmi^1	22.800
          s5^1	22.339
          bp^1	8.965
          s3^1	5.780
          bp^2	5.150
         bmi^2	4.432
          s6^2	3.717
    age^1_sex^1	3.686
         age^2	2.912
         sex^1	2.112
          s1^2	0.720
         age^1	0.000
          s1^1	0.000
          s2^1	0.000
          s4^1	0.000
          s6^1	0.000
    age^1_bmi^1	0.000
    age^1_bp^1	0.000
    age^1_s1^1	0.000
    age^1_s2^1	0.000
    age^1_s3^1	0.000
    age^1_s4^1	0.000
    age^1_s5^1	0.000
    age^1_s6^1	0.000
    sex^1_bmi^1	0.000
    sex^1_bp^1	0.000
    sex^1_s1^1	0.000
    sex^1_s2^1	0.000
    sex^1_s3^1	0.000
    sex^1_s4^1	0.000
    sex^1_s5^1	0.000
    sex^1_s6^1	0.000
    bmi^1_bp^1	0.000
    bmi^1_s1^1	0.000
    bmi^1_s2^1	0.000
    bmi^1_s3^1	0.000
    bmi^1_s4^1	0.000
    bmi^1_s5^1	0.000
    bmi^1_s6^1	0.000
     bp^1_s1^1	0.000
     bp^1_s2^1	0.000
     bp^1_s3^1	0.000
     bp^1_s4^1	0.000
     bp^1_s5^1	0.000
     bp^1_s6^1	0.000
     s1^1_s2^1	0.000
     s1^1_s3^1	0.000
     s1^1_s4^1	0.000
     s1^1_s5^1	0.000
     s1^1_s6^1	0.000
          s2^2	0.000
     s2^1_s3^1	0.000
     s2^1_s4^1	0.000
     s2^1_s5^1	0.000
     s2^1_s6^1	0.000
          s3^2	0.000
     s3^1_s4^1	0.000
     s3^1_s5^1	0.000
     s3^1_s6^1	0.000
          s4^2	0.000
     s4^1_s5^1	0.000
     s4^1_s6^1	0.000
          s5^2	0.000
     s5^1_s6^1	0.000
</code></pre></div></div>

<p>And we can plot the model prediction against the data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## train prediction
</span><span class="n">y_diabetes_train_pred</span> <span class="o">=</span> <span class="n">grid_reg_diabetes_r2</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_diabetes_train</span><span class="p">)</span>
<span class="c1">## test prediction
</span><span class="n">y_diabetes_test_pred</span>  <span class="o">=</span> <span class="n">grid_reg_diabetes_r2</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_diabetes_test</span><span class="p">)</span>


<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span> <span class="n">y_diabetes_train_pred</span> <span class="p">,</span> <span class="n">y_diabetes_train</span> <span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span> <span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">train set</span><span class="sh">'</span>  <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span> <span class="n">y_diabetes_test_pred</span> <span class="p">,</span> <span class="n">y_diabetes_test</span> <span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span> <span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">test set</span><span class="sh">'</span>  <span class="p">)</span>
<span class="n">m</span><span class="p">,</span><span class="n">M</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">y_diabetes_train_pred</span><span class="p">)</span> <span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">y_diabetes_train_pred</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span> <span class="p">[</span><span class="n">m</span><span class="p">,</span><span class="n">M</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="n">m</span><span class="p">,</span><span class="n">M</span><span class="p">]</span> <span class="p">,</span> <span class="sh">'</span><span class="s">k--</span><span class="sh">'</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span> <span class="sh">'</span><span class="s">predicted values</span><span class="sh">'</span> <span class="p">)</span> 
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span> <span class="sh">'</span><span class="s">real values</span><span class="sh">'</span> <span class="p">)</span> 
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
</code></pre></div></div>

<!--<a href="output_52_1.png" rel="noopener noreferrer"><img src="output_52_1.png"  alt="png. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grid_reg_diabetes_r2</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">[</span><span class="sh">'</span><span class="s">scalar</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div>

<h2 id="a-toy-model-to-visualize-logistic-regression">A toy model to visualize logistic regression.</h2>

<p>Let’s imagine a simple case with 2 groups, and a single feature:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span> <span class="p">[</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">300</span><span class="p">)</span> <span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">300</span><span class="p">)</span><span class="o">+</span><span class="mi">4</span> <span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">300</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">300</span> <span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span> <span class="n">x</span><span class="o">=</span><span class="n">X1</span><span class="p">,</span><span class="n">hue</span> <span class="o">=</span> <span class="n">y</span> <span class="p">)</span>
</code></pre></div></div>

<!--<a href="output_55_1.png" rel="noopener noreferrer"><img src="output_55_1.png"  alt="png. "   loading="lazy"></a>-->

<p>We will use a logistic regression to model the relationship between the class and the feature.</p>

<p>Remember : <strong>Logistic regression does not model the class directly, but rather model the class probabilities</strong> (through the logit transform)</p>

<p>Let’s see how regularization affect the class probabilities found by our model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># do not forget to scale the data
</span><span class="n">X1_norm</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">().</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X1</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span> <span class="n">X1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">,</span> <span class="mi">1</span> <span class="p">))</span>

<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span> <span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span> <span class="n">X1_norm</span> <span class="p">,</span> <span class="n">y</span> <span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">y</span> <span class="p">)</span>

<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">]:</span>
    
    <span class="c1"># this implementation does not take alpha but rather C = 1/alpha
</span>    <span class="n">C</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">alpha</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">(</span> <span class="n">penalty</span> <span class="o">=</span> <span class="sh">'</span><span class="s">l2</span><span class="sh">'</span> <span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">C</span> <span class="p">)</span>
    <span class="n">lr</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X1_norm</span> <span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="n">proba</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span> <span class="p">,</span> <span class="n">proba</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">'</span><span class="s">alpha = {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
</code></pre></div></div>

<!--<a href="output_57_1.png" rel="noopener noreferrer"><img src="output_57_1.png"  alt="png. "   loading="lazy"></a>-->

<p>We can see that <strong>when $\alpha$ grows the probabilities evolve more smoothly</strong> ie. we have more regularization.</p>

<blockquote class="comment">
  <div class="box-title comment-title" id="comment-8"><i class="far fa-comment-dots" aria-hidden="true" ></i> Comment</div>

  <p>However, note that all the curves meet at the same point, corresponding to the 0.5 probability.</p>

</blockquote>

<p>This is nice, but <strong>our end-goal is to actually be able to predict the classes</strong>, and not just the probabilities.</p>

<p>Our task is not regression anymore, but rather <strong>classification</strong>.</p>

<p>So here, we do not evaluate the model using $R^2$ or log-likelihood, but a classification metric.</p>

<p>we will discuss a few of these metrics, and we will begin by the most common: <strong>Accuracy</strong></p>

<p>The Accuracy is the proportion of samples which were correctly classified (as either category).</p>

<p>More mathematically:</p>

\[Accuracy = \frac{TP + TN}{TP+FP+FN+TN}\]

<!--<a href="image/TPFP.png" rel="noopener noreferrer"><img src="image/TPFP.png"  alt="image/TPFP.png. "   loading="lazy"></a>-->
<p>Image credit wikipedia user Sharpr for svg version. original work by kakau in a png. Licensed under the <a href="https://creativecommons.org/licenses/by-sa/3.0/deed.en">Creative Commons Attribution-Share Alike 3.0 Unported license</a>.</p>

<ul>
  <li>TP : True Positive</li>
  <li>FP : False Positive</li>
  <li>TN : True Negative</li>
  <li>FN : False Negative</li>
</ul>

<p>So you can see that accuracy forces us to make a choice about the <strong>probability threshold we use predict categories</strong>.</p>

<p>0.5 is a common choice, and the default of the <code class="language-plaintext highlighter-rouge">predict</code> method:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X1_norm</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Accuracy with a threshold of 0.5 : </span><span class="si">{</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y_predicted</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span>  <span class="p">)</span>

<span class="n">pd</span><span class="p">.</span><span class="nf">crosstab</span><span class="p">(</span> <span class="n">y</span> <span class="p">,</span> <span class="n">y_predicted</span> <span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy with a threshold of 0.5 : 0.98
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>col_0</th>
      <th>0</th>
      <th>1</th>
    </tr>
    <tr>
      <th>row_0</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>292</td>
      <td>8</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5</td>
      <td>295</td>
    </tr>
  </tbody>
</table>
</div>

<p>But it can be useful to remember that this is only 1 choice among many:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X1_norm</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">threshold</span>
<span class="nf">print</span><span class="p">(</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Accuracy with a threshold of </span><span class="si">{</span><span class="n">threshold</span><span class="si">}</span><span class="s"> : </span><span class="si">{</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y_predicted</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span>  <span class="p">)</span>
<span class="n">pd</span><span class="p">.</span><span class="nf">crosstab</span><span class="p">(</span> <span class="n">y</span> <span class="p">,</span> <span class="n">y_predicted</span> <span class="p">)</span>
</code></pre></div></div>

<p><strong>Micro-exercise :</strong> modify the threhold in the code above.</p>
<ul>
  <li>in which direction should the threshold move to limit the number of False Positive ?</li>
  <li>for which application could that be useful ?</li>
</ul>

<h2 id="classical-ml-pipeline-logistic-regression">classical ML pipeline logistic regression</h2>

<p>Let’s build a logistic regression model that will be able to predict if a breast tumor is malignant or not</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="c1">#loading the dataset which is comprised in scikit learn already
</span><span class="n">data</span> <span class="o">=</span> <span class="nf">load_breast_cancer</span><span class="p">()</span>

<span class="c1">## we reduce the features because otherwise this problem is a bit too easy ;-)
</span><span class="n">m</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span> <span class="nf">map</span><span class="p">(</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">startswith</span><span class="p">(</span><span class="sh">"</span><span class="s">mean </span><span class="sh">"</span><span class="p">)</span> <span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">feature_names</span><span class="sh">"</span><span class="p">]</span> <span class="p">)</span> <span class="p">)</span>


<span class="n">X_cancer</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">data</span><span class="sh">'</span><span class="p">][:,</span><span class="n">m</span><span class="p">]</span>
<span class="n">y_cancer</span><span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">]</span>

<span class="c1">#making it into a dataframe
</span><span class="n">breast_cancer_df</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">,</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">feature_names</span><span class="sh">"</span><span class="p">][</span><span class="n">m</span><span class="p">])</span>

<span class="n">breast_cancer_df</span><span class="p">[</span><span class="sh">"</span><span class="s">target</span><span class="sh">"</span><span class="p">]</span><span class="o">=</span><span class="n">y_cancer</span>

<span class="n">breast_cancer_df</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">breast_cancer_df</span><span class="p">.</span><span class="n">target</span><span class="p">.</span><span class="nf">value_counts</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>target
0    357
1    212
Name: count, dtype: int64
</code></pre></div></div>

<ul>
  <li>357 benign samples</li>
  <li>212 malignant samples</li>
</ul>

<p>Here, all these covariables / features are defined on very different scales, for them to be treated fairly in their comparison you need to take that into account by scaling.</p>

<p>But first, let’s split the model into a train and a test dataset:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#split your data
</span>
<span class="c1"># stratify is here to make sure that you split keeping the repartition of labels unaffected
</span><span class="n">X_train_cancer</span><span class="p">,</span> <span class="n">X_test_cancer</span><span class="p">,</span> <span class="n">y_train_cancer</span><span class="p">,</span> <span class="n">y_test_cancer</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">,</span>
                                                                                <span class="n">y_cancer</span><span class="p">,</span>
                                                                                <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                                <span class="n">stratify</span><span class="o">=</span><span class="n">y_cancer</span><span class="p">)</span>


<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">fraction of class malignant in train</span><span class="sh">"</span><span class="p">,</span><span class="nf">sum</span><span class="p">(</span><span class="n">y_train_cancer</span><span class="p">)</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">y_train_cancer</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">fraction of class malignant in test </span><span class="sh">"</span><span class="p">,</span><span class="nf">sum</span><span class="p">(</span><span class="n">y_test_cancer</span><span class="p">)</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">y_test_cancer</span><span class="p">)</span> <span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">fraction of class malignant in full </span><span class="sh">"</span><span class="p">,</span><span class="nf">sum</span><span class="p">(</span><span class="n">y_cancer</span><span class="p">)</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">y_cancer</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fraction of class malignant in train 0.3732394366197183
fraction of class malignant in test  0.3706293706293706
fraction of class malignant in full  0.37258347978910367
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="c1">## create the pipeline of data handling :
## scaling, then logistic regression
</span>
<span class="c1">## in order for the LogisticRegression object takes solver="liblinear" argument
## in order to be able to test the l1 and l2 norm
## other solver exists, and are more or less performant / have different capabilities
</span><span class="n">pipeline_lr_cancer</span><span class="o">=</span><span class="nc">Pipeline</span><span class="p">([(</span><span class="sh">'</span><span class="s">scaler</span><span class="sh">'</span><span class="p">,</span><span class="nc">StandardScaler</span><span class="p">()),</span>
                             <span class="p">(</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">,</span><span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">solver</span> <span class="o">=</span> <span class="sh">'</span><span class="s">liblinear</span><span class="sh">'</span><span class="p">))])</span>

<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>

<span class="n">grid_values</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">model__C</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">),</span>
               <span class="sh">'</span><span class="s">model__penalty</span><span class="sh">'</span><span class="p">:[</span><span class="sh">'</span><span class="s">l1</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">l2</span><span class="sh">'</span><span class="p">]}</span>

<span class="n">grid_lr_cancer_acc</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">pipeline_lr_cancer</span><span class="p">,</span> 
                                  <span class="n">param_grid</span> <span class="o">=</span> <span class="n">grid_values</span><span class="p">,</span>
                                  <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">,</span>
                                  <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                  <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">grid_lr_cancer_acc</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_cancer</span><span class="p">,</span> <span class="n">y_train_cancer</span><span class="p">)</span><span class="c1">#train your pipeline
</span>
<span class="c1">#get the best cross-validated score 
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best score (</span><span class="sh">'</span><span class="o">+</span><span class="n">grid_lr_cancer_acc</span><span class="p">.</span><span class="n">scoring</span><span class="o">+</span><span class="sh">'</span><span class="s">): </span><span class="sh">'</span><span class="p">,</span>
      <span class="n">grid_lr_cancer_acc</span><span class="p">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="c1"># print the best parameters
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best parameter :</span><span class="sh">'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">grid_lr_cancer_acc</span><span class="p">.</span><span class="n">best_params_</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s"> {:&gt;20} : {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">))</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Grid best score (accuracy):  0.9436935704514363
Grid best parameter :
             model__C : 0.0210490414451202
       model__penalty : l2
CPU times: user 390 ms, sys: 4.67 ms, total: 394 ms
Wall time: 686 ms
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">1</span><span class="o">/</span><span class="mf">0.02</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>50.0
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># calculate the score of your trained pipeline on the test
</span><span class="n">cancer_acc</span> <span class="o">=</span> <span class="n">grid_lr_cancer_acc</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_test_cancer</span><span class="p">,</span><span class="n">y_test_cancer</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best parameter (max.</span><span class="sh">'</span><span class="o">+</span><span class="n">grid_lr_cancer_acc</span><span class="p">.</span><span class="n">scoring</span><span class="o">+</span><span class="sh">'</span><span class="s">) model on test: </span><span class="sh">'</span><span class="p">,</span> 
      <span class="n">cancer_acc</span><span class="p">)</span> 
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Grid best parameter (max.accuracy) model on test:  0.9370629370629371
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_test_cancer</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(143,)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># we can then access the coefficient of the model, to assess the importance of the different parameters:
</span>
<span class="n">w_lr_cancer</span><span class="o">=</span><span class="n">grid_lr_cancer_acc</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">[</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">].</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">sorted_features</span><span class="o">=</span><span class="nf">sorted</span><span class="p">(</span> <span class="nf">zip</span><span class="p">(</span> <span class="n">breast_cancer_df</span><span class="p">.</span><span class="n">columns</span> <span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span> <span class="n">w_lr_cancer</span> <span class="p">)</span> <span class="p">),</span>
                       <span class="n">key</span><span class="o">=</span><span class="nf">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
                       <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Features sorted per importance in discriminative process</span><span class="sh">'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">f</span><span class="p">,</span><span class="n">ww</span> <span class="ow">in</span> <span class="n">sorted_features</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">{:&gt;25}</span><span class="se">\t</span><span class="s">{:.3f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">ww</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Features sorted per importance in discriminative process
      mean concave points	0.497
              mean radius	0.442
           mean perimeter	0.441
                mean area	0.422
           mean concavity	0.399
             mean texture	0.373
         mean compactness	0.295
          mean smoothness	0.264
            mean symmetry	0.183
   mean fractal dimension	0.070
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">confusion_matrix</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>


<span class="n">y_cancer_test_pred</span> <span class="o">=</span> <span class="n">grid_lr_cancer_acc</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test_cancer</span><span class="p">)</span>

<span class="c1"># get the confusion matrix:
</span><span class="n">confusion_m_cancer</span> <span class="o">=</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">y_test_cancer</span><span class="p">,</span> <span class="n">y_cancer_test_pred</span><span class="p">)</span>

<span class="c1">## recipe to plot the confusion matrix : 
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">confusion_m_cancer</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy:{0:.3f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test_cancer</span><span class="p">,</span><span class="n">y_cancer_test_pred</span><span class="p">)))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">True label</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Predicted label</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div>

<!--<a href="output_76_1.png" rel="noopener noreferrer"><img src="output_76_1.png"  alt="png. "   loading="lazy"></a>-->

<p>So, with its default threshold of 0.5, this model tends to produce more False Positive (ie. benign cancer seen as malignant), than False Negative (ie. malignant cancer seen as benign).</p>

<p>Depending on the particular of the problem we are trying to solve, that may be a desirable outcome.</p>

<p>Whatever the case, it is always interesting to explore a bit more : we will plot how each possible threshold affect the True Positive Rate and the False Positive Rate (<strong>TPR and FPR</strong>) : this is the Receiver Operating Characteristic c urve (<strong>ROC curve</strong>)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.special</span> <span class="kn">import</span> <span class="n">expit</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">auc</span>

<span class="c1"># 1. the decision_function gives you the "score" for a point to be in a class
</span><span class="n">y_score_lr_cancer</span> <span class="o">=</span> <span class="n">grid_lr_cancer_acc</span><span class="p">.</span><span class="nf">decision_function</span><span class="p">(</span><span class="n">X_test_cancer</span><span class="p">)</span>

<span class="c1">#   for the logistic regression, that score is the logit, so can convert it back to 
#   probabilities with the expit function (which is the inverse of the logit)
</span><span class="n">y_proba_lr_cancer</span> <span class="o">=</span> <span class="nf">expit</span><span class="p">(</span><span class="n">y_score_lr_cancer</span> <span class="p">)</span>

<span class="c1"># 2. this calculates the ROC curve : TPR and FPR for each threshold of score
</span><span class="n">fpr_lr_cancer</span><span class="p">,</span> <span class="n">tpr_lr_cancer</span><span class="p">,</span> <span class="n">threshold_cancer</span> <span class="o">=</span> <span class="nf">roc_curve</span><span class="p">(</span><span class="n">y_test_cancer</span><span class="p">,</span> <span class="n">y_proba_lr_cancer</span><span class="p">)</span>

<span class="c1"># we find the point corresponding to a 0.5 theshold
</span><span class="n">keep</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span> <span class="n">threshold_cancer</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="p">)</span> <span class="p">)</span>

<span class="c1"># we compute the area under the ROC curve
</span><span class="n">roc_auc_lr_cancer</span> <span class="o">=</span> <span class="nf">auc</span><span class="p">(</span> <span class="n">fpr_lr_cancer</span><span class="p">,</span> <span class="n">tpr_lr_cancer</span> <span class="p">)</span>

<span class="c1"># 3. plotting 
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">1.01</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">1.01</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">fpr_lr_cancer</span><span class="p">,</span> <span class="n">tpr_lr_cancer</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">LogRegr ROC curve</span><span class="se">\n</span><span class="s"> (area = {:0.2f})</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">roc_auc_lr_cancer</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">fpr_lr_cancer</span><span class="p">[</span><span class="n">keep</span><span class="p">],</span> <span class="n">tpr_lr_cancer</span><span class="p">[</span><span class="n">keep</span><span class="p">],</span><span class="sh">'</span><span class="s">ro</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">threshold=0.5</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">False Positive Rate</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">True Positive Rate</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">ROC curve (logistic classifier)</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">'</span><span class="s">lower right</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">navy</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<!--<a href="output_78_0.png" rel="noopener noreferrer"><img src="output_78_0.png"  alt="png. "   loading="lazy"></a>-->

<p>So with this ROC curve, we can see how the model would behave on different thresholds.</p>

<p><strong>Question:</strong> we have marked the 0.5 threshold on the plot. Where would a higher threshold be on the curve?</p>

<p>You can see that when plotting the ROC curve, I have also computed its “Area Under the Curve” : 
indeed ROC AUC is another common metric when doing classification.</p>

<p>For now, let’s put this aside briefly to explore a very common problem in classification : imbalance</p>

<h2 id="imbalanced-dataset">Imbalanced dataset</h2>

<p>Let’s use the same small example as before, but now instead of 300 sample of each class, imagine we only have 30 of class 1:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span> <span class="p">[</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">300</span><span class="p">)</span> <span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span><span class="o">+</span><span class="mi">2</span> <span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">300</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">30</span> <span class="p">)</span>

<span class="c1"># do not forget to scale the data
</span><span class="n">X1_norm</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">().</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X1</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span> <span class="n">X1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">,</span> <span class="mi">1</span> <span class="p">))</span>

<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span> <span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span> <span class="n">x</span><span class="o">=</span><span class="n">X1</span><span class="p">,</span><span class="n">hue</span> <span class="o">=</span> <span class="n">y</span> <span class="p">,</span> <span class="n">ax</span> <span class="o">=</span><span class="n">ax</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>


<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span> <span class="n">X1_norm</span> <span class="p">,</span> <span class="n">y</span> <span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">y</span> <span class="p">)</span>

<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">]:</span>
    
    <span class="c1"># this implementation does not take alpham but rather C = 1/alpha
</span>    <span class="n">C</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">alpha</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">(</span> <span class="n">penalty</span> <span class="o">=</span> <span class="sh">'</span><span class="s">l2</span><span class="sh">'</span> <span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">C</span> <span class="p">)</span>
    <span class="n">lr</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X1_norm</span> <span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="n">proba</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">100</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span> <span class="p">,</span> <span class="n">proba</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">'</span><span class="s">alpha = {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">legend</span><span class="p">()</span>
</code></pre></div></div>

<!--<a href="output_82_1.png" rel="noopener noreferrer"><img src="output_82_1.png"  alt="png. "   loading="lazy"></a>-->

<p>You can see that now the point where the probability curves for different alpha converge is not 0.5 anymore…</p>

<p>Also, the probability says fairly low even at the right end of the plot.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_predicted</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X1_norm</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Accuracy with a threshold of 0.5 : </span><span class="si">{</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y_predicted</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span>  <span class="p">)</span>
<span class="n">pd</span><span class="p">.</span><span class="nf">crosstab</span><span class="p">(</span> <span class="n">y</span> <span class="p">,</span> <span class="n">y_predicted</span> <span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy with a threshold of 0.5 : 0.92
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>col_0</th>
      <th>0</th>
      <th>1</th>
    </tr>
    <tr>
      <th>row_0</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>298</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>25</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div>

<p>So, most of the class 1 samples are miss-classified (22/30), but we still get a very high accuracy…</p>

<p>That is because, by contruction, both the <strong>logistic regression and accuracy score do not differentiate False Positive and False Negative</strong>.</p>

<p>And the problem gets worse the more imbalance there is :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">recall_score</span>

<span class="c1">## RECALL = TP / (TP + FN)
</span>
<span class="n">recall_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">acc_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">imbalance_list</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">imbalance</span> <span class="ow">in</span> <span class="n">imbalance_list</span><span class="p">:</span>

    <span class="n">n0</span> <span class="o">=</span> <span class="mi">300</span>
    <span class="n">n1</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span> <span class="n">n0</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">imbalance</span><span class="p">)</span> <span class="p">)</span>
    <span class="k">if</span> <span class="n">n1</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">n1</span> <span class="o">=</span> <span class="mi">1</span>
    
    <span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span> <span class="p">[</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n0</span><span class="p">)</span> <span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n1</span><span class="p">)</span><span class="o">+</span><span class="mi">2</span> <span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">n0</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">n1</span> <span class="p">)</span>

    <span class="n">X1_norm</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">().</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X1</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span> <span class="n">X1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">,</span> <span class="mi">1</span> <span class="p">))</span>
    
    <span class="n">C</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">alpha</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">(</span> <span class="n">penalty</span> <span class="o">=</span> <span class="sh">'</span><span class="s">l2</span><span class="sh">'</span> <span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">C</span> <span class="p">)</span>
    <span class="n">lr</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X1_norm</span> <span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X1_norm</span><span class="p">)</span>
    
    <span class="n">recall_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="nf">recall_score</span><span class="p">(</span> <span class="n">y</span> <span class="p">,</span> <span class="n">y_predicted</span> <span class="p">)</span> <span class="p">)</span>
    <span class="n">acc_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y_predicted</span><span class="p">)</span> <span class="p">)</span>

        
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span> <span class="n">imbalance_list</span> <span class="p">,</span> <span class="n">acc_list</span> <span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span> <span class="n">imbalance_list</span> <span class="p">,</span> <span class="n">recall_list</span> <span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">recall</span><span class="sh">'</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">imbalance</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
</code></pre></div></div>

<!--<a href="output_86_1.png" rel="noopener noreferrer"><img src="output_86_1.png"  alt="png. "   loading="lazy"></a>-->

<p>So not only does the precision get worse, the <strong>accuracy actually gets higher as there is more imbalance!</strong></p>

<p>So the problem here may be 2-fold:</p>
<ul>
  <li>imbalance in our dataset skews the <strong>logistic regression</strong> toward a particular outcome</li>
  <li><strong>accuracy</strong> is not able to differenciate between False Positive and False Negative, and so it is <strong>blind to imbalance</strong></li>
</ul>

<p>Consequently, the solutions will have to come both from the model, and from the metric we are using.</p>

<p><strong>For the logistic regression</strong>:</p>
<ul>
  <li>we will re-weight sample according to their class frequency, so that they are more important during the fitting.</li>
  <li>in sklearn : <code class="language-plaintext highlighter-rouge">LogisticRegression( ... , class_weight='balanced')</code></li>
</ul>

<p><strong>For the metric</strong>, there exists several metrics which are sensitive to imbalance problems. 
Here we will introduce the <strong><a href="https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score">balanced accuracy</a></strong>:</p>

\[balanced\\_accuracy = 0.5*( \frac{TP}{TP+FN} + \frac{TN}{TN+FP} )\]

<blockquote class="comment">
  <div class="box-title comment-title" id="comment-9"><i class="far fa-comment-dots" aria-hidden="true" ></i> Comment</div>

  <p>Other scores you may want to look-up : <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score">average-precision score</a>, and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score">f1-score</a>, which are both linked to the precision/recall curve</p>

</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">balanced_accuracy_score</span>


<span class="k">def</span> <span class="nf">check_imbalance_effect</span><span class="p">(</span> <span class="n">imbalance_list</span> <span class="p">,</span> <span class="n">class_weight</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
    
    <span class="n">recall_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">balanced_acc_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">acc_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">imbalance</span> <span class="ow">in</span> <span class="n">imbalance_list</span><span class="p">:</span>

        <span class="n">n0</span> <span class="o">=</span> <span class="mi">300</span>
        <span class="n">n1</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span> <span class="n">n0</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">imbalance</span><span class="p">)</span> <span class="p">)</span>
        <span class="k">if</span> <span class="n">n1</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">n1</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span> <span class="p">[</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n0</span><span class="p">)</span> <span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n1</span><span class="p">)</span><span class="o">+</span><span class="mi">2</span> <span class="p">])</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">n0</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">n1</span> <span class="p">)</span>

        <span class="n">X1_norm</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">().</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X1</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span> <span class="n">X1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">,</span> <span class="mi">1</span> <span class="p">))</span>

        <span class="c1"># LR
</span>        <span class="n">lr</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">(</span> <span class="n">penalty</span> <span class="o">=</span> <span class="sh">'</span><span class="s">l2</span><span class="sh">'</span> <span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">1</span> <span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">)</span>
        <span class="n">lr</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X1_norm</span> <span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X1_norm</span><span class="p">)</span>

        <span class="n">recall_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="nf">recall_score</span><span class="p">(</span> <span class="n">y</span> <span class="p">,</span> <span class="n">y_predicted</span> <span class="p">)</span>  <span class="p">)</span>
        <span class="n">acc_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y_predicted</span><span class="p">)</span> <span class="p">)</span>
        <span class="n">balanced_acc_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="nf">balanced_accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y_predicted</span><span class="p">)</span> <span class="p">)</span>

    <span class="k">return</span> <span class="n">recall_list</span> <span class="p">,</span> <span class="n">acc_list</span> <span class="p">,</span> <span class="n">balanced_acc_list</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">imbalance_list</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span>

<span class="c1">### first, we see what happens without class_weight=None
</span>
<span class="n">recall_list</span> <span class="p">,</span> <span class="n">acc_list</span> <span class="p">,</span> <span class="n">balanced_acc_list</span> <span class="o">=</span> <span class="nf">check_imbalance_effect</span><span class="p">(</span> <span class="n">imbalance_list</span> <span class="p">,</span> 
                                                               <span class="n">class_weight</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span>
    
        
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span> <span class="n">imbalance_list</span> <span class="p">,</span> <span class="n">acc_list</span> <span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">accuracy - class_weight=None</span><span class="sh">'</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span> <span class="n">imbalance_list</span> <span class="p">,</span> <span class="n">recall_list</span> <span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">recall - class_weight=None</span><span class="sh">'</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span> <span class="n">imbalance_list</span> <span class="p">,</span> <span class="n">balanced_acc_list</span> <span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">balanced_accuracy - class_weight=None</span><span class="sh">'</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">imbalance</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">legend</span><span class="p">()</span>
<span class="c1">## now, with class weight 
</span>
<span class="n">recall_list</span> <span class="p">,</span> <span class="n">acc_list</span> <span class="p">,</span> <span class="n">balanced_acc_list</span> <span class="o">=</span> <span class="nf">check_imbalance_effect</span><span class="p">(</span> <span class="n">imbalance_list</span> <span class="p">,</span> 
                                                               <span class="n">class_weight</span> <span class="o">=</span> <span class="sh">'</span><span class="s">balanced</span><span class="sh">'</span><span class="p">)</span>
            
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span> <span class="n">imbalance_list</span> <span class="p">,</span> <span class="n">acc_list</span> <span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">accuracy - class_weight=balanced</span><span class="sh">'</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span> <span class="n">imbalance_list</span> <span class="p">,</span> <span class="n">recall_list</span> <span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">recall - class_weight=balanced</span><span class="sh">'</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span> <span class="n">imbalance_list</span> <span class="p">,</span> <span class="n">balanced_acc_list</span> <span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">balanced_accuracy - class_weight=balanced</span><span class="sh">'</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">imbalance</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">legend</span><span class="p">()</span>
</code></pre></div></div>

<!--<a href="output_89_1.png" rel="noopener noreferrer"><img src="output_89_1.png"  alt="png. "   loading="lazy"></a>-->

<p>So, the <strong>balanced accuracy</strong> is able to detect an imbalance problem.</p>

<p>Setting <code class="language-plaintext highlighter-rouge">class_weight='balanced'</code> in our logistic regression fixes the imbalance at the level of the model.</p>

<p><strong>micro-exercise</strong>:  re-explore the hyper-parameters of the logisitic regression for the cancer data-set, but this time, account for the imbalance between malignant and benign samples.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="c1">## create the pipeline of data handling :
## scaling, then logistic regression
</span>
<span class="c1">## in order for the LogisticRegression object takes solver="liblinear" argument
## in order to be able to test the l1 and l2 norm
## other solver exists, and are more or less performant / have different capabilities
</span><span class="n">pipeline_lr_cancer</span><span class="o">=</span><span class="nc">Pipeline</span><span class="p">([(</span><span class="sh">'</span><span class="s">scaler</span><span class="sh">'</span><span class="p">,</span><span class="nc">StandardScaler</span><span class="p">()),</span>
                             <span class="p">(</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">,</span><span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">solver</span> <span class="o">=</span> <span class="sh">'</span><span class="s">liblinear</span><span class="sh">'</span> <span class="p">))])</span>

<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>

<span class="n">grid_values</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">model__C</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">),</span>
               <span class="sh">'</span><span class="s">model__penalty</span><span class="sh">'</span><span class="p">:[</span><span class="sh">'</span><span class="s">l1</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">l2</span><span class="sh">'</span><span class="p">],</span>
               <span class="sh">'</span><span class="s">model__class_weight</span><span class="sh">'</span><span class="p">:[</span><span class="bp">None</span><span class="p">,</span><span class="sh">'</span><span class="s">balanced</span><span class="sh">'</span><span class="p">]}</span>

<span class="n">grid_lr_cancer_acc</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">pipeline_lr_cancer</span><span class="p">,</span> 
                                  <span class="n">param_grid</span> <span class="o">=</span> <span class="n">grid_values</span><span class="p">,</span>
                                  <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">balanced_accuracy</span><span class="sh">'</span><span class="p">,</span>
                                  <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                  <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">grid_lr_cancer_acc</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_cancer</span><span class="p">,</span> <span class="n">y_train_cancer</span><span class="p">)</span><span class="c1">#train your pipeline
</span>
<span class="c1">#get the best cross-validated score 
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best score (</span><span class="sh">'</span><span class="o">+</span><span class="n">grid_lr_cancer_acc</span><span class="p">.</span><span class="n">scoring</span><span class="o">+</span><span class="sh">'</span><span class="s">): </span><span class="sh">'</span><span class="p">,</span>
      <span class="n">grid_lr_cancer_acc</span><span class="p">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="c1"># print the best parameters
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best parameter :</span><span class="sh">'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">grid_lr_cancer_acc</span><span class="p">.</span><span class="n">best_params_</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s"> {:&gt;20} : {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">))</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Grid best score (balanced_accuracy):  0.9327313828588174
Grid best parameter :
             model__C : 0.09111627561154886
  model__class_weight : balanced
       model__penalty : l2
CPU times: user 722 ms, sys: 12.1 ms, total: 734 ms
Wall time: 1.41 s
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># calculate the score of your trained pipeline on the test
</span><span class="n">cancer_acc</span> <span class="o">=</span> <span class="n">grid_lr_cancer_acc</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_test_cancer</span><span class="p">,</span><span class="n">y_test_cancer</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best parameter (max.</span><span class="sh">'</span><span class="o">+</span><span class="n">grid_lr_cancer_acc</span><span class="p">.</span><span class="n">scoring</span><span class="o">+</span><span class="sh">'</span><span class="s">) model on test: </span><span class="sh">'</span><span class="p">,</span> 
      <span class="n">cancer_acc</span><span class="p">)</span> 
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Grid best parameter (max.balanced_accuracy) model on test:  0.9105870020964361
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># we can then access the coefficient of the model, to assess the importance of the different parameters:
</span>
<span class="n">w_lr_cancer</span><span class="o">=</span><span class="n">grid_lr_cancer_acc</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">[</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">].</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">sorted_features</span><span class="o">=</span><span class="nf">sorted</span><span class="p">(</span> <span class="nf">zip</span><span class="p">(</span> <span class="n">breast_cancer_df</span><span class="p">.</span><span class="n">columns</span> <span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span> <span class="n">w_lr_cancer</span> <span class="p">)</span> <span class="p">),</span>
                       <span class="n">key</span><span class="o">=</span><span class="nf">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
                       <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Features sorted per importance in discriminative process</span><span class="sh">'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">f</span><span class="p">,</span><span class="n">ww</span> <span class="ow">in</span> <span class="n">sorted_features</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">{:&gt;25}</span><span class="se">\t</span><span class="s">{:.3f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">ww</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Features sorted per importance in discriminative process
      mean concave points	0.836
             mean texture	0.733
              mean radius	0.688
           mean perimeter	0.672
                mean area	0.640
           mean concavity	0.577
          mean smoothness	0.511
         mean compactness	0.349
            mean symmetry	0.267
   mean fractal dimension	0.191
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">confusion_matrix</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>


<span class="n">y_cancer_test_pred</span> <span class="o">=</span> <span class="n">grid_lr_cancer_acc</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test_cancer</span><span class="p">)</span>

<span class="c1"># get the confusion matrix:
</span><span class="n">confusion_m_cancer</span> <span class="o">=</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">y_test_cancer</span><span class="p">,</span> <span class="n">y_cancer_test_pred</span><span class="p">)</span>

<span class="c1">## recipe to plot the confusion matrix : 
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">confusion_m_cancer</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy:{0:.3f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test_cancer</span><span class="p">,</span><span class="n">y_cancer_test_pred</span><span class="p">)))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">True label</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Predicted label</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div>

<!--<a href="output_95_1.png" rel="noopener noreferrer"><img src="output_95_1.png"  alt="png. "   loading="lazy"></a>-->

<p>If you want to use a GLM other than the logistic regression:
<a href="https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-regression">GLM in sklearn</a></p>

<h1 id="a-few-very-important-words-on-leakage">A few VERY IMPORTANT words on leakage.</h1>

<p>The most important part in all of the machine learning jobs that we have been presenting above, is that <strong>the data set on which you train and the data set on which you evaluate your model should be clearly separated</strong>(either the validation set when you do hyperparameter tunning, or test set for the final evaluation).</p>

<p>No information directly coming from your test or your validation should pollute your train set. If it does you <strong>loose your ablity to have a meaningful evaluation power.</strong></p>

<p>In general <strong>data leakage</strong> relates to every bits of information that you should not have access to in a real case scenario, being present in your training set.</p>

<p>Among those examples of data leakage you could count :</p>
<ul>
  <li><strong>using performance on the test set to decide which algorithm/hyperparameter to use</strong></li>
  <li>doing imputation or scaling before the train/test split</li>
  <li>inclusion of future data points in a time dependent or event dependent model.</li>
</ul>

<h1 id="exercise--logistic-regression">Exercise : logistic regression</h1>

<p>The framingham dataset links some patient features to their risk to develop a heart disease.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_heart</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">data/framingham.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">df_heart</span><span class="p">.</span><span class="nf">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># removing rows with NA values.
</span>
<span class="nf">print</span><span class="p">(</span><span class="n">df_heart</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">df_heart</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(3658, 16)
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>male</th>
      <th>age</th>
      <th>education</th>
      <th>currentSmoker</th>
      <th>cigsPerDay</th>
      <th>BPMeds</th>
      <th>prevalentStroke</th>
      <th>prevalentHyp</th>
      <th>diabetes</th>
      <th>totChol</th>
      <th>sysBP</th>
      <th>diaBP</th>
      <th>BMI</th>
      <th>heartRate</th>
      <th>glucose</th>
      <th>TenYearCHD</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>39</td>
      <td>4.0</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>195.0</td>
      <td>106.0</td>
      <td>70.0</td>
      <td>26.97</td>
      <td>80.0</td>
      <td>77.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>46</td>
      <td>2.0</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>250.0</td>
      <td>121.0</td>
      <td>81.0</td>
      <td>28.73</td>
      <td>95.0</td>
      <td>76.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>48</td>
      <td>1.0</td>
      <td>1</td>
      <td>20.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>245.0</td>
      <td>127.5</td>
      <td>80.0</td>
      <td>25.34</td>
      <td>75.0</td>
      <td>70.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>61</td>
      <td>3.0</td>
      <td>1</td>
      <td>30.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>225.0</td>
      <td>150.0</td>
      <td>95.0</td>
      <td>28.58</td>
      <td>65.0</td>
      <td>103.0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>46</td>
      <td>3.0</td>
      <td>1</td>
      <td>23.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>285.0</td>
      <td>130.0</td>
      <td>84.0</td>
      <td>23.10</td>
      <td>85.0</td>
      <td>85.0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<p>Implement a logistic regression pipeline to predict the column <code class="language-plaintext highlighter-rouge">'TenYearCHD'</code> (dependent variable : ten year risk of coronary heart disease) by adapting some of the code above.</p>

<p>Assess the performance of your model, and list the features by order of their importance.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">##separation in X and y
</span><span class="n">X_heart</span> <span class="o">=</span> <span class="n">df_heart</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span> <span class="n">columns</span> <span class="o">=</span> <span class="sh">"</span><span class="s">TenYearCHD</span><span class="sh">"</span> <span class="p">)</span>
<span class="n">y_heart</span> <span class="o">=</span> <span class="n">df_heart</span><span class="p">[</span> <span class="sh">"</span><span class="s">TenYearCHD</span><span class="sh">"</span> <span class="p">]</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<p>Solution - reading and setup data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r -20 solutions/solution_03_01.py
</span></code></pre></div></div>

<p>Solution - separate between train and test set</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 22-30 solutions/solution_03_01.py
</span></code></pre></div></div>

<p>Solution - pipeline creation and fitting</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 32-50 solutions/solution_03_01.py
</span></code></pre></div></div>

<p>Solution - evaluation on the test set</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 51-71 solutions/solution_03_01.py
</span></code></pre></div></div>

<p>Solution - plotting the ROC curve</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 72-96 solutions/solution_03_01.py
</span></code></pre></div></div>

<p>Solution - getting feature importance</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 97- solutions/solution_03_01.py
</span></code></pre></div></div>

<h1 id="support-vector-machine">Support Vector Machine</h1>

<h2 id="svm-for-classification">SVM for Classification</h2>

<h3 id="introduction">introduction</h3>

<p>“The basic principle of SVM is pretty simple. SVM aims at finding the ‘good’ threshold (hyperplane) to separate data from different classes. Conceptually it is very different from logistic regression where you maximize the log likelihood of the log odds function. <strong>With SVM you really look for an hyperplane that separates data and that’s it : there is no underlying hypothesis about probability distribution or anything else. It is very geometrical.</strong></p>

<p>So what’s a good threshold? Again it is going to depend on the metric you are interested in. But at least a good threshold should be linked to this biais variance trade off in the sense that it should offer flexibility to your model.</p>

<p>You can imagine that there is a quite a lot of hyperplanes separating data in your training set. You could stick your threshold right where the class 0 point closest to class 1 lies. But in that case it will be very far from the other class 0 points, which can be a problem. <strong>You could decide that your threshold should be right between the two closest extreme of your classes but that is going to be very sensitive to missclassified data or extreme events… Those points choosen as a reference to put your threshold are called support vectors.</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">10815657</span><span class="p">)</span>


<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>

<span class="c1"># case 1 
</span><span class="n">norm1</span><span class="o">=</span><span class="mf">0.2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span>
<span class="n">norm2</span><span class="o">=</span><span class="mf">0.8</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">+</span><span class="mf">2.5</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">norm1</span><span class="p">,[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span><span class="sh">'</span><span class="s">ro</span><span class="sh">'</span><span class="p">,</span><span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">norm2</span><span class="p">,[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span><span class="sh">'</span><span class="s">bo</span><span class="sh">'</span><span class="p">)</span>

<span class="n">min2</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span> <span class="n">norm2</span> <span class="p">)</span>
<span class="n">max1</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span> <span class="n">norm1</span> <span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">axvline</span><span class="p">(</span><span class="n">min2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">defined by the most extreme blue point</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">axvline</span><span class="p">(</span> <span class="p">(</span><span class="n">min2</span><span class="o">+</span><span class="n">max1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">-.</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">middle of extreme of two classes</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">'</span><span class="s">best</span><span class="sh">'</span><span class="p">)</span>


<span class="c1"># case 2
</span><span class="n">cauch</span><span class="o">=</span><span class="mf">0.8</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">standard_cauchy</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span>
<span class="n">norm</span><span class="o">=</span><span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">+</span><span class="mf">2.5</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">cauch</span><span class="p">,[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">10</span><span class="p">,</span><span class="sh">'</span><span class="s">ro</span><span class="sh">'</span><span class="p">,</span><span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">norm</span><span class="p">,[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span><span class="sh">'</span><span class="s">bo</span><span class="sh">'</span><span class="p">)</span>

<span class="n">min2</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span> <span class="n">norm</span> <span class="p">)</span>
<span class="n">max1</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span> <span class="n">cauch</span> <span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">axvline</span><span class="p">(</span><span class="n">min2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">defined by the most extreme blue point</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">axvline</span><span class="p">(</span> <span class="p">(</span><span class="n">min2</span><span class="o">+</span><span class="n">max1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">-.</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">middle of extreme of two classes</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">'</span><span class="s">best</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div>

<!--<a href="output_122_1.png" rel="noopener noreferrer"><img src="output_122_1.png"  alt="png. "   loading="lazy"></a>-->

<p>On the left panel, the two hyperplanes are valid separation but you can imagine that the plane defined by the most extreme blue point doesn’t leave much space for generalization</p>

<p>If your data are not linearly separable, like in the right panel, you need to be able to choose support vectors that are going to do some misclassification but for the greater good.</p>

<p>So, once again, you are confronted to a compromise. You should place your threshold somwhere that is globally best even though that would mean some miss-classification. We are back to our regularization problem and of course <strong>Support vector machine has a regularization parameter : C</strong>. The game now becomes placing your threshold right in the middle of points (support vectors) from  each classes that you have "chosen" to be general points for decision making : <strong>they don’t need to be the two closest points from different classes anymore. They need to be points where your hyperplane makes the least error differentiating classes.</strong></p>

<!--<a href="image/1920px-SVM_margin.png" rel="noopener noreferrer"><img src="image/1920px-SVM_margin.png"  alt="svm. "   loading="lazy"></a>-->

<p>Image source : image by wikipedia user Larhmam, distributed under a <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">CC BY-SA 4.0 license</a>.</p>

<table>
  <tbody>
    <tr>
      <td>So you want to maximize the margin separating the two classes. This margin is $\frac{2}{</td>
      <td> </td>
      <td>\pmb{w}</td>
      <td> </td>
      <td>}$. So you want to minimize $</td>
      <td> </td>
      <td>\pmb{w}</td>
      <td> </td>
      <td>$. The SVM loss function we want to minimize with respect to $\pmb{w}$ and $b$ is:</td>
    </tr>
  </tbody>
</table>

<p>$C\cdot\Sigma^{N}<em>{i=1}\zeta_i + \frac{1}{2}||\pmb{w}||^{2}$ subject to $\zeta_i \ge 0$ and $y</em>{i}(w^{T}x_{i}-b) \ge 1-\zeta_i$, where $\zeta_i = \Sigma^{N}<em>{i=1}max(0,1-y</em>{i}(\pmb{w}\cdot\pmb{x}_i-b))$</p>
<ul>
  <li>$y_i$ is $-1$ or $1$ depending on the class of the point $i$</li>
  <li>the class of point $\pmb{x}$ is determined by the SVM using the sign of $(\pmb{w}\cdot\pmb{x}-b)$ (ie, on which side of the $(\pmb{w}\cdot\pmb{x}-b)$ hyperplane we are).</li>
</ul>

<p>Note that you could also use a L1 regularization but it is not implemented in the function we are going to use.</p>

<p>Indeed if most of the data points are well separated in term of class on each side of the hyperplane then</p>
<ul>
  <li>most of the time $y_{k}(w^{T}x_{k}-b) \geq 1$ and so $max(0,1-y_{k}(w^{T}x_{k}-b)=0$ (that’s good for minimizing our loss function),</li>
  <li>and a few times $y_{k}(w^{T}x_{k}-b) \leq -1$ and so $max(0,1-y_{k}(w^{T}x_{k}-b) \geq 2$ (which is polluting our minimization of the loss function).</li>
</ul>

<p>You can see that there is a <a href="https://en.wikipedia.org/wiki/Dot_product">dot product</a> involved : in the case of a linear hyperplane this dot product is just the cartesian dot product that you probably use all the time. It allows you to calculate distances between points in that cartesian space or between points and hyperplanes. But you might be familiar with other scalar product : like for example when you proceed to a Fourier decomposition of a function. This particular scalar product acts on functions and so is not really of interest for us… But others exist.</p>

<p><strong>So in principle you could use other definitions of distance between points to answer that classification question</strong>. This is what non-linear SVM does and this is why you can choose different so called kernels as hyperparameters as we will see below :</p>

<p>$\overrightarrow{x_{i}}.\overrightarrow{x_{j}}$ : cartesian</p>

<p>$(\overrightarrow{x_{i}}.\overrightarrow{x_{j}})^{d}$ : polynomial degree d</p>

<table>
  <tbody>
    <tr>
      <td>$exp(-\gamma</td>
      <td> </td>
      <td>\overrightarrow{x_{i}}-\overrightarrow{x_{j}}</td>
      <td> </td>
      <td>^{2})$ : gaussian radial basis</td>
    </tr>
  </tbody>
</table>

<p>$tanh(\kappa\overrightarrow{x_{i}}.\overrightarrow{x_{j}}+c)$ : hyperbolic tangent</p>

<p>This is really powerful for classification but going non-linear by using a kernel trick prevents you from interpreting how your features are massaged to create this classifier… So, if you want interpretability and do science rather than engineering : keep it linear.</p>

<!--<a href="image/3d_svm.png" rel="noopener noreferrer"><img src="image/3d_svm.png"  alt="3d_svm. "   loading="lazy"></a>-->

<blockquote class="comment">
  <div class="box-title comment-title" id="comment-10"><i class="far fa-comment-dots" aria-hidden="true" ></i> Comment</div>

  <p>Even though SVM as nothing to do with probablities, we are going to transform the results of our classifier back to probabilities (using logistic regression…) to be able to draw a ROC curve. But again I insist, those are just useful transformations but has actually nothing to do with the technique.</p>

</blockquote>

<h3 id="toy-example-to-visualize-svmc">Toy example to visualize SVMC</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="n">X2</span><span class="p">,</span> <span class="n">y2</span> <span class="o">=</span> <span class="nf">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="p">(</span><span class="mi">250</span><span class="p">,</span><span class="mi">250</span><span class="p">),</span> <span class="n">centers</span><span class="o">=</span><span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X2</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X2</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">y2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">X2</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="nf">max</span><span class="p">(</span><span class="n">X2</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span><span class="o">+</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">X2</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="nf">max</span><span class="p">(</span><span class="n">X2</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span><span class="o">+</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<!--<a href="output_129_0.png" rel="noopener noreferrer"><img src="output_129_0.png"  alt="png. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">utils</span> <span class="kn">import</span> <span class="n">contour_SVM</span>

<span class="nf">contour_SVM</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span><span class="n">y2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">ker</span><span class="o">=</span><span class="sh">'</span><span class="s">linear</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<!--<a href="output_130_0.png" rel="noopener noreferrer"><img src="output_130_0.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_130_1.png" rel="noopener noreferrer"><img src="output_130_1.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_130_2.png" rel="noopener noreferrer"><img src="output_130_2.png"  alt="png. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">contour_SVM</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span><span class="n">y2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span><span class="n">ker</span><span class="o">=</span><span class="sh">'</span><span class="s">linear</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<!--<a href="output_131_0.png" rel="noopener noreferrer"><img src="output_131_0.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_131_1.png" rel="noopener noreferrer"><img src="output_131_1.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_131_2.png" rel="noopener noreferrer"><img src="output_131_2.png"  alt="png. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">contour_SVM</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span><span class="n">y2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ker</span><span class="o">=</span><span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span><span class="n">gam</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<!--<a href="output_132_0.png" rel="noopener noreferrer"><img src="output_132_0.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_132_1.png" rel="noopener noreferrer"><img src="output_132_1.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_132_2.png" rel="noopener noreferrer"><img src="output_132_2.png"  alt="png. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">contour_SVM</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span><span class="n">y2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ker</span><span class="o">=</span><span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span><span class="n">gam</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<!--<a href="output_133_0.png" rel="noopener noreferrer"><img src="output_133_0.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_133_1.png" rel="noopener noreferrer"><img src="output_133_1.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_133_2.png" rel="noopener noreferrer"><img src="output_133_2.png"  alt="png. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">contour_SVM</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span><span class="n">y2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ker</span><span class="o">=</span><span class="sh">'</span><span class="s">poly</span><span class="sh">'</span><span class="p">,</span><span class="n">deg</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>
<!--<a href="output_134_0.png" rel="noopener noreferrer"><img src="output_134_0.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_134_1.png" rel="noopener noreferrer"><img src="output_134_1.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_134_2.png" rel="noopener noreferrer"><img src="output_134_2.png"  alt="png. "   loading="lazy"></a>-->

<h3 id="svm-classifier-pipeline">SVM Classifier pipeline.</h3>

<p>In sklearn SVM classifier is implemented in a single <code class="language-plaintext highlighter-rouge">sklearn.svm.SVC</code> class,
but as we have seen depending on the kernel you have different parameters.</p>

<p>That means that when you specify the grid of hyper-parameters to explore, you could write something like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grid_values</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">model__kernel</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">linear</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">poly</span><span class="sh">'</span><span class="p">],</span>
                 <span class="sh">"</span><span class="s">model__C</span><span class="sh">"</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
                 <span class="sh">"</span><span class="s">model__degree</span><span class="sh">"</span><span class="p">:[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span>
                 <span class="sh">"</span><span class="s">model__gamma</span><span class="sh">"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)}</span>
</code></pre></div></div>

<p>That would work, however that means that even though parameter <code class="language-plaintext highlighter-rouge">gamma</code> is useless when the kernel is <code class="language-plaintext highlighter-rouge">'linear'</code>, the GridSearchCV will still test all the different combination of values.
On this example, rather than testing :</p>
<ul>
  <li>10 combinations for linear kernel (C)</li>
  <li>10*4 = 40 combinations for poly kernel (C and degree)</li>
  <li>10*10 = 100 combinations for rbf kernel (C and gamma)</li>
</ul>

<p>= 150 combinations to test.</p>

<p>You would test $3<em>10</em>4*10 = 1200$ combinations…</p>

<p>Let’s see how we can handle this smartly to avoid these useless computations.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="kn">from</span> <span class="n">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="c1">## set up the pipeline as usual
</span><span class="n">pipe</span> <span class="o">=</span> <span class="nc">Pipeline</span><span class="p">([(</span><span class="sh">'</span><span class="s">scalar</span><span class="sh">'</span><span class="p">,</span><span class="nc">StandardScaler</span><span class="p">()),</span>
                 <span class="p">(</span><span class="sh">"</span><span class="s">classifier</span><span class="sh">"</span><span class="p">,</span> <span class="nc">SVC</span><span class="p">())])</span>

<span class="c1"># the grid of parameter values is not a dictionnary, but now a list 
#  of dictionnaries : smaller grid to explore independently
</span>
<span class="c1"># for each of these grid, we can re-define locally the classifier
</span><span class="n">grid_param</span> <span class="o">=</span> <span class="p">[</span>  <span class="p">{</span><span class="sh">"</span><span class="s">classifier</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="nc">SVC</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="sh">'</span><span class="s">balanced</span><span class="sh">'</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">linear</span><span class="sh">'</span><span class="p">)],</span>
                 <span class="sh">"</span><span class="s">classifier__C</span><span class="sh">"</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)},</span>
                <span class="p">{</span><span class="sh">"</span><span class="s">classifier</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="nc">SVC</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="sh">'</span><span class="s">balanced</span><span class="sh">'</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">)],</span>
                 <span class="sh">"</span><span class="s">classifier__C</span><span class="sh">"</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
                 <span class="sh">"</span><span class="s">classifier__gamma</span><span class="sh">"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)},</span>
                <span class="p">{</span><span class="sh">"</span><span class="s">classifier</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="nc">SVC</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="sh">'</span><span class="s">balanced</span><span class="sh">'</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">poly</span><span class="sh">'</span><span class="p">)],</span>
                 <span class="sh">"</span><span class="s">classifier__C</span><span class="sh">"</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
                 <span class="sh">"</span><span class="s">classifier__degree</span><span class="sh">"</span><span class="p">:[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">]}]</span>
<span class="c1">## NB : we use probability = True in order to make ROC auc computation possible later on
</span>
<span class="n">grid_svm</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> 
                        <span class="n">grid_param</span><span class="p">,</span> 
                        <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                        <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">)</span> 

<span class="n">grid_svm</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_cancer</span><span class="p">,</span> <span class="n">y_train_cancer</span><span class="p">)</span>

<span class="c1">#get the best cross-validated score 
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best score (</span><span class="sh">'</span><span class="o">+</span><span class="n">grid_svm</span><span class="p">.</span><span class="n">scoring</span><span class="o">+</span><span class="sh">'</span><span class="s">): </span><span class="sh">'</span><span class="p">,</span>
      <span class="n">grid_svm</span><span class="p">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="c1"># print the best parameters
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best kernel    :</span><span class="sh">'</span> <span class="p">,</span> <span class="n">grid_svm</span><span class="p">.</span><span class="n">best_params_</span><span class="p">[</span><span class="sh">"</span><span class="s">classifier</span><span class="sh">"</span><span class="p">].</span><span class="n">kernel</span> <span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best parameter :</span><span class="sh">'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">grid_svm</span><span class="p">.</span><span class="n">best_params_</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s"> {:&gt;20} : {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">))</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Grid best score (accuracy):  0.9507250341997265
Grid best kernel    : rbf
Grid best parameter :
           classifier : SVC(class_weight='balanced', probability=True)
        classifier__C : 35.93813663804626
    classifier__gamma : 0.1
CPU times: user 488 ms, sys: 135 μs, total: 488 ms
Wall time: 3.51 s
</code></pre></div></div>

<p>Alternatively, the same as above could be done with multiple separate grid searchs. We would <strong>compare them by their cross-validated score</strong> : <code class="language-plaintext highlighter-rouge">grid.best_score_</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="kn">from</span> <span class="n">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="n">scoring_metric</span> <span class="o">=</span> <span class="sh">"</span><span class="s">accuracy</span><span class="sh">"</span>

<span class="c1"># linear kernel
</span><span class="n">pipe1</span> <span class="o">=</span> <span class="nc">Pipeline</span><span class="p">([(</span><span class="sh">'</span><span class="s">scalar</span><span class="sh">'</span><span class="p">,</span><span class="nc">StandardScaler</span><span class="p">()),</span>
                 <span class="p">(</span><span class="sh">"</span><span class="s">classifier</span><span class="sh">"</span><span class="p">,</span> <span class="nc">SVC</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="sh">'</span><span class="s">balanced</span><span class="sh">'</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">linear</span><span class="sh">'</span><span class="p">))])</span>
<span class="n">grid_param1</span> <span class="o">=</span> <span class="p">{</span> <span class="sh">"</span><span class="s">classifier__C</span><span class="sh">"</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)}</span>

<span class="n">grid_svm_linear</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">pipe1</span><span class="p">,</span> <span class="n">grid_param1</span><span class="p">,</span> 
                        <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring_metric</span><span class="p">)</span> 

<span class="c1"># rbf kernel
</span><span class="n">pipe2</span> <span class="o">=</span> <span class="nc">Pipeline</span><span class="p">([(</span><span class="sh">'</span><span class="s">scalar</span><span class="sh">'</span><span class="p">,</span><span class="nc">StandardScaler</span><span class="p">()),</span>
                 <span class="p">(</span><span class="sh">"</span><span class="s">classifier</span><span class="sh">"</span><span class="p">,</span> <span class="nc">SVC</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="sh">'</span><span class="s">balanced</span><span class="sh">'</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">))])</span>
<span class="n">grid_param2</span> <span class="o">=</span> <span class="p">{</span> <span class="sh">"</span><span class="s">classifier__C</span><span class="sh">"</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="p">,</span> 
               <span class="sh">"</span><span class="s">classifier__gamma</span><span class="sh">"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)}</span>

<span class="n">grid_svm_rbf</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">pipe2</span><span class="p">,</span> <span class="n">grid_param2</span><span class="p">,</span> 
                        <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring_metric</span><span class="p">)</span> 

<span class="c1"># poly kernel
</span><span class="n">pipe3</span> <span class="o">=</span> <span class="nc">Pipeline</span><span class="p">([(</span><span class="sh">'</span><span class="s">scalar</span><span class="sh">'</span><span class="p">,</span><span class="nc">StandardScaler</span><span class="p">()),</span>
                 <span class="p">(</span><span class="sh">"</span><span class="s">classifier</span><span class="sh">"</span><span class="p">,</span> <span class="nc">SVC</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="sh">'</span><span class="s">balanced</span><span class="sh">'</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">poly</span><span class="sh">'</span><span class="p">))])</span>
<span class="n">grid_param3</span> <span class="o">=</span> <span class="p">{</span> <span class="sh">"</span><span class="s">classifier__C</span><span class="sh">"</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="p">,</span> 
               <span class="sh">"</span><span class="s">classifier__degree</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">]}</span>

<span class="n">grid_svm_poly</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">pipe3</span><span class="p">,</span> <span class="n">grid_param3</span><span class="p">,</span> 
                        <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring_metric</span><span class="p">)</span> 



<span class="n">grid_svm_linear</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_cancer</span><span class="p">,</span> <span class="n">y_train_cancer</span><span class="p">)</span>
<span class="n">grid_svm_rbf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_cancer</span><span class="p">,</span> <span class="n">y_train_cancer</span><span class="p">)</span>
<span class="n">grid_svm_poly</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_cancer</span><span class="p">,</span> <span class="n">y_train_cancer</span><span class="p">)</span>


<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">linear Grid best score (</span><span class="sh">'</span><span class="o">+</span><span class="n">grid_svm_linear</span><span class="p">.</span><span class="n">scoring</span><span class="o">+</span><span class="sh">'</span><span class="s">): </span><span class="sh">'</span><span class="p">,</span><span class="n">grid_svm_linear</span><span class="p">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">   rbf Grid best score (</span><span class="sh">'</span><span class="o">+</span><span class="n">grid_svm_rbf</span><span class="p">.</span><span class="n">scoring</span><span class="o">+</span><span class="sh">'</span><span class="s">): </span><span class="sh">'</span>   <span class="p">,</span><span class="n">grid_svm_rbf</span><span class="p">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">  poly Grid best score (</span><span class="sh">'</span><span class="o">+</span><span class="n">grid_svm_poly</span><span class="p">.</span><span class="n">scoring</span><span class="o">+</span><span class="sh">'</span><span class="s">): </span><span class="sh">'</span>  <span class="p">,</span><span class="n">grid_svm_poly</span><span class="p">.</span><span class="n">best_score_</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>linear Grid best score (accuracy):  0.9366621067031463
   rbf Grid best score (accuracy):  0.9507250341997265
  poly Grid best score (accuracy):  0.9201641586867304
CPU times: user 544 ms, sys: 20.8 ms, total: 564 ms
Wall time: 3.51 s
</code></pre></div></div>

<p>In the end we get the same result.</p>

<blockquote class="comment">
  <div class="box-title comment-title" id="comment-11"><i class="far fa-comment-dots" aria-hidden="true" ></i> Comment</div>

  <p>This is also the approach we would take to decide between logistic regression, SVM, or other models.</p>

</blockquote>

<p>Anyhow, let’s look at the best model performance in more depth:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_cancer_test_score</span><span class="o">=</span><span class="n">grid_svm</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_test_cancer</span><span class="p">,</span><span class="n">y_test_cancer</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best parameter (max.</span><span class="sh">'</span><span class="o">+</span><span class="n">grid_svm</span><span class="p">.</span><span class="n">scoring</span><span class="o">+</span><span class="sh">'</span><span class="s">) model on test: </span><span class="sh">'</span><span class="p">,</span> <span class="n">y_cancer_test_score</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Grid best parameter (max.accuracy) model on test:  0.9090909090909091
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_cancer_pred_test</span><span class="o">=</span><span class="n">grid_svm</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test_cancer</span><span class="p">)</span>

<span class="n">confusion_m_cancer_SVMC</span> <span class="o">=</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">y_test_cancer</span><span class="p">,</span> <span class="n">y_cancer_pred_test</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">confusion_m_cancer_SVMC</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">True label</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Predicted label</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">test {} : {:.3f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">grid_svm</span><span class="p">.</span><span class="n">scoring</span> <span class="p">,</span> <span class="n">y_cancer_test_score</span><span class="p">))</span>
</code></pre></div></div>

<!--<a href="output_141_1.png" rel="noopener noreferrer"><img src="output_141_1.png"  alt="png. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">y_cancer_score_SVMC</span> <span class="o">=</span> <span class="n">grid_svm</span><span class="p">.</span><span class="nf">decision_function</span><span class="p">(</span><span class="n">X_test_cancer</span><span class="p">)</span>
<span class="n">fpr_SVMC_cancer</span><span class="p">,</span> <span class="n">tpr_SVMC_cancer</span><span class="p">,</span> <span class="n">thre_SVMC_cancer</span> <span class="o">=</span> <span class="nf">roc_curve</span><span class="p">(</span><span class="n">y_test_cancer</span><span class="p">,</span> <span class="n">y_cancer_score_SVMC</span><span class="p">)</span>
<span class="n">roc_auc_SVMC_cancer</span> <span class="o">=</span> <span class="nf">auc</span><span class="p">(</span><span class="n">fpr_SVMC_cancer</span><span class="p">,</span> <span class="n">tpr_SVMC_cancer</span><span class="p">)</span>

<span class="n">proba</span><span class="o">=</span><span class="nf">expit</span><span class="p">(</span><span class="n">thre_SVMC_cancer</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">proba</span><span class="p">)):</span>
    <span class="k">if</span> <span class="nf">abs</span><span class="p">(</span><span class="n">proba</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span><span class="o">&lt;</span><span class="mf">0.1</span><span class="p">:</span>
        <span class="n">keep</span><span class="o">=</span><span class="n">i</span>
        <span class="k">break</span>
        

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">1.00</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">1.01</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">fpr_SVMC_cancer</span><span class="p">,</span> <span class="n">tpr_SVMC_cancer</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">SVC ROC curve</span><span class="se">\n</span><span class="s"> (area = {:0.2f})</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">roc_auc_SVMC_cancer</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">fpr_SVMC_cancer</span><span class="p">[</span><span class="n">keep</span><span class="p">],</span> <span class="n">tpr_SVMC_cancer</span><span class="p">[</span><span class="n">keep</span><span class="p">],</span><span class="sh">'</span><span class="s">ro</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">threshold=0.5</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">False Positive Rate</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">True Positive Rate</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">ROC curve (SVM classifier)</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">'</span><span class="s">lower right</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">navy</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">)</span>
<span class="c1">#plt.axes().set_aspect('equal')
</span><span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<!--<a href="output_142_0.png" rel="noopener noreferrer"><img src="output_142_0.png"  alt="png. "   loading="lazy"></a>-->

<p>And here the kernel is RBF, so we have no coefficients to grab from the model for interpretation.</p>

<p>But, we still have some options.</p>

<p>For instance, we present here <a href="https://scikit-learn.org/stable/modules/permutation_importance.html">permutation feature importance</a>: it is the <strong>decrease in a model score when a single feature value is randomly shuffled</strong>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="kn">from</span> <span class="n">sklearn.inspection</span> <span class="kn">import</span> <span class="n">permutation_importance</span>

<span class="n">r</span> <span class="o">=</span> <span class="nf">permutation_importance</span><span class="p">(</span><span class="n">grid_svm</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">,</span> <span class="n">X_test_cancer</span><span class="p">,</span> <span class="n">y_test_cancer</span><span class="p">,</span>
                            <span class="n">n_repeats</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                            <span class="n">random_state</span><span class="o">=</span><span class="mi">132987</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">r</span><span class="p">.</span><span class="n">importances_mean</span><span class="p">.</span><span class="nf">argsort</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">r</span><span class="p">.</span><span class="n">importances_mean</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">r</span><span class="p">.</span><span class="n">importances_std</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">breast_cancer_df</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="o">&lt;</span><span class="mi">8</span><span class="si">}</span><span class="sh">"</span>
               <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">r</span><span class="p">.</span><span class="n">importances_mean</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span>
               <span class="sa">f</span><span class="sh">"</span><span class="s"> +/- </span><span class="si">{</span><span class="n">r</span><span class="p">.</span><span class="n">importances_std</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mean area0.091 +/- 0.022
mean concavity0.057 +/- 0.022
mean texture0.037 +/- 0.018
CPU times: user 7.66 s, sys: 0 ns, total: 7.66 s
Wall time: 7.66 s
</code></pre></div></div>

<p>Other methods exists to help analyse black-boxxy models, such as <a href="https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/model_agnostic/Iris%20classification%20with%20scikit-learn.html">SHAP</a></p>

<h2 id="svm-for-regression">SVM for Regression</h2>

<p>We could use the same algorithm for regression, with the only difference that this time instead of finding the hyperplane that is the farthest from the support, we find the hyperplane that is the closest from those support.</p>

<p>For example, on our diabete data set, just by replacing SVC by SVR. 
We just use the <code class="language-plaintext highlighter-rouge">'rbf'</code> kernel (the linear and poly have already been tested with our OLS model earlier).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="kn">from</span> <span class="n">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVR</span>
<span class="c1">## SVR for regression
</span>
<span class="n">pipeline_SVR</span><span class="o">=</span><span class="nc">Pipeline</span><span class="p">([(</span><span class="sh">'</span><span class="s">scalar</span><span class="sh">'</span><span class="p">,</span><span class="nc">StandardScaler</span><span class="p">()),</span>
                       <span class="p">(</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">,</span><span class="nc">SVR</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">))])</span>

<span class="n">grid_values</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">model__C</span><span class="sh">"</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
               <span class="sh">"</span><span class="s">model__gamma</span><span class="sh">"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">20</span><span class="p">)}</span>

<span class="c1">## don't forget to change the metric to one adapted for regression:
</span><span class="n">grid_SVR_diabetes</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">pipeline_SVR</span><span class="p">,</span> 
                                 <span class="n">param_grid</span> <span class="o">=</span> <span class="n">grid_values</span><span class="p">,</span> 
                                 <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">r2</span><span class="sh">'</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">grid_SVR_diabetes</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_diabetes_train</span><span class="p">,</span> <span class="n">y_diabetes_train</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best score (</span><span class="sh">'</span><span class="o">+</span><span class="n">grid_SVR_diabetes</span><span class="p">.</span><span class="n">scoring</span><span class="o">+</span><span class="sh">'</span><span class="s">): </span><span class="sh">'</span><span class="p">,</span> <span class="n">grid_SVR_diabetes</span><span class="p">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best parameter (max.</span><span class="sh">'</span><span class="o">+</span><span class="n">grid_SVR_diabetes</span><span class="p">.</span><span class="n">scoring</span><span class="o">+</span><span class="sh">'</span><span class="s">): </span><span class="sh">'</span><span class="p">,</span> <span class="n">grid_SVR_diabetes</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Grid best score (r2):  0.4972454469408552
Grid best parameter (max.r2):  {'model__C': np.float64(37.92690190732246), 'model__gamma': np.float64(0.026366508987303583)}
CPU times: user 829 ms, sys: 20.3 ms, total: 849 ms
Wall time: 3.44 s
</code></pre></div></div>

<p>If you remember, our OLS model was able to get an $R^2$ of $~0.52$.</p>

<p>So we gain a tiny bit of $R^2$, but we loose interpretability … not the best trade here.</p>

<p>Let’s still have a look at the model predictions :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_diabetes_test_score</span><span class="o">=</span><span class="n">grid_SVR_diabetes</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_diabetes_test</span><span class="p">,</span><span class="n">y_diabetes_test</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best parameter (max.</span><span class="sh">'</span><span class="o">+</span><span class="n">grid_SVR_diabetes</span><span class="p">.</span><span class="n">scoring</span><span class="o">+</span><span class="sh">'</span><span class="s">) model on test: </span><span class="sh">'</span><span class="p">,</span> <span class="n">y_diabetes_test_score</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Grid best parameter (max.r2) model on test:  0.4963096693856498
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## train prediction
</span><span class="n">y_diabetes_train_pred</span> <span class="o">=</span> <span class="n">grid_SVR_diabetes</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_diabetes_train</span><span class="p">)</span>
<span class="c1">## test prediction
</span><span class="n">y_diabetes_test_pred</span>  <span class="o">=</span> <span class="n">grid_SVR_diabetes</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_diabetes_test</span><span class="p">)</span>


<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span> <span class="n">y_diabetes_train_pred</span> <span class="p">,</span> <span class="n">y_diabetes_train</span> <span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span> <span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">train set</span><span class="sh">'</span>  <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span> <span class="n">y_diabetes_test_pred</span> <span class="p">,</span> <span class="n">y_diabetes_test</span> <span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span> <span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">test set</span><span class="sh">'</span>  <span class="p">)</span>
<span class="n">m</span><span class="p">,</span><span class="n">M</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">y_diabetes_train_pred</span><span class="p">)</span> <span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">y_diabetes_train_pred</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span> <span class="p">[</span><span class="n">m</span><span class="p">,</span><span class="n">M</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="n">m</span><span class="p">,</span><span class="n">M</span><span class="p">]</span> <span class="p">,</span> <span class="sh">'</span><span class="s">k--</span><span class="sh">'</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span> <span class="sh">'</span><span class="s">predicted values</span><span class="sh">'</span> <span class="p">)</span> 
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span> <span class="sh">'</span><span class="s">real values</span><span class="sh">'</span> <span class="p">)</span> 
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
</code></pre></div></div>

<!--<a href="output_151_1.png" rel="noopener noreferrer"><img src="output_151_1.png"  alt="png. "   loading="lazy"></a>-->

<!-- Exo : try a  linear kernel. Can you say anything about feature importance? How would you compare this new model to the former. %load  solutions/solution_03_mini.py -->

<h1 id="decision-tree-modeling--a-new-loss-function-and-new-ways-to-do-regularization">Decision tree modeling : a (new?) loss function and new ways to do regularization.</h1>

<h2 id="simple-decision-tree-for-classification">Simple decision tree for classification.</h2>

<p>A simple <strong>decision tree</strong> reduces your problem into a <strong>hierarchichal sequence of questions</strong> on your features that can be answered by yes or no and which subdivides the data into 2 subgroups on which a new question is asked, and so on and so on.</p>

<!--<a href="image/tree_ex.png" rel="noopener noreferrer"><img src="image/tree_ex.png"  alt="tree_ex. "   loading="lazy"></a>-->

<p>Ok, but a huge number of trees can actually be built just by considering the different orders of questions asked. How does the algorithm deals with this?</p>

<p>Quite simply actually: it <strong>tests all the features and chooses the most discriminative</strong> (with respect to your target variable) : the feature where a yes or no question divides the data into 2 subsets which minimizes an <strong>impurity measure</strong>.</p>

<p>Imagine you have a dataset with feature color (red or blue) and feature shape (square or circle), and 2 target classes : 1 and 2.</p>

<!--<a href="image/Tree.png" rel="noopener noreferrer"><img src="image/Tree.png"  alt="tree. "   loading="lazy"></a>-->

<p>Asking <code class="language-plaintext highlighter-rouge">"feature color is red"</code> gives you the following subgroups:</p>
<ul>
  <li>10 class 1, and 1 class 2 (<code class="language-plaintext highlighter-rouge">"feature color is red" == True</code>)</li>
  <li>2 class 1, and 11 class 2 (<code class="language-plaintext highlighter-rouge">"feature color is red" == False</code>)</li>
</ul>

<p>Asking <code class="language-plaintext highlighter-rouge">"feature shape is square"</code> gives you:</p>
<ul>
  <li>5 class 1, and 7 class 2 (<code class="language-plaintext highlighter-rouge">True</code>)</li>
  <li>7 class 1 and 5 class 2 (<code class="language-plaintext highlighter-rouge">False</code>)</li>
</ul>

<p>So, you will prefer asking <code class="language-plaintext highlighter-rouge">"feature color is red?"</code> over <code class="language-plaintext highlighter-rouge">"feature shape is square?"</code>: <code class="language-plaintext highlighter-rouge">"feature color is red?"</code> is more discriminative.</p>

<p>For <strong>categorical variables, the questions test for a specific category</strong>.
For <strong>numerical variables, the questions use a threshold</strong> to as a yes/no question.</p>

<p>The <strong>threshold is, again, chosen to minimize impurity</strong>. And in turn the best threshold for a variable is used to estimate the discriminativeness of that variable.</p>

<p>Of course, you will have to compute this threshold at each step of your tree since at each step you are considering different subdatasets.</p>

<hr />
<p>The <strong>impurity is related to how much your feature splitting is still having mixed classes</strong>. So the impurity ends up giving a score: either it is a simple <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Shannon entropy</a> or it is a <a href="https://en.wikipedia.org/wiki/Gini_coefficient">Gini coefficient</a>.</p>

<h3 id="shannon-entropy">Shannon Entropy</h3>

\[Entropy = - \sum\_{j} p\_j log\_2(p\_j)\]

<p>This measure is linked to information theory, where the information of an event occuring is the $log_2$ of this event’s probability of occuring.
For purity, <strong>0 is the best possible score, and 1 the worst</strong>.</p>

<h3 id="gini-coefficient">Gini coefficient</h3>

\[Gini = 1- \sum\_{j} p\_j^2\]

<p>The idea is to measure the <strong>probability that a dummy classifier mislabels your data</strong>: <strong>0</strong> is <strong>best, **1</strong> is worst.**</p>

<p>Before going further, just a little bit of vocabulary:</p>
<ul>
  <li><strong>Trees</strong> are made of <strong>nodes</strong> (where the question is asked and where the splitting occurs).</li>
  <li>A <strong>branch</strong> is the outcome of a splitting.</li>
  <li>A <strong>leaf</strong> is the last node on a branch (no more splitting).</li>
</ul>

<h3 id="toy-example-to-visualize-decision-tree">Toy example to visualize decision tree.</h3>

<p>Let explore some hyperparameters of this method that, you will see in those examples, act like a regularization:</p>
<ul>
  <li><strong>Max Tree depth</strong>: the maximum number of consecutive questions to ask</li>
  <li><strong>Min Splitting of nodes</strong>: minimum number of points to consider to make a new rule, outside of the leaves</li>
  <li><strong>Min Splitting of leaves</strong>: minimum number of points to consider to make a new rule, at the leaves</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_3</span><span class="p">,</span> <span class="n">y_3</span> <span class="o">=</span> <span class="nf">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">cluster_std</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]],</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X_3</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X_3</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">y_3</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">coolwarm</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<!--<a href="output_157_1.png" rel="noopener noreferrer"><img src="output_157_1.png"  alt="png. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="c1">## creating a decision tree with 1 parameter changed (more on that later)
</span><span class="n">tree</span> <span class="o">=</span> <span class="nc">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">tree</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_3</span><span class="p">,</span> <span class="n">y_3</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pd</span><span class="p">.</span><span class="nf">crosstab</span><span class="p">(</span> <span class="n">tree</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span> <span class="n">X_3</span> <span class="p">)</span> <span class="p">,</span> <span class="n">y_3</span> <span class="p">,</span> <span class="n">rownames</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">truth</span><span class="sh">'</span><span class="p">]</span> <span class="p">,</span> <span class="n">colnames</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">prediction</span><span class="sh">'</span><span class="p">]</span> <span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>prediction</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
    </tr>
    <tr>
      <th>truth</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>35</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5</td>
      <td>38</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>40</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.tree</span> <span class="kn">import</span> <span class="n">plot_tree</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">_</span> <span class="o">=</span> <span class="nf">plot_tree</span><span class="p">(</span> <span class="n">tree</span> <span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">]</span> <span class="p">,</span> 
               <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span> <span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span> <span class="p">,</span> <span class="n">impurity</span><span class="o">=</span><span class="bp">False</span> <span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</code></pre></div></div>

<!--<a href="output_160_0.png" rel="noopener noreferrer"><img src="output_160_0.png"  alt="png. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">utils</span> <span class="kn">import</span> <span class="n">contour_tree</span>
<span class="nf">contour_tree</span><span class="p">(</span><span class="n">X_3</span><span class="p">,</span> <span class="n">y_3</span><span class="p">,</span>
              <span class="n">crit</span> <span class="o">=</span> <span class="sh">'</span><span class="s">entropy</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">maxd</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
              <span class="n">min_s</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
              <span class="n">min_l</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
              <span class="n">max_f</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span>
<span class="c1">#You can see that there are 5 hyperparameters here. Let's see what they do and what they mean.
#I bet you can already guess it is going to be related to regularization....
# After X,y you have 
# * crit = 'entropy' which is one way to calculate impurity (you could also put gini here)
# * maxd : the max depth of your tree
# * min_s : the number of points that should be concerned by the making of a new rule (splitting of the nodes)
# * min_l : #of points that should be considered to make a final leaf classification
# * max_f maximum number of features to consider for making a new rule...
</span></code></pre></div></div>

<!--<a href="output_161_0.png" rel="noopener noreferrer"><img src="output_161_0.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_161_1.png" rel="noopener noreferrer"><img src="output_161_1.png"  alt="png. "   loading="lazy"></a>-->

<p>This is an incredibly complex model.</p>

<p>Please, note that since every node is a question asked on one particular feature and features are never directly compared, you don’t need scaling! This observation that each question always involves one feature at a time can be also seen in the way the boundaries between classes are made in the graph : there is no diagonal boundaries. You can only see lines parallel to the plot axes.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#using another impurity measurement
</span><span class="nf">contour_tree</span><span class="p">(</span><span class="n">X_3</span><span class="p">,</span> <span class="n">y_3</span><span class="p">,</span>
              <span class="n">crit</span> <span class="o">=</span> <span class="sh">'</span><span class="s">gini</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">maxd</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
              <span class="n">min_s</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
              <span class="n">min_l</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
              <span class="n">max_f</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<!--<a href="output_163_0.png" rel="noopener noreferrer"><img src="output_163_0.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_163_1.png" rel="noopener noreferrer"><img src="output_163_1.png"  alt="png. "   loading="lazy"></a>-->

<p>Still some overfitting but it is nice to see that the boundaries are different and that impurity calculations, even if very similar, are making a difference.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Imposing a limit for the depth of the tree : how many questions you ask (here set to 4)
</span><span class="nf">contour_tree</span><span class="p">(</span><span class="n">X_3</span><span class="p">,</span> <span class="n">y_3</span><span class="p">,</span>
              <span class="n">crit</span> <span class="o">=</span> <span class="sh">'</span><span class="s">entropy</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">maxd</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
              <span class="n">min_s</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">min_l</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">max_f</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<!--<a href="output_165_0.png" rel="noopener noreferrer"><img src="output_165_0.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_165_1.png" rel="noopener noreferrer"><img src="output_165_1.png"  alt="png. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">contour_tree</span><span class="p">(</span><span class="n">X_3</span><span class="p">,</span> <span class="n">y_3</span><span class="p">,</span>
              <span class="n">crit</span> <span class="o">=</span> <span class="sh">'</span><span class="s">entropy</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">maxd</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
              <span class="n">min_s</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
              <span class="n">min_l</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
              <span class="n">max_f</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span>
<span class="c1"># min_samples_leaf : 
#     it sets the minimal number of data points that the chain of rules should concern. 
</span>
<span class="c1"># eg. Do you really wish to create a whole new set of rules to explain 
# only one particular data point? 
</span></code></pre></div></div>

<!--<a href="output_166_0.png" rel="noopener noreferrer"><img src="output_166_0.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_166_1.png" rel="noopener noreferrer"><img src="output_166_1.png"  alt="png. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">contour_tree</span><span class="p">(</span><span class="n">X_3</span><span class="p">,</span> <span class="n">y_3</span><span class="p">,</span>
              <span class="n">crit</span> <span class="o">=</span> <span class="sh">'</span><span class="s">entropy</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">maxd</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
              <span class="n">min_s</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
              <span class="n">min_l</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
              <span class="n">max_f</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span>
<span class="c1"># Here it is the same as before but this time it applies to nodes instead of leaves
# This parameter is called min_samples_split and is set to 20 here.
</span></code></pre></div></div>

<!--<a href="output_167_0.png" rel="noopener noreferrer"><img src="output_167_0.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_167_1.png" rel="noopener noreferrer"><img src="output_167_1.png"  alt="png. "   loading="lazy"></a>-->

<p>There are 3 main advantages to this kind of methods:</p>
<ul>
  <li>it works with all types of feature</li>
  <li>you don’t need to rescale</li>
  <li>it already includes non linear fitting</li>
</ul>

<p>Moreover it is ‘easy’ to interpret.</p>

<p>But….(yes there is a but, there is no free lunch)</p>

<p>Even with all of those hyperparamaters <strong>they are still not great on new data (inaccuracy…).</strong></p>

<p>We will see that in the real data example below and we will see more powerful technics based on decision tree that are more costly but generalize better.</p>

<h3 id="single-decision-tree-pipeline">Single decision tree pipeline.</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## the different hyper parameters on the decision tree are quite related to the dataset size, 
##  both in number of columns (for max depth)
##  and in number of rows (for min sample split and min sample leaf)
</span>
<span class="n">X_train_cancer</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(426, 10)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="kn">from</span> <span class="n">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">grid_values</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">criterion</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">entropy</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">gini</span><span class="sh">'</span><span class="p">],</span>
               <span class="sh">'</span><span class="s">max_depth</span><span class="sh">'</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">X_train_cancer</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">2</span><span class="p">),</span>
               <span class="sh">'</span><span class="s">min_samples_split</span><span class="sh">'</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">X_train_cancer</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span><span class="mi">20</span><span class="p">),</span>
              <span class="sh">'</span><span class="s">min_samples_leaf</span><span class="sh">'</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">X_train_cancer</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">)}</span>

<span class="n">grid_tree_cancer</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="nc">DecisionTreeClassifier</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="sh">"</span><span class="s">balanced</span><span class="sh">"</span><span class="p">),</span> 
                                <span class="n">param_grid</span> <span class="o">=</span> <span class="n">grid_values</span><span class="p">,</span> 
                                <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">,</span>
                                <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">grid_tree_cancer</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_cancer</span><span class="p">,</span> <span class="n">y_train_cancer</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best parameter (max. accuracy): </span><span class="sh">'</span><span class="p">,</span> <span class="n">grid_tree_cancer</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best score (accuracy): </span><span class="sh">'</span><span class="p">,</span> <span class="n">grid_tree_cancer</span><span class="p">.</span><span class="n">best_score_</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Grid best parameter (max. accuracy):  {'criterion': 'entropy', 'max_depth': np.int64(9), 'min_samples_leaf': np.int64(11), 'min_samples_split': np.int64(2)}
Grid best score (accuracy):  0.9412859097127223
CPU times: user 3.6 s, sys: 64.5 ms, total: 3.67 s
Wall time: 8.84 s


/home/wandrille/Installed_software/anaconda3/envs/introML2024/lib/python3.11/site-packages/numpy/ma/core.py:2846: RuntimeWarning: invalid value encountered in cast
  _data = np.array(data, dtype=dtype, copy=copy,
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_cancer_test_score</span><span class="o">=</span><span class="n">grid_tree_cancer</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_test_cancer</span><span class="p">,</span><span class="n">y_test_cancer</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best parameter (max. accuracy) model on test: </span><span class="sh">'</span><span class="p">,</span> <span class="n">y_cancer_test_score</span><span class="p">)</span>

<span class="n">y_cancer_pred_test</span> <span class="o">=</span> <span class="n">grid_tree_cancer</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test_cancer</span><span class="p">)</span>

<span class="n">confusion_m_cancer_tree</span> <span class="o">=</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">y_test_cancer</span><span class="p">,</span> <span class="n">y_cancer_pred_test</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">confusion_m_cancer_tree</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">test {} : {:.3f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span> <span class="n">grid_tree_cancer</span><span class="p">.</span><span class="n">scoring</span> <span class="p">,</span> <span class="n">y_cancer_test_score</span> <span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">True label</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Predicted label</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<!--<a href="output_172_2.png" rel="noopener noreferrer"><img src="output_172_2.png"  alt="png. "   loading="lazy"></a>-->

<p>Feature importance can be retrieved from the tree:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w_tree</span><span class="o">=</span><span class="n">grid_tree_cancer</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">.</span><span class="n">feature_importances_</span>

<span class="n">sorted_features</span><span class="o">=</span><span class="nf">sorted</span><span class="p">([[</span><span class="n">breast_cancer_df</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="nf">abs</span><span class="p">(</span><span class="n">w_tree</span><span class="p">[</span><span class="n">i</span><span class="p">])]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">w_tree</span><span class="p">))],</span>
                       <span class="n">key</span><span class="o">=</span><span class="nf">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Features sorted per importance in discriminative process</span><span class="sh">'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">f</span><span class="p">,</span><span class="n">w</span> <span class="ow">in</span> <span class="n">sorted_features</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">{:&gt;25}</span><span class="se">\t</span><span class="s">{:.3f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">w</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Features sorted per importance in discriminative process
      mean concave points	0.741
                mean area	0.093
           mean perimeter	0.062
             mean texture	0.050
           mean concavity	0.020
          mean smoothness	0.018
   mean fractal dimension	0.016
              mean radius	0.000
         mean compactness	0.000
            mean symmetry	0.000
</code></pre></div></div>

<p>And we can even plot the model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.tree</span> <span class="kn">import</span> <span class="n">plot_tree</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="nf">plot_tree</span><span class="p">(</span> <span class="n">grid_tree_cancer</span><span class="p">.</span><span class="n">best_estimator_</span> <span class="p">,</span> 
          <span class="n">feature_names</span><span class="o">=</span><span class="n">breast_cancer_df</span><span class="p">.</span><span class="n">columns</span> <span class="p">,</span> 
          <span class="n">ax</span><span class="o">=</span><span class="n">ax</span> <span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span> <span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span> <span class="p">,</span> <span class="n">impurity</span><span class="o">=</span><span class="bp">False</span> <span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">best single decision tree</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<!--<a href="output_176_1.png" rel="noopener noreferrer"><img src="output_176_1.png"  alt="png. "   loading="lazy"></a>-->

<h2 id="random-forest-in-classification">Random Forest in classification.</h2>

<p>the Random Forest algorithm relies on two main concepts :</p>
<ol>
  <li><strong>randomly producing/training many different trees</strong></li>
  <li><strong>agglomerating the predictions</strong> of all these trees (mainly averaging)</li>
</ol>

<p>The randomness between trees concerns:</p>
<ul>
  <li><strong><a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrapping</a> of the training dataset</strong></li>
  <li>using only a <strong>random subset of features</strong></li>
</ul>

<p><strong>Bootstrapping:</strong> sampling methods in which you randomly draw a subsample from your data, <em>with replacement</em>. The created replicate is the same size as the original distribution.</p>

<p>I am sure you can see intuitively how that is going to help generalization of our model.</p>

<p>So now on top of all the parameters seen before to create each individual trees of the forest, you also have a parameter controlling the number of trees in your forest.</p>

<!--<a href="image/RF.png" rel="noopener noreferrer"><img src="image/RF.png"  alt="RF. "   loading="lazy"></a>-->
<p><strong>In the following plots I am plotting the result for a random forest algorithm and compare it to a single decision tree sharing the same hyperparameters value than the one used in the random forest</strong>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">utils</span> <span class="kn">import</span> <span class="n">contour_RF</span>

<span class="nf">contour_RF</span><span class="p">(</span><span class="n">X_3</span><span class="p">,</span><span class="n">y_3</span><span class="p">,</span><span class="n">n_tree</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> 
            <span class="n">crit</span> <span class="o">=</span> <span class="sh">'</span><span class="s">gini</span><span class="sh">'</span><span class="p">,</span> <span class="n">maxd</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span><span class="n">min_s</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">min_l</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">max_f</span> <span class="o">=</span> <span class="sh">'</span><span class="s">sqrt</span><span class="sh">'</span><span class="p">)</span>
<span class="c1">#Same as for decision tree except that we have here one more hyperparameter, here
# put to 100 and that represents the number of bootstraps 
# (number of trees trained and then participating to the vote)
</span>
<span class="c1"># also, we restrict the number of variables given to each tree to 
# the square root of the original number of variables -&gt;  max_f = 'sqrt'
</span></code></pre></div></div>

<!--<a href="output_179_0.png" rel="noopener noreferrer"><img src="output_179_0.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_179_2.png" rel="noopener noreferrer"><img src="output_179_2.png"  alt="png. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="c1">##### Now we use RandomForestClassifier
</span><span class="n">grid_values</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">criterion</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">entropy</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">gini</span><span class="sh">'</span><span class="p">],</span>
               <span class="sh">'</span><span class="s">n_estimators</span><span class="sh">'</span><span class="p">:[</span><span class="mi">250</span><span class="p">,</span><span class="mi">500</span><span class="p">],</span> 
               <span class="sh">'</span><span class="s">max_depth</span><span class="sh">'</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">X_train_cancer</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">2</span><span class="p">),</span>
               <span class="sh">'</span><span class="s">min_samples_split</span><span class="sh">'</span><span class="p">:[</span><span class="mi">4</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">16</span><span class="p">],</span>
              <span class="sh">'</span><span class="s">min_samples_leaf</span><span class="sh">'</span><span class="p">:[</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">8</span><span class="p">]}</span>

<span class="n">grid_RF_cancer</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="sh">'</span><span class="s">balanced</span><span class="sh">'</span><span class="p">),</span> 
                              <span class="n">param_grid</span> <span class="o">=</span> <span class="n">grid_values</span><span class="p">,</span> 
                              <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">,</span>
                              <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                              <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">grid_RF_cancer</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_cancer</span><span class="p">,</span> <span class="n">y_train_cancer</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best score (</span><span class="sh">'</span><span class="o">+</span><span class="n">grid_RF_cancer</span><span class="p">.</span><span class="n">scoring</span><span class="o">+</span><span class="sh">'</span><span class="s">): </span><span class="sh">'</span><span class="p">,</span> <span class="n">grid_RF_cancer</span><span class="p">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best parameter (max. </span><span class="sh">'</span><span class="o">+</span><span class="n">grid_RF_cancer</span><span class="p">.</span><span class="n">scoring</span><span class="o">+</span><span class="sh">'</span><span class="s">): </span><span class="sh">'</span><span class="p">,</span> <span class="n">grid_RF_cancer</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Grid best score (accuracy):  0.9389603283173734
Grid best parameter (max. accuracy):  {'criterion': 'entropy', 'max_depth': np.int64(6), 'min_samples_leaf': 2, 'min_samples_split': 4, 'n_estimators': 500}
CPU times: user 1.76 s, sys: 98.7 ms, total: 1.86 s
Wall time: 1min 34s
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_test_cancer_score</span> <span class="o">=</span> <span class="n">grid_RF_cancer</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_test_cancer</span><span class="p">,</span><span class="n">y_test_cancer</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best parameter (max. </span><span class="sh">'</span><span class="o">+</span><span class="n">grid_RF_cancer</span><span class="p">.</span><span class="n">scoring</span><span class="o">+</span><span class="sh">'</span><span class="s">) model on test: </span><span class="sh">'</span><span class="p">,</span> <span class="n">y_test_cancer_score</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Grid best parameter (max. accuracy) model on test:  0.9370629370629371
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_test_RF_cancer</span><span class="o">=</span><span class="n">grid_RF_cancer</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test_cancer</span><span class="p">)</span>

<span class="n">confusion_m_RF_cancer</span> <span class="o">=</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">y_test_cancer</span><span class="p">,</span> <span class="n">y_pred_test_RF_cancer</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">confusion_m_RF_cancer</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">RF - {} : {:.3f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span> <span class="n">grid_RF_cancer</span><span class="p">.</span><span class="n">scoring</span> <span class="p">,</span><span class="n">y_test_cancer_score</span>  <span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">True label</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Predicted label</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<!--<a href="output_182_1.png" rel="noopener noreferrer"><img src="output_182_1.png"  alt="png. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_importance_RF_cancer</span> <span class="o">=</span> <span class="n">grid_RF_cancer</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">.</span><span class="n">feature_importances_</span>

<span class="c1">## by gathering the importance accross each individual tree, we can access 
## the standard deviation of this importance
</span><span class="n">std_RF_cancer</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">([</span><span class="n">tree</span><span class="p">.</span><span class="n">feature_importances_</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">grid_RF_cancer</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">.</span><span class="n">estimators_</span><span class="p">],</span> 
                       <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">sorted_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">feature_importance_RF_cancer</span><span class="p">)</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">sorted_idx</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="p">.</span><span class="mi">5</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>


<span class="n">plt</span><span class="p">.</span><span class="nf">barh</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">feature_importance_RF_cancer</span><span class="p">[</span><span class="n">sorted_idx</span><span class="p">],</span><span class="n">xerr</span><span class="o">=</span><span class="n">std_RF_cancer</span><span class="p">[</span><span class="n">sorted_idx</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="sh">'</span><span class="s">center</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">yticks</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">breast_cancer_df</span><span class="p">.</span><span class="n">columns</span><span class="p">))[</span><span class="n">sorted_idx</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Feature Importance (MDI)</span><span class="sh">'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Mean decrease in impurity</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<!--<a href="output_183_0.png" rel="noopener noreferrer"><img src="output_183_0.png"  alt="png. "   loading="lazy"></a>-->

<h2 id="random-forest-in-regression">Random Forest in regression.</h2>

<p>From the standpoint of tree, the only difference is that now, instead of the entropy or Gini criterion, <strong>the decision which variable to use at any node is made using a regression metric</strong>, such as squared error for example.</p>

<p>For example, consider this example of <a href="https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html">regression with a single tree</a>, adapted from the sklearn website:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="c1"># Create a random dataset
</span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nc">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="n">rng</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">80</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">X</span><span class="p">).</span><span class="nf">ravel</span><span class="p">()</span>
<span class="n">y</span><span class="p">[::</span><span class="mi">5</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">3</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">-</span> <span class="n">rng</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">16</span><span class="p">))</span> <span class="c1"># adding additional noise to some of the points
</span>
<span class="c1"># Fit regression model
</span><span class="n">regr_1</span> <span class="o">=</span> <span class="nc">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">regr_2</span> <span class="o">=</span> <span class="nc">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">regr_1</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">regr_2</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Predict
</span><span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">y_1</span> <span class="o">=</span> <span class="n">regr_1</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_2</span> <span class="o">=</span> <span class="n">regr_2</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Plot the results
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">"</span><span class="s">darkorange</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">cornflowerblue</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">max_depth=2</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">yellowgreen</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">max_depth=5</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">target</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Decision Tree Regression</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>

<!--<a href="output_186_0.png" rel="noopener noreferrer"><img src="output_186_0.png"  alt="png. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="nf">plot_tree</span><span class="p">(</span> <span class="n">regr_1</span> <span class="p">,</span> 
          <span class="n">ax</span><span class="o">=</span><span class="n">ax</span> <span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span> <span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span> <span class="p">,</span> <span class="n">impurity</span><span class="o">=</span><span class="bp">False</span> <span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">best single decision tree</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<!--<a href="output_187_1.png" rel="noopener noreferrer"><img src="output_187_1.png"  alt="png. "   loading="lazy"></a>-->

<p>Of course with a single tree you do not get very far, unless the tree becomes absolutely huge.</p>

<p>But with a random forest you can aggregate the estimate from many trees to get somewhere nice.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="n">RFReg</span> <span class="o">=</span> <span class="nc">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span> <span class="p">)</span>
<span class="n">RFReg</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Predict
</span><span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">y_1</span> <span class="o">=</span> <span class="n">regr_1</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_rf</span> <span class="o">=</span> <span class="n">RFReg</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Plot the results
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">"</span><span class="s">darkorange</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">cornflowerblue</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">max_depth=2</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_rf</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">yellowgreen</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">RF</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">target</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Decision Tree Regression</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>


</code></pre></div></div>

<!--<a href="output_189_0.png" rel="noopener noreferrer"><img src="output_189_0.png"  alt="png. "   loading="lazy"></a>-->

<p>With a bit of leg-work, we can even grab the inidividual trees predictions to build an interval around the random forest prediction:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1">## collecting prediction from all individual trees in a big list
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">x_pred</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">RFReg</span><span class="p">.</span><span class="n">estimators_</span> <span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">+=</span> <span class="nf">list</span><span class="p">(</span> <span class="n">tree</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="p">)</span>
    <span class="n">x_pred</span> <span class="o">+=</span> <span class="nf">list</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>


<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">"</span><span class="s">darkorange</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">cornflowerblue</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">max_depth=2</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_rf</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">yellowgreen</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">RF</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_pred</span> <span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_pred</span> <span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">yellowgreen</span><span class="sh">"</span> <span class="p">,</span> <span class="n">errorbar</span> <span class="o">=</span> <span class="sh">'</span><span class="s">sd</span><span class="sh">'</span><span class="p">)</span> 
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">target</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Decision Tree Regression</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>

<!--<a href="output_191_0.png" rel="noopener noreferrer"><img src="output_191_0.png"  alt="png. "   loading="lazy"></a>-->

<p>Let’s try on the diabetes data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_diabetes_train</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(331, 10)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="c1">## when it comes to criterion, we can now choose:
# * “squared_error” (default) for the mean squared error, minimizes the L2 loss
#                                           using the mean of each terminal node,
# * “friedman_mse”, which uses mean squared error with Friedman’s improvement score for potential splits
# * “absolute_error” for the mean absolute error, which minimizes the L1 loss
#                                           using the median of each terminal node,
# * “poisson” which uses reduction in Poisson deviance to find splits.
#
# let's try squared error and absolute error
</span>
<span class="n">grid_values</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">criterion</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">squared_error</span><span class="sh">'</span> <span class="p">,</span> <span class="sh">'</span><span class="s">absolute_error</span><span class="sh">'</span><span class="p">],</span>
               <span class="sh">'</span><span class="s">n_estimators</span><span class="sh">'</span><span class="p">:[</span><span class="mi">500</span><span class="p">],</span> 
               <span class="sh">'</span><span class="s">max_depth</span><span class="sh">'</span><span class="p">:[</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">8</span><span class="p">],</span>
               <span class="sh">'</span><span class="s">min_samples_split</span><span class="sh">'</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="nf">len</span><span class="p">(</span><span class="n">X_diabetes_train</span><span class="p">)</span><span class="o">//</span><span class="mi">5</span><span class="p">,</span><span class="mi">20</span><span class="p">),</span>
              <span class="sh">'</span><span class="s">min_samples_leaf</span><span class="sh">'</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="nf">len</span><span class="p">(</span><span class="n">X_diabetes_train</span><span class="p">)</span><span class="o">//</span><span class="mi">5</span><span class="p">,</span><span class="mi">20</span><span class="p">)}</span>

<span class="n">grid_RF_diabetes</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="nc">RandomForestRegressor</span><span class="p">(),</span>
                                <span class="n">param_grid</span> <span class="o">=</span> <span class="n">grid_values</span><span class="p">,</span> 
                                <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">r2</span><span class="sh">'</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">grid_RF_diabetes</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_diabetes_train</span><span class="p">,</span> <span class="n">y_diabetes_train</span><span class="p">)</span>


<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best score (r2): </span><span class="sh">'</span><span class="p">,</span> <span class="n">grid_RF_diabetes</span><span class="p">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best parameter (max. r2): </span><span class="sh">'</span><span class="p">,</span> <span class="n">grid_RF_diabetes</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Grid best score (r2):  0.4877900229012484
Grid best parameter (max. r2):  {'criterion': 'absolute_error', 'max_depth': 8, 'min_samples_leaf': np.int64(2), 'min_samples_split': np.int64(22), 'n_estimators': 500}
CPU times: user 2.77 s, sys: 75.4 ms, total: 2.85 s
Wall time: 59.4 s
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_decision_fn_scores_RF_diabetes</span><span class="o">=</span><span class="n">grid_RF_diabetes</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_diabetes_test</span><span class="p">,</span><span class="n">y_diabetes_test</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Grid best parameter (max. r2) model on test: </span><span class="sh">'</span><span class="p">,</span> <span class="n">y_decision_fn_scores_RF_diabetes</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Grid best parameter (max. r2) model on test:  0.4419573864797268
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_importance_diabetes</span><span class="o">=</span><span class="n">grid_RF_diabetes</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">.</span><span class="n">feature_importances_</span>

<span class="n">sorted_features</span><span class="o">=</span><span class="nf">sorted</span><span class="p">([[</span><span class="n">df_diabetes</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="nf">abs</span><span class="p">(</span><span class="n">feature_importance_diabetes</span><span class="p">[</span><span class="n">i</span><span class="p">])]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">feature_importance_diabetes</span><span class="p">))],</span><span class="n">key</span><span class="o">=</span><span class="nf">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Features sorted per importance in discriminative process</span><span class="sh">'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">f</span><span class="p">,</span><span class="n">w</span> <span class="ow">in</span> <span class="n">sorted_features</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">{:&gt;20}</span><span class="se">\t</span><span class="s">{:.3f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">w</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Features sorted per importance in discriminative process
                  s5	0.363
                 bmi	0.291
                  bp	0.089
                  s3	0.057
                 age	0.049
                  s6	0.044
                  s2	0.037
                  s1	0.035
                  s4	0.030
                 sex	0.004
</code></pre></div></div>

<p>Tree-based techniques are interesting because:</p>
<ul>
  <li>they do not necessitate scaling</li>
  <li>they give interpretable models and results</li>
  <li>they model arbitrary non-linear problems</li>
</ul>

<p>However as you have seen they tend to take longer to train…</p>

<h1 id="conclusion">Conclusion</h1>

<p>During this notebook we have only given a whirlwind tour of what ML is and what is it about.</p>

<p>We have of course only mentionned a handful of the numerous algorithms that can be used, both for <a href="https://scikit-learn.org/stable/supervised_learning.html">classification and for regression</a> (NB: this link is not an exhaustive list, just what has been implemented in the sklearn library).</p>

<p>However, more than a collection of algorithm, Machine Learning should also be seen as a set of methods to solve some important statistical problems :</p>
<ul>
  <li><strong>regularization</strong> parameters (such as l1 or l2 norm, or max depth), to handle <strong>overfitting</strong></li>
  <li><strong>cross-validation</strong> strategies, to detect <strong>overfitting</strong> and handle <strong>model-selection</strong></li>
  <li><strong>adapted metrics</strong> to handle the specific of our goal and our data (handle imbalance for example).
    <ul>
      <li><a href="https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics">classification metrics</a></li>
      <li><a href="https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics">regression metrics</a></li>
    </ul>
  </li>
</ul>

<h1 id="classification-exercise--predicting-heart-disease-on-the-framingham-data-set">Classification exercise : predicting heart disease on the framingham data-set</h1>

<p>Use everything you have learned to model and predict the column <code class="language-plaintext highlighter-rouge">'TenYearCHD'</code> (dependent variable : ten year risk of coronary heart disease).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">##separation in X and y
</span><span class="n">X_heart</span> <span class="o">=</span> <span class="n">df_heart</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span> <span class="n">columns</span> <span class="o">=</span> <span class="sh">"</span><span class="s">TenYearCHD</span><span class="sh">"</span> <span class="p">)</span>
<span class="n">y_heart</span> <span class="o">=</span> <span class="n">df_heart</span><span class="p">[</span> <span class="sh">"</span><span class="s">TenYearCHD</span><span class="sh">"</span> <span class="p">]</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_heart</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>male</th>
      <th>age</th>
      <th>education</th>
      <th>currentSmoker</th>
      <th>cigsPerDay</th>
      <th>BPMeds</th>
      <th>prevalentStroke</th>
      <th>prevalentHyp</th>
      <th>diabetes</th>
      <th>totChol</th>
      <th>sysBP</th>
      <th>diaBP</th>
      <th>BMI</th>
      <th>heartRate</th>
      <th>glucose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>39</td>
      <td>4.0</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>195.0</td>
      <td>106.0</td>
      <td>70.0</td>
      <td>26.97</td>
      <td>80.0</td>
      <td>77.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>46</td>
      <td>2.0</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>250.0</td>
      <td>121.0</td>
      <td>81.0</td>
      <td>28.73</td>
      <td>95.0</td>
      <td>76.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>48</td>
      <td>1.0</td>
      <td>1</td>
      <td>20.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>245.0</td>
      <td>127.5</td>
      <td>80.0</td>
      <td>25.34</td>
      <td>75.0</td>
      <td>70.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>61</td>
      <td>3.0</td>
      <td>1</td>
      <td>30.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>225.0</td>
      <td>150.0</td>
      <td>95.0</td>
      <td>28.58</td>
      <td>65.0</td>
      <td>103.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>46</td>
      <td>3.0</td>
      <td>1</td>
      <td>23.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>285.0</td>
      <td>130.0</td>
      <td>84.0</td>
      <td>23.10</td>
      <td>85.0</td>
      <td>85.0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<p>Splitting in train/test set</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r -7 solutions/solution_03_03.py
</span><span class="n">X_train_heart</span><span class="p">,</span> <span class="n">X_test_heart</span><span class="p">,</span> <span class="n">y_train_heart</span><span class="p">,</span> <span class="n">y_test_heart</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X_heart</span><span class="p">,</span><span class="n">y_heart</span><span class="p">,</span>
                                                   <span class="n">random_state</span><span class="o">=</span><span class="mi">123456</span><span class="p">,</span><span class="n">stratify</span><span class="o">=</span><span class="n">y_heart</span><span class="p">)</span>
<span class="c1">#stratify is here to make sure that you split keeping the repartition of labels unaffected
</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">fraction of class benign in train</span><span class="sh">"</span><span class="p">,</span><span class="nf">sum</span><span class="p">(</span><span class="n">y_train_heart</span><span class="p">)</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">y_train_heart</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">fraction of class benign in test </span><span class="sh">"</span><span class="p">,</span><span class="nf">sum</span><span class="p">(</span><span class="n">y_test_heart</span><span class="p">)</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">y_test_heart</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">fraction of class benign in full </span><span class="sh">"</span><span class="p">,</span><span class="nf">sum</span><span class="p">(</span><span class="n">y_heart</span><span class="p">)</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">y_heart</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fraction of class benign in train 0.15238789646372586
fraction of class benign in test  0.15191256830601094
fraction of class benign in full  0.15226899945325315
</code></pre></div></div>

<p>Logistic regression</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 9-33 solutions/solution_03_03.py
</span></code></pre></div></div>

<p>SVM</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 34-47 solutions/solution_03_03.py
</span></code></pre></div></div>

<p>random forest</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 48-65 solutions/solution_03_03.py
</span></code></pre></div></div>

<p>Evaluation of the best model on the test set</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 67-90 solutions/solution_03_03.py
</span></code></pre></div></div>

<p>ROC curve</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 93-116 solutions/solution_03_03.py
</span></code></pre></div></div>

<p>getting the most important features</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 119-145 solutions/solution_03_03.py
</span></code></pre></div></div>

<p>Additionnal little diagnostic plot</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 146- solutions/solution_03_03.py
</span></code></pre></div></div>

<h1 id="additionnal-regression-exercise--predicting-daily-maximal-temperature">Additionnal Regression exercise : predicting daily maximal temperature</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">features</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">data/One_hot_temp.csv</span><span class="sh">'</span><span class="p">)</span>
<span class="n">features</span><span class="p">.</span><span class="nf">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>temp_2</th>
      <th>temp_1</th>
      <th>average</th>
      <th>actual</th>
      <th>forecast_noaa</th>
      <th>forecast_acc</th>
      <th>forecast_under</th>
      <th>friend</th>
      <th>week_Fri</th>
      <th>week_Mon</th>
      <th>week_Sat</th>
      <th>week_Sun</th>
      <th>week_Thurs</th>
      <th>week_Tues</th>
      <th>week_Wed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>2016</td>
      <td>1</td>
      <td>1</td>
      <td>45</td>
      <td>45</td>
      <td>45.6</td>
      <td>45</td>
      <td>43</td>
      <td>50</td>
      <td>44</td>
      <td>29</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>2016</td>
      <td>1</td>
      <td>2</td>
      <td>44</td>
      <td>45</td>
      <td>45.7</td>
      <td>44</td>
      <td>41</td>
      <td>50</td>
      <td>44</td>
      <td>61</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>2016</td>
      <td>1</td>
      <td>3</td>
      <td>45</td>
      <td>44</td>
      <td>45.8</td>
      <td>41</td>
      <td>43</td>
      <td>46</td>
      <td>47</td>
      <td>56</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>2016</td>
      <td>1</td>
      <td>4</td>
      <td>44</td>
      <td>41</td>
      <td>45.9</td>
      <td>40</td>
      <td>44</td>
      <td>48</td>
      <td>46</td>
      <td>53</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>2016</td>
      <td>1</td>
      <td>5</td>
      <td>41</td>
      <td>40</td>
      <td>46.0</td>
      <td>44</td>
      <td>46</td>
      <td>46</td>
      <td>46</td>
      <td>41</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<ul>
  <li>year: 2016 for all data points</li>
  <li>month: number for month of the year</li>
  <li>day: number for day of the year</li>
  <li>week: day of the week as a character string</li>
  <li>temp_2: max temperature 2 days prior</li>
  <li>temp_1: max temperature 1 day prior</li>
  <li>average: historical average max temperature</li>
  <li>actual: max temperature measurement</li>
  <li>friend: your friend’s prediction, a random number between 20 below the average and 20 above the average</li>
</ul>

<p>Additionally, all the features noted forecast are weather forecast given by some organisation for that day.</p>

<p>We want to predict <code class="language-plaintext highlighter-rouge">actual</code>, th actual max temperature of a day.</p>

<p>Use a random forest to do so. You can inspire yourself from the examples of code above.</p>

<p>Here are a couple of plots to get you started with the data exploration:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">datetime</span>
<span class="n">feature_list</span><span class="o">=</span><span class="nf">list</span><span class="p">(</span><span class="n">features</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">labels</span><span class="o">=</span><span class="n">features</span><span class="p">[</span><span class="sh">"</span><span class="s">actual</span><span class="sh">"</span><span class="p">]</span>
<span class="c1"># Dates of training values
</span><span class="n">months</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">features</span><span class="p">)[:,</span> <span class="n">feature_list</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="sh">'</span><span class="s">month</span><span class="sh">'</span><span class="p">)]</span>
<span class="n">days</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">features</span><span class="p">)[:,</span> <span class="n">feature_list</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="sh">'</span><span class="s">day</span><span class="sh">'</span><span class="p">)]</span>
<span class="n">years</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">features</span><span class="p">)[:,</span> <span class="n">feature_list</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">)]</span>

<span class="c1"># List and then convert to datetime object
</span><span class="n">dates</span> <span class="o">=</span> <span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">year</span><span class="p">))</span> <span class="o">+</span> <span class="sh">'</span><span class="s">-</span><span class="sh">'</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">month</span><span class="p">))</span> <span class="o">+</span> <span class="sh">'</span><span class="s">-</span><span class="sh">'</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">day</span><span class="p">))</span> <span class="k">for</span> <span class="n">year</span><span class="p">,</span> <span class="n">month</span><span class="p">,</span> <span class="n">day</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">years</span><span class="p">,</span> <span class="n">months</span><span class="p">,</span> <span class="n">days</span><span class="p">)]</span>
<span class="n">dates</span> <span class="o">=</span> <span class="p">[</span><span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="nf">strptime</span><span class="p">(</span><span class="n">date</span><span class="p">,</span> <span class="sh">'</span><span class="s">%Y-%m-%d</span><span class="sh">'</span><span class="p">)</span> <span class="k">for</span> <span class="n">date</span> <span class="ow">in</span> <span class="n">dates</span><span class="p">]</span>

<span class="c1"># Dataframe with true values and dates
</span><span class="n">true_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">:</span> <span class="n">dates</span><span class="p">,</span> <span class="sh">'</span><span class="s">actual</span><span class="sh">'</span><span class="p">:</span> <span class="n">labels</span><span class="p">})</span>


<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Date</span><span class="sh">'</span><span class="p">);</span> 
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Maximum Temperature (F)</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Plot the actual values
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">true_data</span><span class="p">[</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">],</span> <span class="n">true_data</span><span class="p">[</span><span class="sh">'</span><span class="s">actual</span><span class="sh">'</span><span class="p">],</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">'</span><span class="s">actual</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">(</span><span class="n">rotation</span> <span class="o">=</span> <span class="sh">'</span><span class="s">60</span><span class="sh">'</span><span class="p">);</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<!--<a href="output_223_0.png" rel="noopener noreferrer"><img src="output_223_0.png"  alt="png. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">datetime</span>
<span class="n">feature_list</span><span class="o">=</span><span class="nf">list</span><span class="p">(</span><span class="n">features</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">labels</span><span class="o">=</span><span class="n">features</span><span class="p">[</span><span class="sh">"</span><span class="s">average</span><span class="sh">"</span><span class="p">]</span>
<span class="c1"># Dates of training values
</span><span class="n">months</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">features</span><span class="p">)[:,</span> <span class="n">feature_list</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="sh">'</span><span class="s">month</span><span class="sh">'</span><span class="p">)]</span>
<span class="n">days</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">features</span><span class="p">)[:,</span> <span class="n">feature_list</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="sh">'</span><span class="s">day</span><span class="sh">'</span><span class="p">)]</span>
<span class="n">years</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">features</span><span class="p">)[:,</span> <span class="n">feature_list</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">)]</span>

<span class="c1"># List and then convert to datetime object
</span><span class="n">dates</span> <span class="o">=</span> <span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">year</span><span class="p">))</span> <span class="o">+</span> <span class="sh">'</span><span class="s">-</span><span class="sh">'</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">month</span><span class="p">))</span> <span class="o">+</span> <span class="sh">'</span><span class="s">-</span><span class="sh">'</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">day</span><span class="p">))</span> <span class="k">for</span> <span class="n">year</span><span class="p">,</span> <span class="n">month</span><span class="p">,</span> <span class="n">day</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">years</span><span class="p">,</span> <span class="n">months</span><span class="p">,</span> <span class="n">days</span><span class="p">)]</span>
<span class="n">dates</span> <span class="o">=</span> <span class="p">[</span><span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="nf">strptime</span><span class="p">(</span><span class="n">date</span><span class="p">,</span> <span class="sh">'</span><span class="s">%Y-%m-%d</span><span class="sh">'</span><span class="p">)</span> <span class="k">for</span> <span class="n">date</span> <span class="ow">in</span> <span class="n">dates</span><span class="p">]</span>

<span class="c1"># Dataframe with true values and dates
</span><span class="n">true_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">:</span> <span class="n">dates</span><span class="p">,</span> <span class="sh">'</span><span class="s">average</span><span class="sh">'</span><span class="p">:</span> <span class="n">labels</span><span class="p">})</span>


<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Date</span><span class="sh">'</span><span class="p">);</span> 
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Maximum Temperature (F)</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Plot the average values
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">true_data</span><span class="p">[</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">],</span> <span class="n">true_data</span><span class="p">[</span><span class="sh">'</span><span class="s">average</span><span class="sh">'</span><span class="p">],</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">'</span><span class="s">average</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">(</span><span class="n">rotation</span> <span class="o">=</span> <span class="sh">'</span><span class="s">60</span><span class="sh">'</span><span class="p">);</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<!--<a href="output_224_0.png" rel="noopener noreferrer"><img src="output_224_0.png"  alt="png. "   loading="lazy"></a>-->

<p>Solution - Read in data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 1-5 solutions/solution_03_02.py
</span></code></pre></div></div>

<p>Solution - train/test split</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 8-17 solutions/solution_03_02.py
</span></code></pre></div></div>

<p>Solution - setup and fit pipeline</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 19-34 solutions/solution_03_02.py
</span></code></pre></div></div>

<p>Solution - evaluate the model on the test set</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 36-40 solutions/solution_03_02.py
</span></code></pre></div></div>

<p>Solution - get the feature importances</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 41-49 solutions/solution_03_02.py
</span></code></pre></div></div>

<p>Solution - using permutation to get the importances</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load -r 50-73 solutions/solution_03_02.py
</span></code></pre></div></div>

<p>Solution - BONUS - re-thinking the splitting strategy</p>

<!--<a href="image/TimeSeriesSplit.png" rel="noopener noreferrer"><img src="image/TimeSeriesSplit.png"  alt="RF. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load solutions/solution_03_02ter.py
</span></code></pre></div></div>

<p>Solution - BONUS - an even better splitting strategy</p>

<!--<a href="image/BlockedTimeSeriesSplit.png" rel="noopener noreferrer"><img src="image/BlockedTimeSeriesSplit.png"  alt="RF. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load solutions/solution_03_02quat.py
</span></code></pre></div></div>

<h1 id="annexes">Annexes</h1>

<h2 id="features-selection">Features selection</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="o">=</span><span class="n">sns</span><span class="p">.</span><span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">iris</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Here we use the data loader from seaborn but such data loaders also exist with scikit-learn and are more generally delt
#with the dataframe handler pandas
</span><span class="n">df</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal_length</th>
      <th>sepal_width</th>
      <th>petal_length</th>
      <th>petal_width</th>
      <th>species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="p">.</span><span class="nf">pairplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">hue</span><span class="o">=</span><span class="sh">"</span><span class="s">species</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Seaborn allows you to 'split' your data according to a chosen parameter hue. Here I chose to color split the data according
#to the target
#description diagonal
</span></code></pre></div></div>

<!--<a href="output_247_1.png" rel="noopener noreferrer"><img src="output_247_1.png"  alt="png. "   loading="lazy"></a>-->

<p>What do you get from the plots above?</p>

<p>Looking at the diagonal of these plots : petal features separate the species more efficiently than sepal features.</p>

<p>There is a very strong correlation between <code class="language-plaintext highlighter-rouge">petal_length</code> and <code class="language-plaintext highlighter-rouge">petal_width</code> : those two features are probably so similar that keeping them both could be redundant.</p>

<p>The least correlation visible seems to be between <code class="language-plaintext highlighter-rouge">sepal_width</code> and all the others.</p>

<p>By itself <code class="language-plaintext highlighter-rouge">sepal_width</code> is not good at differentiating species but associated with other features we can already see groups forming by species. And since they are very much non-colinear I would say that, in dimension two, <code class="language-plaintext highlighter-rouge">petal_length</code> and <code class="language-plaintext highlighter-rouge">sepal_width</code> are already a good pick for low dimensions models.</p>

<p>You can actually quantify the correlation between features by calling the <code class="language-plaintext highlighter-rouge">corr()</code> function in pandas. You would prefer (and sometime is requiered) having a subset of features that are not correlated to each others.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_corr</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">corr</span><span class="p">()</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">clustermap</span><span class="p">(</span><span class="n">df_corr</span><span class="p">,</span>
               <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span>
               <span class="n">z_score</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">row_cluster</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
               <span class="n">col_cluster</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
               <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">ward</span><span class="sh">'</span><span class="p">,</span>
               <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">coolwarm</span><span class="sh">'</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> 
               <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">size</span><span class="sh">"</span><span class="p">:</span> <span class="mi">13</span><span class="p">},</span><span class="n">cbar_kws</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">:</span> <span class="sh">'</span><span class="s">Pearson</span><span class="se">\n</span><span class="s">correlation</span><span class="sh">'</span><span class="p">})</span>
<span class="c1">## sns allows you to do a hierarchical clustering that simply
</span><span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<!--<a href="output_249_0.png" rel="noopener noreferrer"><img src="output_249_0.png"  alt="png. "   loading="lazy"></a>-->

<h3 id="classification">Classification</h3>
<p>One thing (among others) that you can do is to look for a <strong>subset of features that seems to be important to describe the target class</strong>. It’s like the pairplots above but instead of just looking at it you choose the features you want to keep.</p>

<p>You can choose different metrics for ‘how important to describe the class’ a feature is. 
Many of those metrics utilize concepts that we haven’t introduced yet, in contexts that we haven’t seen yet, so I will introduce two metrics for classification that don’t need too much of <em>a priori</em> knowledge.</p>

<p><code class="language-plaintext highlighter-rouge">Scikit-learn</code> lets you specify a threshold on the features are kept, either as:</p>
<ul>
  <li>a direct number: <code class="language-plaintext highlighter-rouge">SelectKBest</code>.</li>
  <li>important features from a percentile of your top importance score: <code class="language-plaintext highlighter-rouge">SelectPercentile</code>.</li>
  <li>an error type: <code class="language-plaintext highlighter-rouge">SelectFpr</code> or <code class="language-plaintext highlighter-rouge">SelectFdr</code> (see course 2 logistic regression part).</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Scikit-learn</code> offers you different scores to calculate the importance of your features.</p>

<ul>
  <li><strong>ANOVA-F</strong> : F=$\frac{Var_{feature_i}(Between_class)}{Var_{feature_i}(Within_class)}$.</li>
</ul>

<p>          
<strong>F</strong> itself gives you how much a feature $i$ variance is different between classes, normalized by the intrinsic variance of that feature per class.</p>

<p>           
So if <strong>F</strong> is big it means that the variation that you observe between classes is big compared to the variance of this feature :</p>

<p>           
it behaves differently for different classes so it it is a good feature to keep for the classification.</p>

<p>           
To this <strong>F</strong> is associated a <strong>p-value</strong> that you would use for scoring.</p>

<ul>
  <li><strong>Chi2</strong> ($\chi^{2}$) test.</li>
</ul>

<p>          
You suppose the null hypothesis that this feature $i$ is homogenously distributed among classes</p>

<p>          
and so you are expecting that its representation in different classes should be very similar to what you can calculate for the bulk data</p>

<p>          
 i.e. $\frac{\Sigma^{n_points} feature_{i}}{n_points}$.</p>

<p>          
You then compare the actual distribution of this feature in different classes to your null model predictions. If this <strong>sum of square differences</strong>:</p>

<p>          
$\Sigma^{n_class}<em>{k}\frac{(expected_form_null_hypothesis</em>{k}-observed_{k})^{2}}{observed}$</p>

<p>           
is big then the null hypothesis has to be rejected and this feature is significant for classifying.</p>

<p>          
The sum of these square quantities over the different classes asymptotically follows a $\chi^{2}$</p>

<p>          
distribution and thus you have access to a <strong>p-value for scoring</strong>.</p>

<p>Another score would be to use the amount of <a href="https://en.wikipedia.org/wiki/Mutual_information">Mutual Information</a> shared between a feature and our target.</p>

<p>The way this mutual information is caclulated is out of the scope of this class as it is a bit technical.</p>

<p>For regression just use correlation or Mutual Information</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="n">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">chi2</span>

<span class="n">skb</span> <span class="o">=</span> <span class="nc">SelectKBest</span><span class="p">(</span><span class="n">chi2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="c1">#creating the object SelectKBest and settling for 2 best features (k=2) in term of chi2 score
</span><span class="n">skb</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="nf">list</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="n">df</span><span class="p">[</span><span class="nf">list</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span><span class="c1">#calculating the chi2 for each features
</span>
<span class="n">dico_pval</span><span class="o">=</span><span class="p">{</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span><span class="n">v</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">skb</span><span class="p">.</span><span class="n">pvalues_</span><span class="p">)}</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">features Chi2 scores (p-values):</span><span class="sh">"</span><span class="p">)</span><span class="c1">#all the features and the chi2 pvalues associated. use .pvalues_
</span><span class="k">for</span> <span class="n">feature</span><span class="p">,</span><span class="n">pval</span> <span class="ow">in</span> <span class="n">dico_pval</span><span class="p">.</span><span class="nf">items</span><span class="p">()</span> <span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\t</span><span class="sh">'</span><span class="p">,</span><span class="n">feature</span> <span class="p">,</span> <span class="sh">'</span><span class="s">:</span><span class="sh">'</span> <span class="p">,</span> <span class="n">pval</span> <span class="p">)</span>

<span class="n">X_new</span><span class="o">=</span><span class="n">skb</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="nf">list</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span><span class="c1"># keep only the k=2 best features according to the score
</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">New data with only the k=2 best features kept :</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">X_new</span><span class="p">[:</span><span class="mi">5</span><span class="p">,])</span> <span class="c1">#printing only the 5 first entries
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">...</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>features Chi2 scores (p-values):
     sepal_length : 0.004476514990225755
     sepal_width : 0.15639598043162506
     petal_length : 5.533972277193705e-26
     petal_width : 2.7582496530033412e-15
New data with only the k=2 best features kept :
[[1.4 0.2]
 [1.4 0.2]
 [1.3 0.2]
 [1.5 0.2]
 [1.4 0.2]]
...
</code></pre></div></div>

<h3 id="multi-classes">Multi classes</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X3</span><span class="p">,</span> <span class="n">y3</span> <span class="o">=</span> <span class="nf">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">cluster_std</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span><span class="c1"># 120 points, 3 blobs/clusters with some spread=3
#Random_state is here just to be sure that every time you will get the same blobs. If you change the random_state or do not
#specify it then you will get a new plot every time you call the function (random seed)
</span></code></pre></div></div>

<p>Of course all of that can be applied to a multi-classes classification. How is it tipically done?</p>

<p>There are many different ways of tackling the problem, that end up being a combination of these 4 elements :</p>

<ul>
  <li>
    <p><strong>Either you treat the problem as one class vs one class</strong>.</p>
  </li>
  <li><strong>Or you treat the problem as a one class vs the rest : you subdivide the problem into three different problems either your are class 1 and you consider the other classes as being one big class “non 1”, and you do the same for the other class</strong>.</li>
  <li><strong>You change your loss function to a multinomial one : softmax intead of a sigmoid.</strong></li>
</ul>

<p>In any case you need to decide <strong>how you are going to agglomerate those different statistics (different ROC curves for example)</strong>:</p>

<ul>
  <li><strong>micro average</strong> : pull all raw numbers together (eg. number of FP, TP), group them and then calculate your overall statistic (eg. TPR)</li>
  <li><strong>macro average</strong> : calculate each statistics separately and then do the average.</li>
</ul>

<p>Think about the differences induced by those metrics. Why should you use one more than the other? Or maybe you should always use all of them?</p>

<p>Spoiler it has to do with overall separability and balance between the different class.</p>

<p>What strategy your logistic regression uses so you can plot the right curves, is a tricky question. For a first pass on your data always set the multiclasses method to be ovr (one vs rest) : understanding the hyperplanes relation to decision probability and the ROC curve is more intuitive that way, and I believe less sensitive to imbalance dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">utils</span> <span class="kn">import</span> <span class="n">contour_lr_more</span> 
<span class="c1">#one vs rest implementation
</span><span class="nf">contour_lr_more</span><span class="p">(</span><span class="sh">'</span><span class="s">l2</span><span class="sh">'</span><span class="p">,</span><span class="n">X3</span><span class="p">,</span><span class="n">y3</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="sh">'</span><span class="s">ovr</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div>

<!--<a href="output_255_0.png" rel="noopener noreferrer"><img src="output_255_0.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_255_1.png" rel="noopener noreferrer"><img src="output_255_1.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_255_2.png" rel="noopener noreferrer"><img src="output_255_2.png"  alt="png. "   loading="lazy"></a>-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">utils</span> <span class="kn">import</span> <span class="n">contour_lr_more</span> 
<span class="c1">#softmax implementation (something only available with logistic regression), and again different from one vs one
#and one vs rest
</span><span class="nf">contour_lr_more</span><span class="p">(</span><span class="sh">'</span><span class="s">l2</span><span class="sh">'</span><span class="p">,</span><span class="n">X3</span><span class="p">,</span><span class="n">y3</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="sh">'</span><span class="s">multinomial</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div>

<!--<a href="output_256_0.png" rel="noopener noreferrer"><img src="output_256_0.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_256_1.png" rel="noopener noreferrer"><img src="output_256_1.png"  alt="png. "   loading="lazy"></a>-->

<!--<a href="output_256_2.png" rel="noopener noreferrer"><img src="output_256_2.png"  alt="png. "   loading="lazy"></a>-->


                </section>

                <section aria-label="Tutorial Footer, Feedback, Citation" id="tutorial-footer">
                        <h3>You've Finished the Tutorial</h3>
                        <button id="tutorial-finish-button" class="btn btn-primary" onclick="tutorial_finish()">I finished this tutorial 👍</button>
                        <p style="display: none" id="tutorial-finish-text">Please also consider filling out the <a href="#gtn-feedback">Feedback Form</a> as well!</p>
                        <script>
                        function tutorial_finish() {
                          if(typeof plausible !== 'undefined'){
                            // Plausible may be undefined (script blocked)
                            // or it may be defined, but opted-out (select box/DNT),
                            // which means `plausible()` will work but not send data, *nor* execute the callback.
                            plausible('TutorialComplete', {props: {path: document.location.pathname}})
                          }
                          // since the callback is completely cosmetic, we'll just issue it optimistically.
                          tutorial_finish_finish();
                        }
                        function tutorial_finish_finish() {
                          document.getElementById("tutorial-finish-button").innerText = 'Congrats! Thanks for letting us know! 🎉'
                          document.getElementById("tutorial-finish-button").disabled = true
                          document.getElementById("tutorial-finish-button").disabled = true
                          document.getElementById("tutorial-finish-text").style.display = 'block'
                        }
                        </script>

                
                <blockquote class="key_points">
                    <div class="box-title"><i class="fas fa-key" aria-hidden="true"></i> Key points</div>
                    <ul>
                        
                        <li><p>To be added</p>
</li>
                        
                    </ul>
                </blockquote>
                

                <h1>Frequently Asked Questions</h1>
                Have questions about this tutorial? Have a look at the available FAQ pages and support channels
                <ul>
                
                
                  <li><a href="/training-material/topics/statistics/faqs/">Topic  FAQs</a></li>
                
                  <li><a href="/training-material/faqs/galaxy/">Galaxy FAQs</a> </li>
                  <li><a href="https://gitter.im/Galaxy-Training-Network/Lobby">GTN Matrix Chat</a></li>
                  <li><a href="https://help.galaxyproject.org">Galaxy Help Forum</a></li>
                </ul>

		<!--
                
		-->

                


                

                

                <h1 id="gtn-feedback">Feedback</h1>
                <p class="text-muted">Did you use this material as an instructor? Feel free to give us feedback on <a href="https://github.com/galaxyproject/training-material/issues/1452">how it went</a>.
                <br>Did you use this material as a learner or student? Click the form below to leave feedback.<i class="fas fa-hand-point-down"></i>
                </p>

                <iframe id="feedback-google" class="google-form" src="https://docs.google.com/forms/d/e/1FAIpQLSd4VZptFTQ03kHkMz0JyW9b6_S8geU5KjNE_tLM0dixT3ZQmA/viewform?embedded=true&entry.1235803833=statistics/intro-to-ml-with-python"><a href="https://docs.google.com/forms/d/e/1FAIpQLSd4VZptFTQ03kHkMz0JyW9b6_S8geU5KjNE_tLM0dixT3ZQmA/viewform?embedded=true&entry.1235803833=statistics/intro-to-ml-with-python">Feedback Form</a></iframe>

                <h1>Citing this Tutorial</h1>
                <p>
                    <ol>
                        <li id="citation-text">
                            Wandrille Duchemin,  <b>Foundational Aspects of Machine Learning using Python (Galaxy Training Materials)</b>. <a href="https://training.galaxyproject.org/training-material/topics/statistics/tutorials/intro-to-ml-with-python/tutorial.html">https://training.galaxyproject.org/training-material/topics/statistics/tutorials/intro-to-ml-with-python/tutorial.html</a> Online; accessed TODAY
                        </li>
                        <li>
                        Hiltemann, Saskia, Rasche, Helena et al., 2023 <b>Galaxy Training: A Powerful Framework for Teaching!</b> PLOS Computational Biology <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010752">10.1371/journal.pcbi.1010752</a>
                        </li>
                        <li>
                        Batut et al., 2018 <b>Community-Driven Data Analysis Training for Biology</b> Cell Systems <a href="https://doi.org/10.1016%2Fj.cels.2018.05.012">10.1016/j.cels.2018.05.012</a>
                        </li>
                    </ol>
                </p>

                <!-- collapsible boxcontaining the BibTeX-formatted citation -->
                <blockquote class="details">

                  <div id="citation-bibtex" class="box-title">
                    <button type="button" aria-controls="citation-bibtex" aria-expanded="false">
                      <i class="fas fa-info-circle" aria-hidden="true"></i>
                      <span class="visually-hidden"></span> BibTeX<span role="button" class="fold-unfold fa fa-minus-square" aria-hidden="true"></span>
                    </button>
                   </div>
                   <p style="display: none;">

                   <div class="highlighter-rouge"><div class="highlight"><pre class="highlight">


<code id="citation-code">@misc{statistics-intro-to-ml-with-python,
author = "Wandrille Duchemin",
	title = "Foundational Aspects of Machine Learning using Python (Galaxy Training Materials)",
	year = "",
	month = "",
	day = "",
	url = "\url{https://training.galaxyproject.org/training-material/topics/statistics/tutorials/intro-to-ml-with-python/tutorial.html}",
	note = "[Online; accessed TODAY]"
}
@article{Hiltemann_2023,
	doi = {10.1371/journal.pcbi.1010752},
	url = {https://doi.org/10.1371%2Fjournal.pcbi.1010752},
	year = 2023,
	month = {jan},
	publisher = {Public Library of Science ({PLoS})},
	volume = {19},
	number = {1},
	pages = {e1010752},
	author = {Saskia Hiltemann and Helena Rasche and Simon Gladman and Hans-Rudolf Hotz and Delphine Larivi{\`{e}}re and Daniel Blankenberg and Pratik D. Jagtap and Thomas Wollmann and Anthony Bretaudeau and Nadia Gou{\'{e}} and Timothy J. Griffin and Coline Royaux and Yvan Le Bras and Subina Mehta and Anna Syme and Frederik Coppens and Bert Droesbeke and Nicola Soranzo and Wendi Bacon and Fotis Psomopoulos and Crist{\'{o}}bal Gallardo-Alba and John Davis and Melanie Christine Föll and Matthias Fahrner and Maria A. Doyle and Beatriz Serrano-Solano and Anne Claire Fouilloux and Peter van Heusden and Wolfgang Maier and Dave Clements and Florian Heyl and Björn Grüning and B{\'{e}}r{\'{e}}nice Batut and},
	editor = {Francis Ouellette},
	title = {Galaxy Training: A powerful framework for teaching!},
	journal = {PLoS Comput Biol}
}
</code>
                   </pre></div></div>
                   </p>
                </blockquote>

        


<script>
// update the date on load, or leave fallback of 'today'
const citationTodaysDate = new Date();
document.getElementById("citation-code").innerHTML = document.getElementById("citation-code").innerHTML.replace("TODAY", citationTodaysDate.toDateString());
document.getElementById("citation-text").innerHTML = document.getElementById("citation-text").innerHTML.replace("TODAY", citationTodaysDate.toDateString());
</script>

                <i class="far fa-thumbs-up" aria-hidden="true"></i> Congratulations on successfully completing this tutorial!

                

                

		
                <blockquote class="details follow-up" id="admins-install-missing-tools">
                  <div id="admin-missing-tools" class="box-title">
                    <button type="button" aria-controls="admin-missing-tools" aria-expanded="false">
                      <i class="fas fa-info-circle" aria-hidden="true"></i>
                      <span class="visually-hidden"></span> Galaxy Administrators: Install the missing tools<span role="button" class="fold-unfold fa fa-minus-square" aria-hidden="true"></span>
                    </button>
                   </div>


			<p>You can use Ephemeris's <code>shed-tools install</code> command to install the tools used in this tutorial.</p>
<div class="highlight"><pre class="highlight"><code>shed-tools install [-g GALAXY] [-a API_KEY] -t &lt;(curl https://training.galaxyproject.org/training-material/api/topics/statistics/tutorials/intro-to-ml-with-python/tutorial.json | jq .admin_install_yaml -r)</code></pre></div>
<p>Alternatively you can copy and paste the following YAML</p>
<div class="highlight"><pre class="highlight"><code>---
install_tool_dependencies: true
install_repository_dependencies: true
install_resolver_dependencies: true
tools: []
</code></pre></div>
                </blockquote>
		

		<blockquote class="details hide-when-printing" id="feedback-responses">
                  <div id="feedback-response-c" class="box-title">
                    <button type="button" aria-controls="feedback-response-c" aria-expanded="false">
                      <i class="fas fa-info-circle" aria-hidden="true"></i>
                      <span class="visually-hidden"></span> Feedback<span role="button" class="fold-unfold fa fa-minus-square" aria-hidden="true"></span>
                    </button>
                   </div>

		   <p>
		   
		   <table class="charts-css bar show-labels" style="--labels-size: 8rem; overflow-y: hidden">
		   
		   </table>
		   </p>
		
        
        
            <p>No feedback has been recieved yet for this training. Be the first one by filling in the feedback form. </p>
        
		</blockquote>




		
		
		

                </section>

            </div>
        </div>
    </div>
</article>
<br/>
<br/>
<br/>

        </div>
        <footer>
	<hr />
	<div class="container">
		<div class="row">
			<div class="col-sm-3">
				<span style="font-size: 2em">GTN</span>
				<p>
					The GTN provides learners with a free, open repository of online training
					materials, with a focus on hands-on training that aims to be directly applicable for learners.
					We aim to connect researchers and learners with local trainers, and events worldwide.
				</p>
				<p>
					We promote FAIR and Open Science practices worldwide, are committed to the accessibility of this platform and training for everyone.
				</p>
			</div>
			<div class="col-sm-3">
				<span style="font-size: 1.3em">About Us</span>
				<ul class="no-bullets">
					<li><a href="/training-material/about.html">About</a></li>
					<li><a rel="code-of-conduct" href="https://galaxyproject.org/community/coc/">Code of Conduct</a></li>
					<li><a href="/training-material/accessibility.html">Accessibility</a></li>
					<li><a href="/training-material/faqs/gtn/fair_training.html">100% FAIR Training</a></li>
					<li><a href="/training-material/faqs/gtn/collaborative_development.html">Collaborative Development</a></li>
				</ul>
				<span style="font-size: 1.3em">Page</span>
				<ul class="no-bullets">
					

					

					<li>
						<a rel="license" href="https://spdx.org/licenses/CC-BY-4.0">
							Content licensed under Creative Commons Attribution 4.0 International License
						</a>
					</li>
					<li>
						<a href="https://github.com/galaxyproject/training-material/edit/main/topics/statistics/tutorials/intro-to-ml-with-python/tutorial.md">
						<i class="fab fa-github" aria-hidden="true"></i><span class="visually-hidden">github</span> Edit on GitHub
						</a>
					</li>
					<li>
						<a href="https://github.com/galaxyproject/training-material/commits/main/topics/statistics/tutorials/intro-to-ml-with-python/tutorial.md">
						<i class="fab fa-github" aria-hidden="true"></i><span class="visually-hidden">github</span> View Changes on GitHub
						</a>
					</li>
				</ul>
			</div>
			<div class="col-sm-3">
				<span style="font-size: 1.3em">Support</span>
				<ul class="no-bullets">
					<li><a rel="me" href="/training-material/faqs/galaxy/">Galaxy FAQs</a></li>
					<li><a rel="me" href="https://help.galaxyproject.org">Galaxy Help Forum</a></li>
					<li><a rel="me" href="http://gxy.io/gtn-slack">GTN Slack Chat</a></li>
					<li><a rel="me" href="https://matrix.to/#/%23Galaxy-Training-Network_Lobby%3Agitter.im">GTN Matrix Chat</a></li>
					<li><a rel="me" href="https://matrix.to/#/#galaxyproject_Lobby:gitter.im">Galaxy Matrix Chat</a></li>
				</ul>
				<span style="font-size: 1.3em">Framework</span>
				<ul class="no-bullets">
					<li>Revision <a href="https://github.com/galaxyproject/training-material/commit/c8f75f0bb8b08e7d5da83ea63d24e71238e0309c">c8f75f0</a></li>
					<li><a rel="license" href="https://github.com/galaxyproject/training-material/blob/main/LICENSE.md">MIT</a> Licensed</li>
					<li><a href="https://jekyllrb.com/">Jekyll(4.3.2 | production)</a></li>
				</ul>
			</div>
			<div class="col-sm-3">
				<span style="font-size: 1.3em">Follow Us!</span>
				<ul class="no-bullets">
					<li><span style="fill: var(--hyperlink);"><svg   width="1em"   height="1em"   viewBox="0 0 8.4937906 9.1084023"   version="1.1"   id="svg356"   xmlns="http://www.w3.org/2000/svg"   xmlns:svg="http://www.w3.org/2000/svg">  <g     id="layer1"     transform="translate(-70.566217,-144.26757)">    <path       style="fill-opacity:1;stroke:none;stroke-width:0.0179182"       d="m 76.39081,152.24155 c -0.737138,0.20763 -1.554999,0.29101 -2.311453,0.14333 -0.475335,-0.0928 -0.891898,-0.32923 -1.031589,-0.82423 -0.04356,-0.15434 -0.06132,-0.32388 -0.06142,-0.48378 0.353724,0.0457 0.702251,0.1304 1.057176,0.17407 0.701338,0.0864 1.394702,0.0784 2.096434,0.008 0.744056,-0.0745 1.433711,-0.21546 2.060598,-0.64854 0.243974,-0.16855 0.474672,-0.39133 0.603487,-0.66252 0.181421,-0.38195 0.175886,-0.89336 0.204447,-1.30803 0.0923,-1.34029 0.20588,-2.98599 -1.076708,-3.846 -0.499561,-0.33497 -1.208891,-0.39913 -1.791824,-0.45742 -0.987026,-0.0987 -1.971078,-0.0946 -2.956509,0.0338 -0.841146,0.10961 -1.595223,0.31468 -2.1065,1.0443 -0.493296,0.70396 -0.509564,1.52563 -0.509564,2.34729 0,1.37831 -0.05534,2.87744 0.595934,4.13911 0.504703,0.97774 1.498709,1.29589 2.52184,1.41832 0.473239,0.0566 0.96049,0.0849 1.434158,0.0172 0.328853,-0.0471 0.650325,-0.0999 0.966886,-0.20511 0.08957,-0.0298 0.266911,-0.0614 0.322027,-0.14486 0.04089,-0.0618 0.0099,-0.15812 0.0035,-0.22545 -0.01611,-0.16924 -0.02094,-0.34967 -0.02096,-0.51963 m -1.594723,-5.48298 c 0.214822,-0.25951 0.315898,-0.56088 0.60922,-0.75705 0.687899,-0.46006 1.692038,-0.11202 1.992096,0.63161 0.214571,0.5317 0.140174,1.15913 0.140174,1.72017 v 1.03925 c 0,0.0911 0.04009,0.30954 -0.01842,0.38339 -0.04193,0.053 -0.173018,0.0287 -0.232436,0.0287 h -0.698809 v -1.88142 c 0,-0.28413 0.04813,-0.63823 -0.09912,-0.89591 -0.234746,-0.4108 -0.875019,-0.36105 -1.092116,0.0358 -0.123368,0.22555 -0.116792,0.50369 -0.116792,0.75257 v 1.0751 h -0.931726 v -1.05718 c 0,-0.2555 0.0024,-0.53932 -0.121773,-0.77049 -0.21432,-0.39919 -0.857782,-0.44403 -1.090217,-0.0358 -0.147324,0.25871 -0.09604,0.61056 -0.09604,0.89591 v 1.88142 H 72.09042 v -1.98893 c 0,-0.4711 -0.01604,-0.95902 0.233201,-1.3797 0.585269,-0.98786 2.133584,-0.74836 2.472454,0.32253 z"       id="path2318" />  </g></svg></span> <a rel="me" href="https://mstdn.science/@gtn">Mastodon</a></li>
					<li><span style="fill: var(--hyperlink);"><svg  viewBox="0 0 64 57" width="1em" ><path style="fill-opacity:1;stroke:none;stroke-width:0.0179182" d="M13.873 3.805C21.21 9.332 29.103 20.537 32 26.55v15.882c0-.338-.13.044-.41.867-1.512 4.456-7.418 21.847-20.923 7.944-7.111-7.32-3.819-14.64 9.125-16.85-7.405 1.264-15.73-.825-18.014-9.015C1.12 23.022 0 8.51 0 6.55 0-3.268 8.579-.182 13.873 3.805ZM50.127 3.805C42.79 9.332 34.897 20.537 32 26.55v15.882c0-.338.13.044.41.867 1.512 4.456 7.418 21.847 20.923 7.944 7.111-7.32 3.819-14.64-9.125-16.85 7.405 1.264 15.73-.825 18.014-9.015C62.88 23.022 64 8.51 64 6.55c0-9.818-8.578-6.732-13.873-2.745Z"></path></svg></span><a rel="me" href="https://bsky.app/profile/galaxytraining.bsky.social"> Bluesky</a></li>
				</ul>

				<span style="font-size: 1.3em">Publications</span>
				<ul class="no-bullets">
					<li><a href="https://doi.org/10.1371/journal.pcbi.1010752">Hiltemann et al. 2023</a></li>
					<li><a href="https://doi.org/10.1016/j.cels.2018.05.012"> Batut et al. 2018</a></li>
					<li><a href="/training-material/faqs/gtn/gtn_citing.html">Citing Us</a></li>
				</ul>
			</div>
		</div>
	</div>
</footer>


        <script  async defer src='/training-material/assets/js/bundle.main.40d4e218.js'></script>

	
	<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
	
	

    </body>
</html>